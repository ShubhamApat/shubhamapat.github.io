[{"content":"The Goal Building systems for degree verification and academic fraud detection requires authentic examples. I collected degrees from LinkedIn graduation posts across 8 countries.\nCoverage 1,400 unique degrees from:\nUSA: Ivy League, top universities, business schools India: IITs, IIMs, AIIMS UK: Oxford, Cambridge, Russell Group Australia: Group of Eight (Go8) Europe: Top institutions (ETH Zurich, TUM, etc.) Asia: NUS, NTU, Seoul National, University of Tokyo Collection Method def collect_graduation_posts(keywords, universities): posts = [] for university in universities: search_queries = [ f\u0026#34;{university} graduation 2024\u0026#34;, f\u0026#34;{university} degree ceremony\u0026#34;, f\u0026#34;graduated from {university}\u0026#34; ] for query in search_queries: results = linkedin_scraper.search_posts( query=query, limit=50, date_range=\u0026#34;2023-2024\u0026#34; ) posts.extend(results) return posts Quality Control Deduplication Used perceptual hashing to remove duplicates:\nfrom imagehash import average_hash hash_value = average_hash(image) if hash_value not in seen_hashes: seen_hashes.add(hash_value) unique_degrees.append(degree) Removed: ~15% duplicates\nValidation Resolution check: Minimum 800x600 Format check: JPEG/PNG only Content verification: OCR-based degree detection The Dataset Structure { \u0026#34;degree_id\u0026#34;: \u0026#34;deg_0001\u0026#34;, \u0026#34;university\u0026#34;: \u0026#34;Stanford University\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;USA\u0026#34;, \u0026#34;degree_type\u0026#34;: \u0026#34;Bachelor of Science\u0026#34;, \u0026#34;graduation_year\u0026#34;: 2024, \u0026#34;image_path\u0026#34;: \u0026#34;degrees/usa/stanford_001.jpg\u0026#34;, \u0026#34;quality_score\u0026#34;: 0.95, \u0026#34;verified\u0026#34;: true } Statistics USA: 450 degrees India: 300 degrees UK: 180 degrees Australia: 150 degrees Others: 320 degrees Applications 1. Document Verification Train ML models to verify degree authenticity\n2. Fraud Detection Identify fake or tampered certificates\n3. Academic Research Study degree formats and university branding\n4. Market Analysis Track educational trends and university popularity\nLessons Learned 1. Source Quality Matters LinkedIn posts provided authentic, high-quality degree images\n2. Global Coverage is Hard Some countries had fewer graduation posts\n3. Privacy Considerations Anonymized student information, used for research only\n4. Documentation is Critical Tracked sources, preprocessing, and validation steps\nImpact This dataset enables:\nAutomated degree verification systems Fraud detection for educational credentials Academic research on document security Market analysis of educational trends Key Takeaway Large-scale data collection from social media requires careful curation, quality control, and ethical considerations.\nThis comprehensive degree dataset supports academic integrity and fraud detection research worldwide.\n","permalink":"http://localhost:1313/post/university-degrees-dataset/","summary":"\u003ch2 id=\"the-goal\"\u003eThe Goal\u003c/h2\u003e\n\u003cp\u003eBuilding systems for \u003cstrong\u003edegree verification\u003c/strong\u003e and \u003cstrong\u003eacademic fraud detection\u003c/strong\u003e requires authentic examples. I collected degrees from LinkedIn graduation posts across 8 countries.\u003c/p\u003e\n\u003ch2 id=\"coverage\"\u003eCoverage\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e1,400 unique degrees\u003c/strong\u003e from:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eUSA\u003c/strong\u003e: Ivy League, top universities, business schools\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIndia\u003c/strong\u003e: IITs, IIMs, AIIMS\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUK\u003c/strong\u003e: Oxford, Cambridge, Russell Group\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAustralia\u003c/strong\u003e: Group of Eight (Go8)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEurope\u003c/strong\u003e: Top institutions (ETH Zurich, TUM, etc.)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAsia\u003c/strong\u003e: NUS, NTU, Seoul National, University of Tokyo\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"collection-method\"\u003eCollection Method\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecollect_graduation_posts\u003c/span\u003e(keywords, universities):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    posts \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e university \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e universities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        search_queries \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003euniversity\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e graduation 2024\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003euniversity\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e degree ceremony\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;graduated from \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003euniversity\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e query \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e search_queries:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            results \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e linkedin_scraper\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esearch_posts(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                query\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003equery,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                limit\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e50\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                date_range\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;2023-2024\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            posts\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eextend(results)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e posts\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"quality-control\"\u003eQuality Control\u003c/h2\u003e\n\u003ch3 id=\"deduplication\"\u003eDeduplication\u003c/h3\u003e\n\u003cp\u003eUsed perceptual hashing to remove duplicates:\u003c/p\u003e","title":"I Collected 1,400 University Degrees to Build a Verification Dataset"},{"content":"Overview Built a comprehensive LinkedIn scraping tool that can extract posts, profiles, and job listings without requiring authentication, using advanced anti-detection techniques.\nFeatures Data Extraction Capabilities Topic-based Posts\nQuery any topic (e.g., \u0026ldquo;LLMs\u0026rdquo;) Configurable number of posts (e.g., 50 recent posts) Dynamic input for flexible scraping User Profiles\nComplete profile information extraction Profile descriptions and details Professional history Company Profiles\nCompany information extraction Business details and descriptions Company posts and updates Job Listings\nExtract jobs for any role Comprehensive job details Company and location information Technical Approach No Authentication Required Google Search Bypass: Opens Google first for search queries Indirect Access: Scrapes via Google search results No LinkedIn Login: Works without account credentials Anti-Detection Mechanisms reCAPTCHA Bypass: Successfully bypasses Google\u0026rsquo;s reCAPTCHA Human-like Behavior: Mimics real user interactions Stealth Mode: Advanced detection avoidance Data Extraction Text Content: Full post and description extraction Images: Downloads embedded images Metadata: Publication dates, engagement metrics Structured Output: Clean, organized data format Technical Implementation Search Strategy: Google search for initial link discovery Link Processing: Opens relevant links automatically Content Extraction: Comprehensive data parsing Multi-format Output: Supports various data formats Use Cases Market research and competitive analysis Professional network monitoring Job market analysis Content aggregation Lead generation Academic research Technologies Used Python Web Scraping Google Search API Selenium WebDriver Beautiful Soup reCAPTCHA Bypass Data Extraction Social Media Mining ","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eBuilt a comprehensive LinkedIn scraping tool that can extract posts, profiles, and job listings without requiring authentication, using advanced anti-detection techniques.\u003c/p\u003e\n\u003ch2 id=\"features\"\u003eFeatures\u003c/h2\u003e\n\u003ch3 id=\"data-extraction-capabilities\"\u003eData Extraction Capabilities\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTopic-based Posts\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eQuery any topic (e.g., \u0026ldquo;LLMs\u0026rdquo;)\u003c/li\u003e\n\u003cli\u003eConfigurable number of posts (e.g., 50 recent posts)\u003c/li\u003e\n\u003cli\u003eDynamic input for flexible scraping\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eUser Profiles\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eComplete profile information extraction\u003c/li\u003e\n\u003cli\u003eProfile descriptions and details\u003c/li\u003e\n\u003cli\u003eProfessional history\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCompany Profiles\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCompany information extraction\u003c/li\u003e\n\u003cli\u003eBusiness details and descriptions\u003c/li\u003e\n\u003cli\u003eCompany posts and updates\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJob Listings\u003c/strong\u003e\u003c/p\u003e","title":"LinkedIn Scraper - Comprehensive Social Media Data Extraction"},{"content":"Overview Leveraged OpenAI\u0026rsquo;s CLIP (Contrastive Language-Image Pre-training) model for building an advanced image moderation pipeline with high accuracy and flexibility.\nClassification System Classes NSFW SEXY NEUTRAL VIOLENCE DRUGS Methodology CLIP uses both vision and text encoders to understand images through natural language descriptions.\nApproach Prompt Engineering Prompts per Class: 60+ text prompts per category Strategy: Diverse descriptive prompts for each class Text Encoder: CLIP\u0026rsquo;s language model generates embeddings Classification: Based on similarity scores between image and text embeddings Scoring Mechanism Similarity Calculation: Vision encoder processes input image Text Matching: Compare against all class prompts Normalization: Sigmoid function on similarity scores Classification: Highest score determines class Model Performance Optimization Techniques Grid Search CV: Systematic hyperparameter exploration Hyperparameter Tuning: Fine-tuned model parameters False Positive Analysis: Analyzed misclassifications per class Results Overall Accuracy: 92% Robust Classification: Effective across all 5 classes Generalization: Strong performance on diverse image types Advantages of CLIP Approach Flexibility: Easy to add new classes with prompts Zero-shot Learning: Can classify without class-specific training Multi-modal Understanding: Leverages both visual and textual understanding Scalable: Simple to expand classification categories Technologies Used OpenAI CLIP Computer Vision Multi-modal AI Prompt Engineering Hyperparameter Tuning Python Image Classification Natural Language Processing ","permalink":"http://localhost:1313/projects/image-moderation-clip/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eLeveraged OpenAI\u0026rsquo;s CLIP (Contrastive Language-Image Pre-training) model for building an advanced image moderation pipeline with high accuracy and flexibility.\u003c/p\u003e\n\u003ch2 id=\"classification-system\"\u003eClassification System\u003c/h2\u003e\n\u003ch3 id=\"classes\"\u003eClasses\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eNSFW\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSEXY\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNEUTRAL\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVIOLENCE\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDRUGS\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"methodology\"\u003eMethodology\u003c/h3\u003e\n\u003cp\u003eCLIP uses both vision and text encoders to understand images through natural language descriptions.\u003c/p\u003e\n\u003ch2 id=\"approach\"\u003eApproach\u003c/h2\u003e\n\u003ch3 id=\"prompt-engineering\"\u003ePrompt Engineering\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePrompts per Class\u003c/strong\u003e: 60+ text prompts per category\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStrategy\u003c/strong\u003e: Diverse descriptive prompts for each class\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eText Encoder\u003c/strong\u003e: CLIP\u0026rsquo;s language model generates embeddings\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eClassification\u003c/strong\u003e: Based on similarity scores between image and text embeddings\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"scoring-mechanism\"\u003eScoring Mechanism\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eSimilarity Calculation\u003c/strong\u003e: Vision encoder processes input image\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eText Matching\u003c/strong\u003e: Compare against all class prompts\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNormalization\u003c/strong\u003e: Sigmoid function on similarity scores\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eClassification\u003c/strong\u003e: Highest score determines class\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"model-performance\"\u003eModel Performance\u003c/h2\u003e\n\u003ch3 id=\"optimization-techniques\"\u003eOptimization Techniques\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGrid Search CV\u003c/strong\u003e: Systematic hyperparameter exploration\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHyperparameter Tuning\u003c/strong\u003e: Fine-tuned model parameters\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFalse Positive Analysis\u003c/strong\u003e: Analyzed misclassifications per class\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"results\"\u003eResults\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eOverall Accuracy\u003c/strong\u003e: 92%\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRobust Classification\u003c/strong\u003e: Effective across all 5 classes\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGeneralization\u003c/strong\u003e: Strong performance on diverse image types\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"advantages-of-clip-approach\"\u003eAdvantages of CLIP Approach\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFlexibility\u003c/strong\u003e: Easy to add new classes with prompts\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eZero-shot Learning\u003c/strong\u003e: Can classify without class-specific training\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-modal Understanding\u003c/strong\u003e: Leverages both visual and textual understanding\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eScalable\u003c/strong\u003e: Simple to expand classification categories\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"technologies-used\"\u003eTechnologies Used\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eOpenAI CLIP\u003c/li\u003e\n\u003cli\u003eComputer Vision\u003c/li\u003e\n\u003cli\u003eMulti-modal AI\u003c/li\u003e\n\u003cli\u003ePrompt Engineering\u003c/li\u003e\n\u003cli\u003eHyperparameter Tuning\u003c/li\u003e\n\u003cli\u003ePython\u003c/li\u003e\n\u003cli\u003eImage Classification\u003c/li\u003e\n\u003cli\u003eNatural Language Processing\u003c/li\u003e\n\u003c/ul\u003e","title":"Image Moderation using OpenAI CLIP"},{"content":"The Challenge Building a content moderation system for a social media app requires training data. Lots of it.\nDip needed to automatically detect:\nNSFW content Violence Drugs Weapons Hateful content Neutral content The question: where do you get 150,000+ labeled images?\nThis was my task at ByteCitadel: collect, curate, and label a massive dataset for Dip\u0026rsquo;s image moderation pipeline.\nThe Plan Six categories, ~30,000 images each:\nViolence (30K) Drugs (30K) NSFW (30K) Neutral (30K) Hateful Memes (30K) Weapons (30K) Total: 180,000 images (we collected extra for quality control)\nData Sources 1. Kaggle Datasets Kaggle has tons of public datasets. I found:\nViolence datasets - From academic papers on conflict detection Drug-related imagery - For medical/research purposes Weapon detection datasets - Security/computer vision research # Download Kaggle datasets import kaggle kaggle.api.dataset_download_files( \u0026#39;tapakah68/violence-detection-dataset\u0026#39;, path=\u0026#39;./datasets/violence/\u0026#39;, unzip=True ) 2. Research Papers Academic papers often release datasets alongside publications:\n# Search for datasets mentioned in papers paper_datasets = [ \u0026#34;Flames-Dataset\u0026#34;, # Violence detection \u0026#34;Drug-Image-Dataset\u0026#34;, # Drug classification \u0026#34;Meme-Classification-Dataset\u0026#34; # Hateful memes ] for dataset in paper_datasets: download_dataset(dataset) 3. Roboflow Roboflow has high-quality, pre-labeled computer vision datasets:\n# Export from Roboflow import roboflow rf = roboflow.Roboflow(api_key=\u0026#34;YOUR_API_KEY\u0026#34;) project = rf.workspace(\u0026#34;workspace\u0026#34;).project(\u0026#34;weapon-detection\u0026#34;) dataset = project.version(1).download(\u0026#34;coco\u0026#34;) 4. Custom Collection For edge cases, I scraped and labeled manually:\n# Custom scraping for underrepresented categories def collect_images(query, num_images=1000): results = [] for image_url in image_search_api.search(query, num_images): download_image(image_url, f\u0026#34;./datasets/{query}/\u0026#34;) results.append(image_url) return results The Curation Process Quality Filtering Not all downloaded images were usable:\ndef filter_image_quality(image_path): # Check file size if os.path.getsize(image_path) \u0026lt; 10 * 1024: # Less than 10KB return False, \u0026#34;Too small\u0026#34; # Verify it\u0026#39;s actually an image try: img = Image.open(image_path) img.verify() except: return False, \u0026#34;Corrupted\u0026#34; # Check resolution img = Image.open(image_path) if img.width \u0026lt; 224 or img.height \u0026lt; 224: return False, \u0026#34;Low resolution\u0026#34; # Check if it\u0026#39;s actually an image (not blank/black) if is_blank_image(img): return False, \u0026#34;Blank image\u0026#34; return True, \u0026#34;Valid\u0026#34; Deduplication Images from different sources often overlapped:\nfrom imagehash import average_hash def remove_duplicates(image_paths): hashes = {} unique_images = [] for img_path in image_paths: # Calculate perceptual hash img = Image.open(img_path) hash_value = average_hash(img) if hash_value not in hashes: hashes[hash_value] = img_path unique_images.append(img_path) else: print(f\u0026#34;Duplicate found: {img_path}\u0026#34;) return unique_images # Found and removed ~15,000 duplicates across the dataset Class Balancing Ensuring equal representation across categories:\n# Check class distribution class_counts = {} for category in categories: count = len(os.listdir(f\u0026#39;./datasets/{category}/\u0026#39;)) class_counts[category] = count print(class_counts) # Expected: ~30K per class # If imbalance \u0026gt; 10%, collect more data for minority classes Labeling Strategy Automated Labeling For well-known datasets, labels were already provided:\n# Process COCO-formatted labels import json with open(\u0026#39;annotations.json\u0026#39;, \u0026#39;r\u0026#39;) as f: coco_data = json.load(f) for annotation in coco_data[\u0026#39;annotations\u0026#39;]: image_id = annotation[\u0026#39;image_id\u0026#39;] category_id = annotation[\u0026#39;category_id\u0026#39;] image_path = f\u0026#39;images/{image_id}.jpg\u0026#39; category = id_to_category[category_id] move_to_category(image_path, category) Manual Labeling For custom-collected images, I had to label manually:\n# Labeling tool for edge cases def label_image(image_path): img = Image.open(image_path) img.show() label = input(f\u0026#34;What category is this image? \u0026#34;) # Categories: violence, drugs, nsfw, neutral, weapons, hateful_memes # Save label save_label(image_path, label) \u0026ldquo;Labeling 10,000 images by hand teaches you to appreciate automated annotation tools.\u0026rdquo;\nQuality Validation Double-checking labels for accuracy:\ndef validate_labels(image_path, expected_label): img = Image.open(image_path) actual_label = manual_label(image_path) if expected_label != actual_label: print(f\u0026#34;MISLABELED: {image_path}\u0026#34;) print(f\u0026#34;Expected: {expected_label}, Got: {actual_label}\u0026#34;) correct_label(expected_label) Challenges 1. Copyright \u0026amp; Legal Issues Using internet images requires care:\nAcademic use only - Educational/research purposes No redistribution - Keep dataset internal Fair use - Transformative use for ML training 2. Cultural Sensitivity What counts as \u0026ldquo;inappropriate\u0026rdquo; varies by culture:\nWestern vs. Indian context Religious imagery Historical vs. contemporary content 3. Data Imbalance Some categories were harder to find:\nWeapons: Plenty of gun images, fewer knives Drugs: Need diverse substances, not just weed Violence: Real violence vs. fictional content 4. False Positives Training on misleading data:\nPhotoshoot violence vs. real violence Medical imagery (drugs for legitimate use) Art depicting weapons (not real weapons) The Final Dataset Statistics Category Images Duplicates Removed Final Count Violence 32,000 2,000 30,000 Drugs 35,000 5,000 30,000 NSFW 38,000 8,000 30,000 Neutral 31,000 1,000 30,000 Hateful Memes 29,000 0 29,000 Weapons 33,000 3,000 30,000 Total: 198,000 → 179,000 (after quality filtering)\nData Structure dataset/ ├── violence/ │ ├── img_00001.jpg │ ├── img_00002.jpg │ └── ... ├── drugs/ ├── nsfw/ ├── neutral/ ├── hateful_memes/ └── weapons/ Each image has a corresponding metadata file:\n{ \u0026#34;image_id\u0026#34;: \u0026#34;img_00001\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;violence\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;kaggle_violence_dataset\u0026#34;, \u0026#34;resolution\u0026#34;: \u0026#34;1920x1080\u0026#34;, \u0026#34;file_size\u0026#34;: \u0026#34;245KB\u0026#34;, \u0026#34;date_collected\u0026#34;: \u0026#34;2024-09-15\u0026#34;, \u0026#34;verified\u0026#34;: true, \u0026#34;quality_score\u0026#34;: 0.95 } Model Training Results With this dataset, Dip\u0026rsquo;s content moderation model achieved:\n92% accuracy across all categories 94% precision for violence detection 89% precision for NSFW detection \u0026lt;3% false positive rate on neutral images # Train multi-class classifier from torchvision import models import torch model = models.resnet50(pretrained=True) model.fc = torch.nn.Linear(2048, 6) # 6 categories # Train with data augmentation train_loader = DataLoader(dataset, batch_size=32, shuffle=True) for epoch in range(10): for batch in train_loader: images, labels = batch outputs = model(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() Lessons Learned 1. Quality Over Quantity 30K high-quality images \u0026gt; 100K mediocre ones\n2. Source Diversity Matters Using multiple sources prevents model bias\n3. Documentation is Critical Track:\nData sources Preprocessing steps Quality checks Labeling methodology 4. Expect Legal Complexity Using internet images requires legal review\n5. Human Validation is Essential Automated filters catch obvious cases, but humans catch edge cases\nKey Takeaways Data collection is 80% of ML - Garbage in, garbage out Multiple sources reduce bias and improve generalization Quality filtering is as important as data collection Legal considerations matter for production systems Document everything for reproducibility This dataset became the foundation for Dip\u0026rsquo;s image moderation system, processing millions of images daily and keeping the platform safe for users.\nThe 179,000 images across 6 categories now power real-time content moderation for thousands of users. It\u0026rsquo;s a reminder that machine learning is only as good as your training data.\nThis dataset collection was part of building Dip\u0026rsquo;s content moderation pipeline. If you need help with dataset creation or content moderation, feel free to reach out.\n","permalink":"http://localhost:1313/post/nsfw-moderation-dataset/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBuilding a content moderation system for a social media app requires \u003cstrong\u003etraining data\u003c/strong\u003e. Lots of it.\u003c/p\u003e\n\u003cp\u003eDip needed to automatically detect:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNSFW content\u003c/li\u003e\n\u003cli\u003eViolence\u003c/li\u003e\n\u003cli\u003eDrugs\u003c/li\u003e\n\u003cli\u003eWeapons\u003c/li\u003e\n\u003cli\u003eHateful content\u003c/li\u003e\n\u003cli\u003eNeutral content\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe question: \u003cstrong\u003ewhere do you get 150,000+ labeled images?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis was my task at ByteCitadel: collect, curate, and label a massive dataset for Dip\u0026rsquo;s image moderation pipeline.\u003c/p\u003e\n\u003ch2 id=\"the-plan\"\u003eThe Plan\u003c/h2\u003e\n\u003cp\u003eSix categories, ~30,000 images each:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eViolence\u003c/strong\u003e (30K)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDrugs\u003c/strong\u003e (30K)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNSFW\u003c/strong\u003e (30K)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNeutral\u003c/strong\u003e (30K)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHateful Memes\u003c/strong\u003e (30K)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWeapons\u003c/strong\u003e (30K)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTotal: \u003cstrong\u003e180,000 images\u003c/strong\u003e (we collected extra for quality control)\u003c/p\u003e","title":"I Collected 150,000+ Images to Train a Content Moderation Model"},{"content":"The Problem Content moderation at scale is hard. You need to process thousands of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the model was taking 250ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nThe standard approach didn\u0026rsquo;t work for us. We tried everything—batch processing, async queues, caching. But eventually, we realized we needed to optimize the model itself.\nThe Dual-Model Approach Our pipeline used two models working together:\nNudeNet for initial NSFW detection MobileNet for secondary classification Both models achieved 97% accuracy, which is great. But they were too slow. And since this was for a consumer app, we needed CPU inference—we couldn\u0026rsquo;t rely on users having GPUs.\nThe Optimization Strategy I tried three approaches:\n1. PyTorch → ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\n# Convert PyTorch to ONNX torch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: ~10% speedup. Nice, but not enough.\n2. ONNX → OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another ~15% speedup. We\u0026rsquo;re getting somewhere!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to FP16 (and sometimes INT8) Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Another ~15% speedup!\nThe Numbers Metric Before After Improvement Inference Time 250ms 180ms 28% faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~200ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nI was genuinely surprised. When you optimize properly, you can often beat official benchmarks. The key is testing with your actual workload, not just running synthetic benchmarks.\nThe Technical Details Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter The accuracy impact was \u0026lt;0.1%, which is negligible for content moderation use cases.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Real-World Testing We tested on a 10,000 image dataset:\nIntel Xeon CPU (production-like environment) 180ms ± 5ms across all runs 97.3% accuracy maintained Lessons Learned 1. Hardware-Specific Optimization Matters Different hardware requires different optimization strategies:\nIntel CPUs → OpenVINO NVIDIA GPUs → TensorRT Apple Silicon → Core ML ARM devices → TFLite 2. Don\u0026rsquo;t Trust Benchmarks Blindly Official benchmarks are great starting points, but your mileage will vary. Test with your:\nActual data distribution Real hardware constraints Production environment variables 3. The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch → ONNX → OpenVINO) Quantization (FP32 → FP16) These two steps gave us 85% of our performance improvement.\n4. Trade-offs are Manageable 28% faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments.\nCode Implementation The final pipeline was beautifully simple:\nimport openvino as ov # Load OpenVINO model core = ov.Core() model = core.compile_model(\u0026#34;openvino_model.xml\u0026#34;, \u0026#34;CPU\u0026#34;) # Run inference results = model(input_image) # Post-process results nsfw_score = results[0][1] # Confidence for NSFW class if nsfw_score \u0026gt; 0.8: return {\u0026#34;nsfw\u0026#34;: True, \u0026#34;confidence\u0026#34;: nsfw_score} That\u0026rsquo;s it. Three lines of code to load and run the optimized model.\nBusiness Impact For Dip App Real-time moderation: No lag in content upload Better UX: Users don\u0026rsquo;t wait for processing Scalability: Handle 10x more images per server For User Experience Faster uploads: Content appears instantly Better performance: App feels snappier Lower battery drain: More efficient CPU usage The Bigger Picture This optimization work taught me that model performance isn\u0026rsquo;t just about model architecture. It\u0026rsquo;s about the entire inference pipeline:\nModel format (PyTorch → ONNX → OpenVINO) Optimization level (FP32 → FP16 → INT8) Hardware acceleration (CPU-specific optimizations) Graph optimization (removing unnecessary operations) By optimizing each layer, we achieved production-ready performance without sacrificing accuracy.\nKey Takeaways Always profile first - Don\u0026rsquo;t optimize blindly Test on target hardware - Desktop benchmarks ≠ production reality Hardware-specific toolchains matter immensely Simple optimizations often yield the biggest gains Measure real-world impact - Not just synthetic benchmarks The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eContent moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process thousands of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the model was taking \u003cstrong\u003e250ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made NSFW Detection 28% Faster and Beat Ultralytics' Own Benchmark"},{"content":"The Problem Event platforms like BookMyShow are full of fake listings. Users waste money on non-existent events. Platforms need automated verification.\nThe solution: compare claimed event details with actual page content using fuzzy string matching.\nThe Approach from fuzzywuzzy import fuzz def verify_event_credibility(claimed_info, actual_content): scores = {} # Compare titles title_score = fuzz.partial_ratio( claimed_info[\u0026#39;title\u0026#39;].lower(), actual_content[\u0026#39;title\u0026#39;].lower() ) # Compare descriptions desc_score = fuzz.token_sort_ratio( claimed_info[\u0026#39;description\u0026#39;], actual_content[\u0026#39;description\u0026#39;] ) # Overall credibility score overall_score = (title_score * 0.3 + desc_score * 0.4 + date_score * 0.2 + location_score * 0.1) return { \u0026#39;overall_score\u0026#39;: overall_score, \u0026#39;credible\u0026#39;: overall_score \u0026gt; 75 } Results 5,000 events tested 92% accuracy 70% reduction in fake event complaints Key Takeaway Simple fuzzy matching can solve complex fraud detection problems.\nThis system was developed to combat event fraud across multiple event listing platforms.\n","permalink":"http://localhost:1313/post/event-verifier-crawler/","summary":"\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eEvent platforms like BookMyShow are full of fake listings. Users waste money on non-existent events. Platforms need automated verification.\u003c/p\u003e\n\u003cp\u003eThe solution: \u003cstrong\u003ecompare claimed event details with actual page content using fuzzy string matching\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e fuzzywuzzy \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e fuzz\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003everify_event_credibility\u003c/span\u003e(claimed_info, actual_content):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    scores \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Compare titles\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    title_score \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e fuzz\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epartial_ratio(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        claimed_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;title\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elower(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        actual_content[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;title\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elower()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Compare descriptions\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    desc_score \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e fuzz\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etoken_sort_ratio(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        claimed_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;description\u0026#39;\u003c/span\u003e],\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        actual_content[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;description\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Overall credibility score\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    overall_score \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (title_score \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.3\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e desc_score \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.4\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    date_score \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.2\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e location_score \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.1\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;overall_score\u0026#39;\u003c/span\u003e: overall_score,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;credible\u0026#39;\u003c/span\u003e: overall_score \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e75\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e5,000 events tested\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e92% accuracy\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e70% reduction\u003c/strong\u003e in fake event complaints\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"key-takeaway\"\u003eKey Takeaway\u003c/h2\u003e\n\u003cp\u003eSimple fuzzy matching can solve complex fraud detection problems.\u003c/p\u003e","title":"I Built a Bot to Catch Fake Event Listings with Fuzzy Matching"},{"content":"The Mission College students in India struggle to find affordable PGs and hostels. I scraped data from three major property sites to build a comprehensive database.\nThe Stack class PropertyScraper: def __init__(self): self.scrapers = { \u0026#39;magicbricks\u0026#39;: MagicBricksScraper(), \u0026#39;justdial\u0026#39;: JustDialScraper(), \u0026#39;99acres\u0026#39;: NinetyNineAcresScraper() } Each platform needed custom scraping logic.\nThe Numbers Platform Properties Success Rate Magic Bricks 28,500 94% JustDial 25,000 89% 99acres 16,500 91% Total 70,000 91% Challenges Anti-bot measures: CAPTCHA, IP blocking, rate limiting Solution: Rotated user agents, added delays, proxy rotation\nDynamic content: JavaScript-rendered listings Solution: Selenium WebDriver with scroll handling\nData inconsistency: Different formats across platforms Solution: Normalization pipeline\nImpact 70,000 properties across 50 cities Student-focused filtering (budget, amenities, location) Platform integration for easy search Key Takeaway Web scraping at scale requires platform-specific solutions and robust anti-detection strategies.\nThis property data collection project was built to help students find affordable and suitable accommodation near their colleges.\n","permalink":"http://localhost:1313/post/property-scraper-analysis/","summary":"\u003ch2 id=\"the-mission\"\u003eThe Mission\u003c/h2\u003e\n\u003cp\u003eCollege students in India struggle to find affordable PGs and hostels. I scraped data from three major property sites to build a comprehensive database.\u003c/p\u003e\n\u003ch2 id=\"the-stack\"\u003eThe Stack\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ePropertyScraper\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e__init__\u003c/span\u003e(self):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003escrapers \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;magicbricks\u0026#39;\u003c/span\u003e: MagicBricksScraper(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;justdial\u0026#39;\u003c/span\u003e: JustDialScraper(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;99acres\u0026#39;\u003c/span\u003e: NinetyNineAcresScraper()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eEach platform needed custom scraping logic.\u003c/p\u003e\n\u003ch2 id=\"the-numbers\"\u003eThe Numbers\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003ePlatform\u003c/th\u003e\n          \u003cth\u003eProperties\u003c/th\u003e\n          \u003cth\u003eSuccess Rate\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eMagic Bricks\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e28,500\u003c/td\u003e\n          \u003ctd\u003e94%\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eJustDial\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e25,000\u003c/td\u003e\n          \u003ctd\u003e89%\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e99acres\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e16,500\u003c/td\u003e\n          \u003ctd\u003e91%\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eTotal\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003e70,000\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003e91%\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"challenges\"\u003eChallenges\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eAnti-bot measures\u003c/strong\u003e: CAPTCHA, IP blocking, rate limiting\n\u003cstrong\u003eSolution\u003c/strong\u003e: Rotated user agents, added delays, proxy rotation\u003c/p\u003e","title":"I Scraped 70K Property Listings to Help Students Find Housing"},{"content":"Overview Built a multi-stage pipeline for verifying student ID cards using advanced deep learning models to detect various types of forgery and tampering.\nPipeline Architecture Test Dataset 97 manually collected and cleaned ID card images Diverse ID card formats and designs Model Components 1. Face Detection (YOLOv8) High-accuracy face detection in ID cards Fast inference for real-time processing 2. Gender Validation DeepFace-based gender classification Accuracy: 80% (with potential for improvement via dedicated training) 3. Splicing Detection Model: Image Splicing Detector (image_splicer_quant.tflite) Architecture: DenseNet121 with ELA preprocessing Training Data: CASIA dataset Preprocessing: Error Level Analysis (ELA) Performance: Train accuracy: 97.35% Validation accuracy: 82.31% Output: Splicing score (threshold: 0.90) Average test score: 0.95 4. Copy-Move Detection Model: New UNet (new_unet.tflite) Training: COMOFOD dataset Training: 30 epochs Accuracy: 98.7% Output: Copy-move score (threshold: 0.22) Average test score: 0.19 OCR \u0026amp; Text Validation File: speed_uppp2.py Features: OCR text extraction Text consistency check Keyword matching (case/special character validation) Keyword match percentage calculation ELA analysis Pipeline Flow Face Detection (FaceDetection_YOLO.py) Gender Validation (gender_validation.py) Splicing Detection (splicing_detector.py) Copy-Move Detection (copymove_detector.py) OCR \u0026amp; Text Analysis (speed_uppp2.py) Validation Results Successfully processes diverse ID card formats Robust detection of multiple forgery types Multi-layer validation ensures high accuracy Real-time processing capability Technologies Used YOLOv8 TensorFlow Lite UNet DenseNet121 DeepFace OCR (Optical Character Recognition) Error Level Analysis (ELA) Python Computer Vision ","permalink":"http://localhost:1313/projects/student-id-verification-pipeline/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eBuilt a multi-stage pipeline for verifying student ID cards using advanced deep learning models to detect various types of forgery and tampering.\u003c/p\u003e\n\u003ch2 id=\"pipeline-architecture\"\u003ePipeline Architecture\u003c/h2\u003e\n\u003ch3 id=\"test-dataset\"\u003eTest Dataset\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e97 manually collected and cleaned ID card images\u003c/li\u003e\n\u003cli\u003eDiverse ID card formats and designs\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"model-components\"\u003eModel Components\u003c/h3\u003e\n\u003ch4 id=\"1-face-detection-yolov8\"\u003e1. Face Detection (YOLOv8)\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eHigh-accuracy face detection in ID cards\u003c/li\u003e\n\u003cli\u003eFast inference for real-time processing\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"2-gender-validation\"\u003e2. Gender Validation\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eDeepFace-based gender classification\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAccuracy\u003c/strong\u003e: 80% (with potential for improvement via dedicated training)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"3-splicing-detection\"\u003e3. Splicing Detection\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: Image Splicing Detector (image_splicer_quant.tflite)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eArchitecture\u003c/strong\u003e: DenseNet121 with ELA preprocessing\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraining Data\u003c/strong\u003e: CASIA dataset\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePreprocessing\u003c/strong\u003e: Error Level Analysis (ELA)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePerformance\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eTrain accuracy: 97.35%\u003c/li\u003e\n\u003cli\u003eValidation accuracy: 82.31%\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOutput\u003c/strong\u003e: Splicing score (threshold: 0.90)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAverage test score\u003c/strong\u003e: 0.95\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"4-copy-move-detection\"\u003e4. Copy-Move Detection\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: New UNet (new_unet.tflite)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraining\u003c/strong\u003e: COMOFOD dataset\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraining\u003c/strong\u003e: 30 epochs\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAccuracy\u003c/strong\u003e: 98.7%\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOutput\u003c/strong\u003e: Copy-move score (threshold: 0.22)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAverage test score\u003c/strong\u003e: 0.19\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"ocr--text-validation\"\u003eOCR \u0026amp; Text Validation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFile\u003c/strong\u003e: speed_uppp2.py\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFeatures\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eOCR text extraction\u003c/li\u003e\n\u003cli\u003eText consistency check\u003c/li\u003e\n\u003cli\u003eKeyword matching (case/special character validation)\u003c/li\u003e\n\u003cli\u003eKeyword match percentage calculation\u003c/li\u003e\n\u003cli\u003eELA analysis\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"pipeline-flow\"\u003ePipeline Flow\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eFace Detection\u003c/strong\u003e (FaceDetection_YOLO.py)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGender Validation\u003c/strong\u003e (gender_validation.py)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSplicing Detection\u003c/strong\u003e (splicing_detector.py)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCopy-Move Detection\u003c/strong\u003e (copymove_detector.py)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOCR \u0026amp; Text Analysis\u003c/strong\u003e (speed_uppp2.py)\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"validation-results\"\u003eValidation Results\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSuccessfully processes diverse ID card formats\u003c/li\u003e\n\u003cli\u003eRobust detection of multiple forgery types\u003c/li\u003e\n\u003cli\u003eMulti-layer validation ensures high accuracy\u003c/li\u003e\n\u003cli\u003eReal-time processing capability\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"technologies-used\"\u003eTechnologies Used\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eYOLOv8\u003c/li\u003e\n\u003cli\u003eTensorFlow Lite\u003c/li\u003e\n\u003cli\u003eUNet\u003c/li\u003e\n\u003cli\u003eDenseNet121\u003c/li\u003e\n\u003cli\u003eDeepFace\u003c/li\u003e\n\u003cli\u003eOCR (Optical Character Recognition)\u003c/li\u003e\n\u003cli\u003eError Level Analysis (ELA)\u003c/li\u003e\n\u003cli\u003ePython\u003c/li\u003e\n\u003cli\u003eComputer Vision\u003c/li\u003e\n\u003c/ul\u003e","title":"Student ID Verification Pipeline for dip"},{"content":"Overview Developed an interactive data visualization dashboard from scratch to analyze student-focused businesses around colleges and universities across major Indian cities.\nDataset Source: Scraped from Google Maps Size: 3 GB of comprehensive business data Coverage: Ahmedabad, Gandhinagar, Mumbai, Pune, Kolkata, Hyderabad, Bangalore Content: All business information available on Google Maps Data Pipeline Scraping: Automated data collection from Google Maps Cleaning: Data preprocessing and validation Feature Extraction: Relevant business attributes Data Visualization: Interactive charts and maps Dashboard Features Interactive Visualizations ScatterMapBox\nCity-wise filtering Clickable pincode bubbles Geographic distribution of businesses Sunburst Chart\nUpdates based on selected pincode Shows business categories and quantities Hierarchical data representation Histogram\nTop 50 pincodes per city Based on number of businesses Clickable to filter other visualizations Data Table\nDetailed business listings Category filtering Sortable columns Tree Map\nBusiness category distribution City-wise analysis Download Feature Users can download complete business data for any selected pincode\nPerformance Optimization Initial Challenge Problem: 90+ second loading time Cause: Large 3GB dataset with inefficient processing Optimization Techniques Flask Caching: Implemented intelligent caching strategies CSV to Parquet Conversion: More efficient file format Data Type Optimization: Reduced memory footprint Query Optimization: Faster data access patterns Result Before: 90+ seconds After: 1 second Improvement: 90x speedup Technologies Used Python Dash by Plotly Flask Pandas Data Processing Interactive Visualization Caching ","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eDeveloped an interactive data visualization dashboard from scratch to analyze student-focused businesses around colleges and universities across major Indian cities.\u003c/p\u003e\n\u003ch2 id=\"dataset\"\u003eDataset\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSource\u003c/strong\u003e: Scraped from Google Maps\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSize\u003c/strong\u003e: 3 GB of comprehensive business data\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCoverage\u003c/strong\u003e: Ahmedabad, Gandhinagar, Mumbai, Pune, Kolkata, Hyderabad, Bangalore\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eContent\u003c/strong\u003e: All business information available on Google Maps\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"data-pipeline\"\u003eData Pipeline\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eScraping\u003c/strong\u003e: Automated data collection from Google Maps\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCleaning\u003c/strong\u003e: Data preprocessing and validation\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFeature Extraction\u003c/strong\u003e: Relevant business attributes\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Visualization\u003c/strong\u003e: Interactive charts and maps\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"dashboard-features\"\u003eDashboard Features\u003c/h2\u003e\n\u003ch3 id=\"interactive-visualizations\"\u003eInteractive Visualizations\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eScatterMapBox\u003c/strong\u003e\u003c/p\u003e","title":"College Hotspots Dashboard - Interactive Data Visualization"},{"content":"The Question Can you tell crocodiles apart? Probably not—unless you\u0026rsquo;re a wildlife biologist with decades of experience.\nBut what if I told you that every mugger crocodile has a unique back scute pattern (like human fingerprints)? And what if we could build a computer vision model to automatically identify individual crocodiles from drone footage?\nThat\u0026rsquo;s exactly what I spent 6 months doing as a research intern at Ahmedabad University under Prof. Mehul Raval.\nThe Challenge Wildlife conservation is data-starved. Tracking individual animals requires:\nExpensive GPS collars Manual photo identification Lots of time and expertise Frequent human error \u0026ldquo;160+ individual crocodiles, 1000 images each = 500GB of data\u0026rdquo;\nWe had UAV (drone) imagery from Gujarat\u0026rsquo;s western and southern regions. The footage was stunning—but analyzing it manually would take years.\nThe Dataset This wasn\u0026rsquo;t your typical image classification dataset:\n160 individual crocodiles (each with unique ID) 1,000 images per crocodile 500GB total of high-resolution drone footage Gujarat, India - Western and Southern coastal regions Each crocodile\u0026rsquo;s back scute pattern was annotated by wildlife experts. Think of it like facial recognition, but for crocodile backs.\nThe Infrastructure Problem Training on 500GB of data requires serious compute:\nParam Shavak Supercomputer - A Linux-based HPC system No GUI - Command-line only NVIDIA P5000 GPUs - Multiple multi-day training runs Custom CLI scripts - For data processing Since it was a research project with limited resources, I had to optimize everything. No room for inefficient code.\nThe Model Architecture Why YOLOv8? YOLOv8 (You Only Look Once version 8) is great for this use case:\nReal-time detection - Process video streams High accuracy - State-of-the-art performance Multiple sizes - We used YOLOv8-XL for best results # Load YOLOv8-XL model model = YOLO(\u0026#39;yolov8x.pt\u0026#39;) # Train on crocodile dataset model.train( data=\u0026#39;crocodile.yaml\u0026#39;, epochs=100, imgsz=640, batch_size=16, device=\u0026#39;0,1,2,3\u0026#39; # 4 GPUs ) The Training Process Training took 72 hours per run on 4 P5000 GPUs:\nData preprocessing - Custom scripts for massive dataset Hyperparameter tuning - Grid search across multiple runs Threshold optimization - 0.82 was the sweet spot Multi-day training - With checkpointing The model learned to:\nDetect crocodiles in images Classify them into individual IDs (160 classes) Handle varying lighting/weather conditions The Results Performance Metrics Metric Value mAP (Mean Average Precision) 98.50% Threshold 0.82 Training Time 72 hours (4 GPUs) Inference Speed ~50ms per image Visual Examples The model correctly identified:\nCrocodiles in various poses Different lighting conditions Partial occlusions Water reflections \u0026ldquo;98.5% accuracy on 160 individual crocodiles = better than human experts\u0026rdquo;\nTechnical Challenges 1. Massive Dataset Management 500GB of images doesn\u0026rsquo;t fit in RAM. I built a streaming data loader:\nclass CrocodileDataset(Dataset): def __init__(self, image_paths, labels): self.image_paths = image_paths self.labels = labels def __getitem__(self, idx): # Stream from disk (no loading all into memory) image = self.load_image(idx) label = self.labels[idx] return image, label 2. Linux-Only Environment The Param Shavak supercomputer had no GUI. Everything had to be command-line:\n# Submit training job qsub -l gpu=4 -N crocodile_training train_model.sh # Monitor progress tail -f training.log I couldn\u0026rsquo;t use Jupyter notebooks or PyCharm. Just Vim and command-line tools.\n3. Hyperparameter Tuning With 160 classes and limited compute time, I had to be smart:\n# Parameter grid (tested iteratively) param_grid = { \u0026#39;learning_rate\u0026#39;: [0.001, 0.01, 0.1], \u0026#39;batch_size\u0026#39;: [8, 16, 32], \u0026#39;optimizer\u0026#39;: [\u0026#39;SGD\u0026#39;, \u0026#39;Adam\u0026#39;], \u0026#39;weight_decay\u0026#39;: [0.0001, 0.001, 0.01] } 4. Threshold Optimization Finding the right confidence threshold is crucial:\n# True Positive/Negative rates at different thresholds thresholds = np.arange(0.5, 0.95, 0.05) for t in thresholds: tp, tn, fp, fn = calculate_confusion_matrix(predictions, t) tpr = tp / (tp + fn) # True Positive Rate tnr = tn / (tn + fp) # True Negative Rate The 0.82 threshold maximized TPR/TNR balance for wildlife monitoring.\nReal-World Applications This model isn\u0026rsquo;t just academic—it has concrete uses:\n1. Behavioral Studies Track individual crocodile movements Understand migration patterns Study social behaviors 2. Population Monitoring Count populations without capture Estimate survival rates Track breeding success 3. Conservation Impact Monitor endangered species Assess habitat health Guide protection efforts 4. Research Applications Non-invasive monitoring Automated tracking systems Long-term studies Lessons Learned 1. Domain Expertise Matters The wildlife biologists\u0026rsquo; annotation was invaluable. They knew:\nWhich back scutes are unique identifiers How to handle ambiguous cases The behavioral context behind patterns \u0026ldquo;Computer vision is only as good as your training data\u0026rdquo;\n2. Infrastructure Limitations Linux-only environments force you to:\nMaster command-line tools Write robust, reproducible code Think carefully about resources 3. Multi-Day Training Requires Patience 72-hour training runs mean:\nBugs are expensive Checkpointing is critical You can\u0026rsquo;t iterate quickly 4. Conservation Needs Practical Solutions Academic models need to be:\nDeployable in field conditions Robust to real-world variations Useful for actual conservation The Bigger Picture This project showed me that computer vision can make a real difference in conservation. We\u0026rsquo;re not just building models for fun—we\u0026rsquo;re building tools that help protect endangered species.\nThe 98.5% accuracy means wildlife researchers can:\nReplace manual identification (which takes hours) Process massive datasets automatically Track populations at scale Key Takeaways Deep learning + conservation = powerful combination Infrastructure constraints push you to be creative Domain expertise is irreplaceable Real-world deployment needs practical engineering Conservation impact makes technical challenges worthwhile This project taught me that computer vision isn\u0026rsquo;t just for tech companies. It\u0026rsquo;s a tool that can help solve some of the world\u0026rsquo;s most pressing environmental challenges.\nThe 98.5% accuracy on 160 individual crocodiles proves that with enough data and the right approach, we can build models that rival expert human performance—while being faster, more consistent, and more scalable.\nThis research was conducted at Ahmedabad University under Prof. Mehul Raval. If you\u0026rsquo;re interested in conservation technology or need help with computer vision projects, feel free to reach out.\n","permalink":"http://localhost:1313/post/crocodile-identification-yolov8/","summary":"\u003ch2 id=\"the-question\"\u003eThe Question\u003c/h2\u003e\n\u003cp\u003eCan you tell crocodiles apart? Probably not—unless you\u0026rsquo;re a wildlife biologist with decades of experience.\u003c/p\u003e\n\u003cp\u003eBut what if I told you that \u003cstrong\u003eevery mugger crocodile has a unique back scute pattern\u003c/strong\u003e (like human fingerprints)? And what if we could build a computer vision model to automatically identify individual crocodiles from drone footage?\u003c/p\u003e\n\u003cp\u003eThat\u0026rsquo;s exactly what I spent 6 months doing as a research intern at \u003cstrong\u003eAhmedabad University\u003c/strong\u003e under Prof. Mehul Raval.\u003c/p\u003e","title":"I Trained a Model to Identify 160+ Individual Crocodiles with 98.5% Accuracy"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"Overview Successfully scraped BookMyShow\u0026rsquo;s highly secured platform, extracting comprehensive event data across all Indian cities and categories.\nChallenge BookMyShow employs multiple anti-scraping measures:\nAdvanced bot detection No direct API access Dynamic content loading Security against automated scraping Solution Core Technologies Selenium: Browser automation with human-like interactions Beautiful Soup: HTML parsing and data extraction Multithreading: Parallel processing for efficiency Coverage Cities: 1,898 cities across India Categories: Plays Sports Events Activities Anti-Detection Mechanisms Human-like scrolling patterns Randomized delays Session management Performance Optimization Challenge: Default single-threaded approach was too slow Solution: Multithreading approach Problem: Threads running in background were blocked Innovation: Windows multi-window layout\nRunning 4 simultaneous windows in foreground Result: 60x performance improvement Key Features Complete city coverage (1,898 cities) All event categories Bypass anti-bot detection 60x optimized performance Robust error handling Technologies Used Python Selenium WebDriver Beautiful Soup Multithreading Windows GUI Automation HTML Parsing ","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eSuccessfully scraped BookMyShow\u0026rsquo;s highly secured platform, extracting comprehensive event data across all Indian cities and categories.\u003c/p\u003e\n\u003ch2 id=\"challenge\"\u003eChallenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow employs multiple anti-scraping measures:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eNo direct API access\u003c/li\u003e\n\u003cli\u003eDynamic content loading\u003c/li\u003e\n\u003cli\u003eSecurity against automated scraping\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"solution\"\u003eSolution\u003c/h2\u003e\n\u003ch3 id=\"core-technologies\"\u003eCore Technologies\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelenium\u003c/strong\u003e: Browser automation with human-like interactions\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBeautiful Soup\u003c/strong\u003e: HTML parsing and data extraction\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMultithreading\u003c/strong\u003e: Parallel processing for efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"coverage\"\u003eCoverage\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCities\u003c/strong\u003e: 1,898 cities across India\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCategories\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003ePlays\u003c/li\u003e\n\u003cli\u003eSports\u003c/li\u003e\n\u003cli\u003eEvents\u003c/li\u003e\n\u003cli\u003eActivities\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"anti-detection-mechanisms\"\u003eAnti-Detection Mechanisms\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eHuman-like scrolling patterns\u003c/li\u003e\n\u003cli\u003eRandomized delays\u003c/li\u003e\n\u003cli\u003eSession management\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"performance-optimization\"\u003ePerformance Optimization\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eChallenge\u003c/strong\u003e: Default single-threaded approach was too slow\n\u003cstrong\u003eSolution\u003c/strong\u003e: Multithreading approach\n\u003cstrong\u003eProblem\u003c/strong\u003e: Threads running in background were blocked\n\u003cstrong\u003eInnovation\u003c/strong\u003e: Windows multi-window layout\u003c/p\u003e","title":"BookMyShow Scraper - High-Performance Web Scraping"},{"content":"Day One at ByteCitadel I showed up to my first day at ByteCitadel expecting to work on some fancy deep learning model. Instead, I got assigned to:\n\u0026ldquo;Go clean these 20,000 confessions. We need them for content moderation training.\u0026rdquo;\nThis was my first day as a data science intern. I thought I\u0026rsquo;d be doing computer vision or NLP. Instead, I learned the unglamorous truth about machine learning:\n\u0026ldquo;80% of ML is data cleaning. The other 20% is cleaning data.\u0026rdquo;\nThe App: dip: life beyond colleges dip is a social media platform where college students share anonymous confessions. Think Yik Yak but for Indian colleges.\nThe problem: students were posting problematic content (harassment, hate speech, personal info), but dip needed automated moderation. That requires training data.\nThe Dataset 20,000+ raw confession entries. Sounds manageable, right?\nNot when you see what \u0026ldquo;raw data\u0026rdquo; actually means:\nRaw Confession 1: \u0026#34;Hey everyone!!! :D :P i want to share something lol...\u0026#34; Raw Confession 2: \u0026lt;P\u0026gt; this is html text scraped from some forum \u0026lt;/p\u0026gt; Raw Confession 3: ASDFGHJKL PLEASE HELP IM IN NEED OF GUIDANCE about a girl... Raw Confession 4: \u0026lt;BODY\u0026gt; MALICIOUS CODE INJECTION ATTEMPT \u0026lt;/BODY\u0026gt; This was a mess. I needed to turn this chaos into something useful.\nThe Cleaning Pipeline Step 1: Text Normalization First, I removed all the junk:\ndef clean_confession(text): # Remove HTML tags text = re.sub(r\u0026#39;\u0026lt;[^\u0026gt;]+\u0026gt;\u0026#39;, \u0026#39;\u0026#39;, text) # Normalize whitespace text = re.sub(r\u0026#39;\\s+\u0026#39;, \u0026#39; \u0026#39;, text).strip() # Handle emoji representations text = text.replace(\u0026#39; :)\u0026#39;, \u0026#39; 😊\u0026#39;).replace(\u0026#39; :D\u0026#39;, \u0026#39; 😃\u0026#39;) # Remove extra punctuation text = re.sub(r\u0026#39;[!]{2,}\u0026#39;, \u0026#39;!\u0026#39;, text) return text.lower() Before: \u0026quot;Hey everyone!!! :D :P i want 2 share something lol...\u0026quot; After: \u0026quot;hey everyone! i want to share something lol\u0026quot;\nStep 2: Duplicate Detection Some confessions were posted multiple times:\nfrom fuzzywuzzy import fuzz def find_duplicates(confessions): duplicates = [] for i, conf in enumerate(confessions): for j, other_conf in enumerate(confessions[i+1:], i+1): similarity = fuzz.ratio(conf[\u0026#39;text\u0026#39;], other_conf[\u0026#39;text\u0026#39;]) if similarity \u0026gt; 90: duplicates.append((i, j, similarity)) return duplicates Found 3.2% duplicates across 20K confessions. Not bad.\nStep 3: Quality Filtering Not all confessions were usable:\ndef meets_quality_criteria(text): # Minimum length: 20 characters if len(text) \u0026lt; 20: return False, \u0026#34;Too short\u0026#34; # Maximum length: 5,000 characters if len(text) \u0026gt; 5000: return False, \u0026#34;Too long\u0026#34; # Check for spam patterns spam_patterns = [\u0026#39;click here\u0026#39;, \u0026#39;visit my channel\u0026#39;, \u0026#39;subscribe\u0026#39;] if any(pattern in text.lower() for pattern in spam_patterns): return False, \u0026#34;Spam detected\u0026#34; # Remove test data test_patterns = [\u0026#39;lorem ipsum\u0026#39;, \u0026#39;test data\u0026#39;, \u0026#39;sample text\u0026#39;] if any(pattern in text.lower() for pattern in test_patterns): return False, \u0026#34;Test data\u0026#34; return True, \u0026#34;Valid\u0026#34; Results:\n5.8% removed (too short) 1.2% removed (spam/test data) 93% retention rate - Good enough to proceed The Labeling Strategy Now for the fun part: what category is this confession?\nWe needed to label each confession for the content moderation model:\nSafe (68%) - General sharing, advice, experiences Needs Review (18%) - Ambiguous content requiring human review Inappropriate (8%) - Violates community guidelines Personal Info (4%) - Contains contact details, addresses Promotional (2%) - Spam or promotional content Automated First Pass I built rules for obvious cases:\ndef auto_label(confession): text = confession.lower() # Personal info patterns if re.search(r\u0026#39;\\b\\d{10,}\\b\u0026#39;, text): # Phone numbers return \u0026#34;personal_info\u0026#34; if re.search(r\u0026#39;\\b[\\w.-]+@[\\w.-]+\\.\\w+\\b\u0026#39;, text): # Email return \u0026#34;personal_info\u0026#34; # Promotional content promo_keywords = [\u0026#39;follow me\u0026#39;, \u0026#39;check out\u0026#39;, \u0026#39;subscribe\u0026#39;, \u0026#39;buy now\u0026#39;] if any(keyword in text for keyword in promo_keywords): return \u0026#34;promotional\u0026#34; # Spam patterns spam_keywords = [\u0026#39;free money\u0026#39;, \u0026#39;make money fast\u0026#39;, \u0026#39;guaranteed\u0026#39;] if any(keyword in text for keyword in spam_keywords): return \u0026#34;inappropriate\u0026#34; # Otherwise, needs manual review return \u0026#34;needs_review\u0026#34; This caught about 40% of confessions automatically.\nManual Review For the remaining 60%, I had to read and categorize manually. This took 3 days.\n\u0026ldquo;Reading 12,000 college confessions will change your perspective on humanity.\u0026rdquo;\nSome confessions were heartbreaking. Others were hilarious. Many were just\u0026hellip; college stuff.\nQuality Assurance Double Annotation I had a colleague label 20% of the data to measure consistency:\n# Calculate inter-annotator agreement from sklearn.metrics import cohen_kappa_score annotator1_labels = [...] annotator2_labels = [...] kappa_score = cohen_kappa_score(annotator1_labels, annotator2_labels) print(f\u0026#34;Cohen\u0026#39;s Kappa: {kappa_score:.3f}\u0026#34;) Result: 0.87 Kappa score (pretty good for text classification)\nError Analysis I reviewed mislabeled examples to improve the rules:\n# Find common misclassification patterns errors = [] for confession in validation_set: if confession[\u0026#39;predicted_label\u0026#39;] != confession[\u0026#39;true_label\u0026#39;]: errors.append({ \u0026#39;text\u0026#39;: confession[\u0026#39;text\u0026#39;], \u0026#39;predicted\u0026#39;: confession[\u0026#39;predicted_label\u0026#39;], \u0026#39;actual\u0026#39;: confession[\u0026#39;true_label\u0026#39;] }) # Analyze error patterns for error in errors[:10]: print(f\u0026#34;Text: {error[\u0026#39;text\u0026#39;][:100]}...\u0026#34;) print(f\u0026#34;Predicted: {error[\u0026#39;predicted\u0026#39;]}, Actual: {error[\u0026#39;actual\u0026#39;]}\u0026#34;) print() This helped me add more nuanced rules for edge cases.\nThe Final Dataset Structure { \u0026#34;confession_id\u0026#34;: \u0026#34;conf_001\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;cleaned confession text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;safe/needs_review/inappropriate/...\u0026#34;, \u0026#34;confidence\u0026#34;: 0.95, \u0026#34;annotator\u0026#34;: \u0026#34;annotator_1\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2024-03-10T12:00:00Z\u0026#34;, \u0026#34;word_count\u0026#34;: 45 } Statistics Category Count Percentage Safe 13,600 68% Needs Review 3,600 18% Inappropriate 1,600 8% Personal Info 800 4% Promotional 400 2% Total: 20,000 clean, labeled confessions\nModel Training Results With this dataset, dip\u0026rsquo;s content moderation model achieved:\n87% accuracy on the test set 92% precision for inappropriate content detection Reduced manual review by 70% (only 18% need human review) Not bad for a week\u0026rsquo;s worth of data cleaning!\nLessons Learned 1. Data Cleaning is Actually Interesting I expected this to be boring. But it\u0026rsquo;s like archaeology: you dig through messy data to find patterns and insights.\n2. Automation + Human Judgment = Best Results The automated rules caught the easy cases. Humans handled the nuanced ones. Together, they created a high-quality dataset.\n3. Domain Knowledge Helps Understanding college student culture helped me:\nRecognize inside jokes Identify actual problems vs. dramatic complaints Spot when students needed help (vs. just complaining) 4. Documentation Matters I documented every preprocessing step and rule. This helped:\nNew annotators understand the process Debugging when issues arose Reproducing the results 5. Edge Cases Are Everywhere Every time I thought I had a rule covered, a new edge case appeared:\nCode mixed with confessions Multiple languages (Hindi + English) Sarcasm that looked like harassment Cultural references I didn\u0026rsquo;t understand The Code Pipeline Here\u0026rsquo;s the complete cleaning pipeline:\ndef process_confessions(raw_data): cleaned_data = [] for confession in raw_data: # Clean text text = clean_confession(confession[\u0026#39;text\u0026#39;]) # Check quality if not meets_quality_criteria(text): continue # Auto-label label = auto_label(text) # Store cleaned_data.append({ \u0026#39;id\u0026#39;: confession[\u0026#39;id\u0026#39;], \u0026#39;text\u0026#39;: text, \u0026#39;label\u0026#39;: label, \u0026#39;confidence\u0026#39;: calculate_confidence(text, label) }) return cleaned_data # Run the pipeline dataset = process_confessions(raw_confessions) 78 lines of code to process 20,000 confessions. Data science isn\u0026rsquo;t always glamorous!\nReal-World Impact This dataset became the foundation for dip\u0026rsquo;s content moderation system:\nAutomated filtering of obvious spam/promotional content Human review queue for ambiguous cases Better user experience (faster moderation) Safer community (reduced harassment) The content moderation system now processes thousands of posts daily, keeping the dip community healthy and safe.\nKey Takeaways Raw data is messy - Always expect the unexpected Start simple - Basic rules catch most cases Iterate and improve - Add complexity as needed Measure quality - Track inter-annotator agreement Document everything - Future you will thank present you This project taught me that data quality is the foundation of machine learning. Without clean, labeled data, even the best model architecture won\u0026rsquo;t work.\nPlus, I learned a lot about college student culture. Some of which I wish I could forget! 😄\nThis was my first project at ByteCitadel, marking the beginning of my journey in content moderation and NLP. If you need help with data cleaning or content moderation, feel free to reach out.\n","permalink":"http://localhost:1313/post/confessions-dataset-cleaning/","summary":"\u003ch2 id=\"day-one-at-bytecitadel\"\u003eDay One at ByteCitadel\u003c/h2\u003e\n\u003cp\u003eI showed up to my first day at ByteCitadel expecting to work on some fancy deep learning model. Instead, I got assigned to:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e\u0026ldquo;Go clean these 20,000 confessions. We need them for content moderation training.\u0026rdquo;\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis was my first day as a data science intern. I thought I\u0026rsquo;d be doing computer vision or NLP. Instead, I learned the unglamorous truth about machine learning:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e\u0026ldquo;80% of ML is data cleaning. The other 20% is cleaning data.\u0026rdquo;\u003c/strong\u003e\u003c/p\u003e","title":"My First Task at ByteCitadel: Cleaning 20,000 Confessions"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms.\nChallenge Mangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality Solution Data Collection Strategy Geo point-based sampling: Over 1,100 manually labeled points across India\u0026rsquo;s coastal regions Used Global Mangrove Watch data for reference Separated into mangrove/non-mangrove classes Feature Engineering Spectral bands: B2, B3, B4, B8, B11 from Sentinel-2A Indices: NDVI (Normalized Difference Vegetation Index), NDWI (Normalized Difference Water Index) Multi-spectral analysis for robust classification Model Performance Algorithm: Random Forest Classifier Accuracy: 99.03% on test data Generalization: Strong performance on unseen regions Deployment Google Earth Engine tile exports for classified imagery Custom Streamlit web application for real-time visualization Interactive classification by user clicks on map Technologies Used Google Earth Engine (GEE) Sentinel-2A Multispectral Imagery Random Forest Streamlit Python Geospatial Analysis ","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms.\u003c/p\u003e\n\u003ch2 id=\"challenge\"\u003eChallenge\u003c/h2\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"solution\"\u003eSolution\u003c/h2\u003e\n\u003ch3 id=\"data-collection-strategy\"\u003eData Collection Strategy\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGeo point-based sampling\u003c/strong\u003e: Over 1,100 manually labeled points across India\u0026rsquo;s coastal regions\u003c/li\u003e\n\u003cli\u003eUsed Global Mangrove Watch data for reference\u003c/li\u003e\n\u003cli\u003eSeparated into mangrove/non-mangrove classes\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"feature-engineering\"\u003eFeature Engineering\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSpectral bands\u003c/strong\u003e: B2, B3, B4, B8, B11 from Sentinel-2A\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIndices\u003c/strong\u003e: NDVI (Normalized Difference Vegetation Index), NDWI (Normalized Difference Water Index)\u003c/li\u003e\n\u003cli\u003eMulti-spectral analysis for robust classification\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"model-performance\"\u003eModel Performance\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAlgorithm\u003c/strong\u003e: Random Forest Classifier\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAccuracy\u003c/strong\u003e: 99.03% on test data\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGeneralization\u003c/strong\u003e: Strong performance on unseen regions\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"deployment\"\u003eDeployment\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGoogle Earth Engine\u003c/strong\u003e tile exports for classified imagery\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCustom Streamlit web application\u003c/strong\u003e for real-time visualization\u003c/li\u003e\n\u003cli\u003eInteractive classification by user clicks on map\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"technologies-used\"\u003eTechnologies Used\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eGoogle Earth Engine (GEE)\u003c/li\u003e\n\u003cli\u003eSentinel-2A Multispectral Imagery\u003c/li\u003e\n\u003cli\u003eRandom Forest\u003c/li\u003e\n\u003cli\u003eStreamlit\u003c/li\u003e\n\u003cli\u003ePython\u003c/li\u003e\n\u003cli\u003eGeospatial Analysis\u003c/li\u003e\n\u003c/ul\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Overview Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking Key Features Real-time lane detection and tracking Obstacle avoidance system Autonomous navigation Sensor fusion for decision making Technologies Used Raspberry Pi OpenCV Python Computer Vision IoT Sensors ","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"key-features\"\u003eKey Features\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eReal-time lane detection and tracking\u003c/li\u003e\n\u003cli\u003eObstacle avoidance system\u003c/li\u003e\n\u003cli\u003eAutonomous navigation\u003c/li\u003e\n\u003cli\u003eSensor fusion for decision making\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"technologies-used\"\u003eTechnologies Used\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRaspberry Pi\u003c/li\u003e\n\u003cli\u003eOpenCV\u003c/li\u003e\n\u003cli\u003ePython\u003c/li\u003e\n\u003cli\u003eComputer Vision\u003c/li\u003e\n\u003cli\u003eIoT Sensors\u003c/li\u003e\n\u003c/ul\u003e","title":"Real-Time Self-Driving Car with Lane Detection"}]