[{"content":"The Goal Building systems for degree verification and academic fraud detection requires authentic examples. I collected degrees from LinkedIn graduation posts across 8 countries.\nCoverage 1,400 unique degrees from:\nUSA: Ivy League, top universities, business schools India: IITs, IIMs, AIIMS UK: Oxford, Cambridge, Russell Group Australia: Group of Eight (Go8) Europe: Top institutions (ETH Zurich, TUM, etc.) Asia: NUS, NTU, Seoul National, University of Tokyo Collection Method def collect_graduation_posts(keywords, universities): posts = [] for university in universities: search_queries = [ f\u0026#34;{university} graduation 2024\u0026#34;, f\u0026#34;{university} degree ceremony\u0026#34;, f\u0026#34;graduated from {university}\u0026#34; ] for query in search_queries: results = linkedin_scraper.search_posts( query=query, limit=50, date_range=\u0026#34;2023-2024\u0026#34; ) posts.extend(results) return posts Quality Control Deduplication Used perceptual hashing to remove duplicates:\nfrom imagehash import average_hash hash_value = average_hash(image) if hash_value not in seen_hashes: seen_hashes.add(hash_value) unique_degrees.append(degree) Removed: ~15% duplicates\nValidation Resolution check: Minimum 800x600 Format check: JPEG/PNG only Content verification: OCR-based degree detection The Dataset Structure { \u0026#34;degree_id\u0026#34;: \u0026#34;deg_0001\u0026#34;, \u0026#34;university\u0026#34;: \u0026#34;Stanford University\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;USA\u0026#34;, \u0026#34;degree_type\u0026#34;: \u0026#34;Bachelor of Science\u0026#34;, \u0026#34;graduation_year\u0026#34;: 2024, \u0026#34;image_path\u0026#34;: \u0026#34;degrees/usa/stanford_001.jpg\u0026#34;, \u0026#34;quality_score\u0026#34;: 0.95, \u0026#34;verified\u0026#34;: true } Statistics USA: 450 degrees India: 300 degrees UK: 180 degrees Australia: 150 degrees Others: 320 degrees Applications 1. Document Verification Train ML models to verify degree authenticity\n2. Fraud Detection Identify fake or tampered certificates\n3. Academic Research Study degree formats and university branding\n4. Market Analysis Track educational trends and university popularity\nLessons Learned 1. Source Quality Matters LinkedIn posts provided authentic, high-quality degree images\n2. Global Coverage is Hard Some countries had fewer graduation posts\n3. Privacy Considerations Anonymized student information, used for research only\n4. Documentation is Critical Tracked sources, preprocessing, and validation steps\nImpact This dataset enables:\nAutomated degree verification systems Fraud detection for educational credentials Academic research on document security Market analysis of educational trends Key Takeaway Large-scale data collection from social media requires careful curation, quality control, and ethical considerations.\nThis comprehensive degree dataset supports academic integrity and fraud detection research worldwide.\n","permalink":"https://shubhamapat7.github.io/post/university-degrees-dataset/","summary":"\u003ch2 id=\"the-goal\"\u003eThe Goal\u003c/h2\u003e\n\u003cp\u003eBuilding systems for \u003cstrong\u003edegree verification\u003c/strong\u003e and \u003cstrong\u003eacademic fraud detection\u003c/strong\u003e requires authentic examples. I collected degrees from LinkedIn graduation posts across 8 countries.\u003c/p\u003e\n\u003ch2 id=\"coverage\"\u003eCoverage\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e1,400 unique degrees\u003c/strong\u003e from:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eUSA\u003c/strong\u003e: Ivy League, top universities, business schools\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIndia\u003c/strong\u003e: IITs, IIMs, AIIMS\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUK\u003c/strong\u003e: Oxford, Cambridge, Russell Group\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAustralia\u003c/strong\u003e: Group of Eight (Go8)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEurope\u003c/strong\u003e: Top institutions (ETH Zurich, TUM, etc.)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAsia\u003c/strong\u003e: NUS, NTU, Seoul National, University of Tokyo\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"collection-method\"\u003eCollection Method\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecollect_graduation_posts\u003c/span\u003e(keywords, universities):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    posts \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e university \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e universities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        search_queries \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003euniversity\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e graduation 2024\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003euniversity\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e degree ceremony\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;graduated from \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003euniversity\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e query \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e search_queries:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            results \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e linkedin_scraper\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esearch_posts(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                query\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003equery,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                limit\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e50\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                date_range\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;2023-2024\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            posts\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eextend(results)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e posts\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"quality-control\"\u003eQuality Control\u003c/h2\u003e\n\u003ch3 id=\"deduplication\"\u003eDeduplication\u003c/h3\u003e\n\u003cp\u003eUsed perceptual hashing to remove duplicates:\u003c/p\u003e","title":"I Collected 1,400 University Degrees to Build a Verification Dataset"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"https://shubhamapat7.github.io/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"The Approach Traditional image classification requires:\nThousands of labeled images Weeks of training GPU resources CLIP is different: it already \u0026ldquo;understands\u0026rdquo; images through natural language.\nThe Method 1. Define Prompts Instead of training, I wrote text prompts for each class:\n# NSFW class prompts nsfw_prompts = [ \u0026#34;a nude person\u0026#34;, \u0026#34;explicit sexual content\u0026#34;, \u0026#34;inappropriate intimate image\u0026#34;, \u0026#34;adult content not suitable for work\u0026#34;, # ... 60 total prompts ] # NEUTRAL class prompts neutral_prompts = [ \u0026#34;a normal everyday photo\u0026#34;, \u0026#34;safe for work content\u0026#34;, \u0026#34;appropriate business image\u0026#34;, \u0026#34;general photography\u0026#34;, # ... 60 total prompts ] 2. Compute Similarity import clip import torch # Load CLIP model model, preprocess = clip.load(\u0026#34;ViT-B/32\u0026#34;) device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; model.to(device) def classify_image(image_path): # Load and preprocess image image = preprocess(Image.open(image_path)).unsqueeze(0).to(device) # Get image features with torch.no_grad(): image_features = model.encode_image(image) # Compare with all class prompts best_class = None best_score = -1 for class_name, prompts in classes.items(): class_scores = [] for prompt in prompts: # Tokenize text text = clip.tokenize(prompt).to(device) # Get text features with torch.no_grad(): text_features = model.encode_text(text) # Compute similarity (cosine similarity) similarity = torch.cosine_similarity(image_features, text_features) class_scores.append(similarity.item()) # Average score for this class avg_score = np.mean(class_scores) if avg_score \u0026gt; best_score: best_score = avg_score best_class = class_name return best_class, best_score 3. Grid Search for Best Thresholds from sklearn.model_selection import GridSearchCV # Optimize decision threshold thresholds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7] best_threshold = 0.5 best_f1 = 0 for threshold in thresholds: predictions = [1 if score \u0026gt; threshold else 0 for score in similarity_scores] f1 = f1_score(true_labels, predictions) if f1 \u0026gt; best_f1: best_f1 = f1 best_threshold = threshold The Results Metric Value Overall Accuracy 92% NSFW Detection 94% precision Violence Detection 89% precision Neutral Images 96% precision Why This Works CLIP was trained on 400 million image-text pairs. It learned to understand the relationship between images and words.\nSo when I ask \u0026ldquo;is this image similar to \u0026lsquo;a nude person\u0026rsquo;?\u0026rdquo;, CLIP can answer based on its trainingâ€”no fine-tuning needed!\nAdvantages 1. No Training Required # Add new class instantly new_class_prompts = [\u0026#34;an image of a car\u0026#34;, \u0026#34;vehicle photography\u0026#34;, \u0026#34;automobile\u0026#34;] classes[\u0026#39;VEHICLE\u0026#39;] = new_class_prompts 2. Flexible Easy to add new categories with just text prompts\n3. Interpretable I can see which prompts matched best:\n# Debugging for prompt in nsfw_prompts: score = compute_similarity(image, prompt) print(f\u0026#34;{prompt}: {score:.3f}\u0026#34;) Comparison: CLIP vs Traditional CNN Approach Training Data Training Time Accuracy Flexibility Traditional CNN 50K labeled images 2-3 days 94% Low (retrain for new class) CLIP 0 (zero-shot) 0 92% High (just add prompts) Use Cases Content moderation (what I built) Search by description (\u0026ldquo;find photos of dogs\u0026rdquo;) Image filtering (\u0026ldquo;show only professional photos\u0026rdquo;) Automated tagging (\u0026ldquo;tag images with labels\u0026rdquo;) Key Takeaway Sometimes the best ML solution is not to train a model. CLIP\u0026rsquo;s zero-shot learning made this 100x easier than building a custom classifier.\n92% accuracy with 0 hours of training? That\u0026rsquo;s the power of pre-trained multi-modal models.\nThis CLIP-based moderation pipeline achieves production-ready accuracy without requiring labeled training data.\n","permalink":"https://shubhamapat7.github.io/projects/image-moderation-clip/","summary":"\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eTraditional image classification requires:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThousands of labeled images\u003c/li\u003e\n\u003cli\u003eWeeks of training\u003c/li\u003e\n\u003cli\u003eGPU resources\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCLIP is different: it already \u0026ldquo;understands\u0026rdquo; images through natural language.\u003c/p\u003e\n\u003ch2 id=\"the-method\"\u003eThe Method\u003c/h2\u003e\n\u003ch3 id=\"1-define-prompts\"\u003e1. Define Prompts\u003c/h3\u003e\n\u003cp\u003eInstead of training, I wrote text prompts for each class:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# NSFW class prompts\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ensfw_prompts \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;a nude person\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;explicit sexual content\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;inappropriate intimate image\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;adult content not suitable for work\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# ... 60 total prompts\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# NEUTRAL class prompts  \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eneutral_prompts \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;a normal everyday photo\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;safe for work content\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;appropriate business image\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;general photography\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# ... 60 total prompts\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"2-compute-similarity\"\u003e2. Compute Similarity\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e clip\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e torch\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Load CLIP model\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emodel, preprocess \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e clip\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eload(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;ViT-B/32\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edevice \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;cuda\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecuda\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eis_available() \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;cpu\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emodel\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto(device)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eclassify_image\u003c/span\u003e(image_path):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Load and preprocess image\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    image \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e preprocess(Image\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eopen(image_path))\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eunsqueeze(\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto(device)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Get image features\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ewith\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eno_grad():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        image_features \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eencode_image(image)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Compare with all class prompts\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    best_class \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eNone\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    best_score \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e class_name, prompts \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e classes\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitems():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        class_scores \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e prompt \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e prompts:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# Tokenize text\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            text \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e clip\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etokenize(prompt)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto(device)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# Get text features\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003ewith\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eno_grad():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                text_features \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eencode_text(text)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# Compute similarity (cosine similarity)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            similarity \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecosine_similarity(image_features, text_features)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            class_scores\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(similarity\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitem())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#75715e\"\u003e# Average score for this class\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        avg_score \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emean(class_scores)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e avg_score \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e best_score:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            best_score \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e avg_score\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            best_class \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e class_name\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e best_class, best_score\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"3-grid-search-for-best-thresholds\"\u003e3. Grid Search for Best Thresholds\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e sklearn.model_selection \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e GridSearchCV\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Optimize decision threshold\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ethresholds \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#ae81ff\"\u003e0.2\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e0.3\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e0.4\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e0.6\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e0.7\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ebest_threshold \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ebest_f1 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e threshold \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e thresholds:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    predictions \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e score \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e threshold \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                   \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e score \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e similarity_scores]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    f1 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e f1_score(true_labels, predictions)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e f1 \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e best_f1:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        best_f1 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e f1\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        best_threshold \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e threshold\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"the-results\"\u003eThe Results\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eMetric\u003c/th\u003e\n          \u003cth\u003eValue\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eOverall Accuracy\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003e92%\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eNSFW Detection\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e94% precision\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eViolence Detection\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e89% precision\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eNeutral Images\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e96% precision\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"why-this-works\"\u003eWhy This Works\u003c/h2\u003e\n\u003cp\u003eCLIP was trained on 400 million image-text pairs. It learned to understand the \u003cstrong\u003erelationship\u003c/strong\u003e between images and words.\u003c/p\u003e","title":"I Used OpenAI CLIP for Image Moderation - No Training Required"},{"content":"The Challenge Building a content moderation system for a social media app requires training data. Lots of it.\nDip needed to automatically detect:\nNSFW content Violence Drugs Weapons Hateful content Neutral content The question: where do you get 150,000+ labeled images?\nThis was my task at ByteCitadel: collect, curate, and label a massive dataset for Dip\u0026rsquo;s image moderation pipeline.\nThe Plan Six categories, ~30,000 images each:\nViolence (30K) Drugs (30K) NSFW (30K) Neutral (30K) Hateful Memes (30K) Weapons (30K) Total: 180,000 images (we collected extra for quality control)\nData Sources 1. Kaggle Datasets Kaggle has tons of public datasets. I found:\nViolence datasets - From academic papers on conflict detection Drug-related imagery - For medical/research purposes Weapon detection datasets - Security/computer vision research # Download Kaggle datasets import kaggle kaggle.api.dataset_download_files( \u0026#39;tapakah68/violence-detection-dataset\u0026#39;, path=\u0026#39;./datasets/violence/\u0026#39;, unzip=True ) 2. Research Papers Academic papers often release datasets alongside publications:\n# Search for datasets mentioned in papers paper_datasets = [ \u0026#34;Flames-Dataset\u0026#34;, # Violence detection \u0026#34;Drug-Image-Dataset\u0026#34;, # Drug classification \u0026#34;Meme-Classification-Dataset\u0026#34; # Hateful memes ] for dataset in paper_datasets: download_dataset(dataset) 3. Roboflow Roboflow has high-quality, pre-labeled computer vision datasets:\n# Export from Roboflow import roboflow rf = roboflow.Roboflow(api_key=\u0026#34;YOUR_API_KEY\u0026#34;) project = rf.workspace(\u0026#34;workspace\u0026#34;).project(\u0026#34;weapon-detection\u0026#34;) dataset = project.version(1).download(\u0026#34;coco\u0026#34;) 4. Custom Collection For edge cases, I scraped and labeled manually:\n# Custom scraping for underrepresented categories def collect_images(query, num_images=1000): results = [] for image_url in image_search_api.search(query, num_images): download_image(image_url, f\u0026#34;./datasets/{query}/\u0026#34;) results.append(image_url) return results The Curation Process Quality Filtering Not all downloaded images were usable:\ndef filter_image_quality(image_path): # Check file size if os.path.getsize(image_path) \u0026lt; 10 * 1024: # Less than 10KB return False, \u0026#34;Too small\u0026#34; # Verify it\u0026#39;s actually an image try: img = Image.open(image_path) img.verify() except: return False, \u0026#34;Corrupted\u0026#34; # Check resolution img = Image.open(image_path) if img.width \u0026lt; 224 or img.height \u0026lt; 224: return False, \u0026#34;Low resolution\u0026#34; # Check if it\u0026#39;s actually an image (not blank/black) if is_blank_image(img): return False, \u0026#34;Blank image\u0026#34; return True, \u0026#34;Valid\u0026#34; Deduplication Images from different sources often overlapped:\nfrom imagehash import average_hash def remove_duplicates(image_paths): hashes = {} unique_images = [] for img_path in image_paths: # Calculate perceptual hash img = Image.open(img_path) hash_value = average_hash(img) if hash_value not in hashes: hashes[hash_value] = img_path unique_images.append(img_path) else: print(f\u0026#34;Duplicate found: {img_path}\u0026#34;) return unique_images # Found and removed ~15,000 duplicates across the dataset Class Balancing Ensuring equal representation across categories:\n# Check class distribution class_counts = {} for category in categories: count = len(os.listdir(f\u0026#39;./datasets/{category}/\u0026#39;)) class_counts[category] = count print(class_counts) # Expected: ~30K per class # If imbalance \u0026gt; 10%, collect more data for minority classes Labeling Strategy Automated Labeling For well-known datasets, labels were already provided:\n# Process COCO-formatted labels import json with open(\u0026#39;annotations.json\u0026#39;, \u0026#39;r\u0026#39;) as f: coco_data = json.load(f) for annotation in coco_data[\u0026#39;annotations\u0026#39;]: image_id = annotation[\u0026#39;image_id\u0026#39;] category_id = annotation[\u0026#39;category_id\u0026#39;] image_path = f\u0026#39;images/{image_id}.jpg\u0026#39; category = id_to_category[category_id] move_to_category(image_path, category) Manual Labeling For custom-collected images, I had to label manually:\n# Labeling tool for edge cases def label_image(image_path): img = Image.open(image_path) img.show() label = input(f\u0026#34;What category is this image? \u0026#34;) # Categories: violence, drugs, nsfw, neutral, weapons, hateful_memes # Save label save_label(image_path, label) \u0026ldquo;Labeling 10,000 images by hand teaches you to appreciate automated annotation tools.\u0026rdquo;\nQuality Validation Double-checking labels for accuracy:\ndef validate_labels(image_path, expected_label): img = Image.open(image_path) actual_label = manual_label(image_path) if expected_label != actual_label: print(f\u0026#34;MISLABELED: {image_path}\u0026#34;) print(f\u0026#34;Expected: {expected_label}, Got: {actual_label}\u0026#34;) correct_label(expected_label) Challenges 1. Copyright \u0026amp; Legal Issues Using internet images requires care:\nAcademic use only - Educational/research purposes No redistribution - Keep dataset internal Fair use - Transformative use for ML training 2. Cultural Sensitivity What counts as \u0026ldquo;inappropriate\u0026rdquo; varies by culture:\nWestern vs. Indian context Religious imagery Historical vs. contemporary content 3. Data Imbalance Some categories were harder to find:\nWeapons: Plenty of gun images, fewer knives Drugs: Need diverse substances, not just weed Violence: Real violence vs. fictional content 4. False Positives Training on misleading data:\nPhotoshoot violence vs. real violence Medical imagery (drugs for legitimate use) Art depicting weapons (not real weapons) The Final Dataset Statistics Category Images Duplicates Removed Final Count Violence 32,000 2,000 30,000 Drugs 35,000 5,000 30,000 NSFW 38,000 8,000 30,000 Neutral 31,000 1,000 30,000 Hateful Memes 29,000 0 29,000 Weapons 33,000 3,000 30,000 Total: 198,000 â†’ 179,000 (after quality filtering)\nData Structure dataset/ â”œâ”€â”€ violence/ â”‚ â”œâ”€â”€ img_00001.jpg â”‚ â”œâ”€â”€ img_00002.jpg â”‚ â””â”€â”€ ... â”œâ”€â”€ drugs/ â”œâ”€â”€ nsfw/ â”œâ”€â”€ neutral/ â”œâ”€â”€ hateful_memes/ â””â”€â”€ weapons/ Each image has a corresponding metadata file:\n{ \u0026#34;image_id\u0026#34;: \u0026#34;img_00001\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;violence\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;kaggle_violence_dataset\u0026#34;, \u0026#34;resolution\u0026#34;: \u0026#34;1920x1080\u0026#34;, \u0026#34;file_size\u0026#34;: \u0026#34;245KB\u0026#34;, \u0026#34;date_collected\u0026#34;: \u0026#34;2024-09-15\u0026#34;, \u0026#34;verified\u0026#34;: true, \u0026#34;quality_score\u0026#34;: 0.95 } Model Training Results With this dataset, Dip\u0026rsquo;s content moderation model achieved:\n92% accuracy across all categories 94% precision for violence detection 89% precision for NSFW detection \u0026lt;3% false positive rate on neutral images # Train multi-class classifier from torchvision import models import torch model = models.resnet50(pretrained=True) model.fc = torch.nn.Linear(2048, 6) # 6 categories # Train with data augmentation train_loader = DataLoader(dataset, batch_size=32, shuffle=True) for epoch in range(10): for batch in train_loader: images, labels = batch outputs = model(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() Lessons Learned 1. Quality Over Quantity 30K high-quality images \u0026gt; 100K mediocre ones\n2. Source Diversity Matters Using multiple sources prevents model bias\n3. Documentation is Critical Track:\nData sources Preprocessing steps Quality checks Labeling methodology 4. Expect Legal Complexity Using internet images requires legal review\n5. Human Validation is Essential Automated filters catch obvious cases, but humans catch edge cases\nKey Takeaways Data collection is 80% of ML - Garbage in, garbage out Multiple sources reduce bias and improve generalization Quality filtering is as important as data collection Legal considerations matter for production systems Document everything for reproducibility This dataset became the foundation for Dip\u0026rsquo;s image moderation system, processing millions of images daily and keeping the platform safe for users.\nThe 179,000 images across 6 categories now power real-time content moderation for thousands of users. It\u0026rsquo;s a reminder that machine learning is only as good as your training data.\nThis dataset collection was part of building Dip\u0026rsquo;s content moderation pipeline. If you need help with dataset creation or content moderation, feel free to reach out.\n","permalink":"https://shubhamapat7.github.io/post/nsfw-moderation-dataset/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBuilding a content moderation system for a social media app requires \u003cstrong\u003etraining data\u003c/strong\u003e. Lots of it.\u003c/p\u003e\n\u003cp\u003eDip needed to automatically detect:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNSFW content\u003c/li\u003e\n\u003cli\u003eViolence\u003c/li\u003e\n\u003cli\u003eDrugs\u003c/li\u003e\n\u003cli\u003eWeapons\u003c/li\u003e\n\u003cli\u003eHateful content\u003c/li\u003e\n\u003cli\u003eNeutral content\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe question: \u003cstrong\u003ewhere do you get 150,000+ labeled images?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis was my task at ByteCitadel: collect, curate, and label a massive dataset for Dip\u0026rsquo;s image moderation pipeline.\u003c/p\u003e\n\u003ch2 id=\"the-plan\"\u003eThe Plan\u003c/h2\u003e\n\u003cp\u003eSix categories, ~30,000 images each:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eViolence\u003c/strong\u003e (30K)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDrugs\u003c/strong\u003e (30K)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNSFW\u003c/strong\u003e (30K)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNeutral\u003c/strong\u003e (30K)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHateful Memes\u003c/strong\u003e (30K)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWeapons\u003c/strong\u003e (30K)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTotal: \u003cstrong\u003e180,000 images\u003c/strong\u003e (we collected extra for quality control)\u003c/p\u003e","title":"I Collected 150,000+ Images to Train a Content Moderation Model"},{"content":"The Problem Content moderation at scale is hard. You need to process thousands of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the model was taking 250ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nThe standard approach didn\u0026rsquo;t work for us. We tried everythingâ€”batch processing, async queues, caching. But eventually, we realized we needed to optimize the model itself.\nThe Dual-Model Approach Our pipeline used two models working together:\nNudeNet for initial NSFW detection MobileNet for secondary classification Both models achieved 97% accuracy, which is great. But they were too slow. And since this was for a consumer app, we needed CPU inferenceâ€”we couldn\u0026rsquo;t rely on users having GPUs.\nThe Optimization Strategy I tried three approaches:\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\n# Convert PyTorch to ONNX torch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: ~10% speedup. Nice, but not enough.\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another ~15% speedup. We\u0026rsquo;re getting somewhere!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to FP16 (and sometimes INT8) Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Another ~15% speedup!\nThe Numbers Metric Before After Improvement Inference Time 250ms 180ms 28% faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~200ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nI was genuinely surprised. When you optimize properly, you can often beat official benchmarks. The key is testing with your actual workload, not just running synthetic benchmarks.\nThe Technical Details Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter The accuracy impact was \u0026lt;0.1%, which is negligible for content moderation use cases.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Real-World Testing We tested on a 10,000 image dataset:\nIntel Xeon CPU (production-like environment) 180ms Â± 5ms across all runs 97.3% accuracy maintained Lessons Learned 1. Hardware-Specific Optimization Matters Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite 2. Don\u0026rsquo;t Trust Benchmarks Blindly Official benchmarks are great starting points, but your mileage will vary. Test with your:\nActual data distribution Real hardware constraints Production environment variables 3. The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\n4. Trade-offs are Manageable 28% faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments.\nCode Implementation The final pipeline was beautifully simple:\nimport openvino as ov # Load OpenVINO model core = ov.Core() model = core.compile_model(\u0026#34;openvino_model.xml\u0026#34;, \u0026#34;CPU\u0026#34;) # Run inference results = model(input_image) # Post-process results nsfw_score = results[0][1] # Confidence for NSFW class if nsfw_score \u0026gt; 0.8: return {\u0026#34;nsfw\u0026#34;: True, \u0026#34;confidence\u0026#34;: nsfw_score} That\u0026rsquo;s it. Three lines of code to load and run the optimized model.\nBusiness Impact For Dip App Real-time moderation: No lag in content upload Better UX: Users don\u0026rsquo;t wait for processing Scalability: Handle 10x more images per server For User Experience Faster uploads: Content appears instantly Better performance: App feels snappier Lower battery drain: More efficient CPU usage The Bigger Picture This optimization work taught me that model performance isn\u0026rsquo;t just about model architecture. It\u0026rsquo;s about the entire inference pipeline:\nModel format (PyTorch â†’ ONNX â†’ OpenVINO) Optimization level (FP32 â†’ FP16 â†’ INT8) Hardware acceleration (CPU-specific optimizations) Graph optimization (removing unnecessary operations) By optimizing each layer, we achieved production-ready performance without sacrificing accuracy.\nKey Takeaways Always profile first - Don\u0026rsquo;t optimize blindly Test on target hardware - Desktop benchmarks â‰  production reality Hardware-specific toolchains matter immensely Simple optimizations often yield the biggest gains Measure real-world impact - Not just synthetic benchmarks The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"https://shubhamapat7.github.io/post/nsmfw-moderation-optimization/","summary":"\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eContent moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process thousands of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the model was taking \u003cstrong\u003e250ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made NSFW Detection 28% Faster and Beat Ultralytics' Own Benchmark"},{"content":"The Problem Event platforms like BookMyShow are full of fake listings. Users waste money on non-existent events. Platforms need automated verification.\nThe solution: compare claimed event details with actual page content using fuzzy string matching.\nThe Approach from fuzzywuzzy import fuzz def verify_event_credibility(claimed_info, actual_content): scores = {} # Compare titles title_score = fuzz.partial_ratio( claimed_info[\u0026#39;title\u0026#39;].lower(), actual_content[\u0026#39;title\u0026#39;].lower() ) # Compare descriptions desc_score = fuzz.token_sort_ratio( claimed_info[\u0026#39;description\u0026#39;], actual_content[\u0026#39;description\u0026#39;] ) # Overall credibility score overall_score = (title_score * 0.3 + desc_score * 0.4 + date_score * 0.2 + location_score * 0.1) return { \u0026#39;overall_score\u0026#39;: overall_score, \u0026#39;credible\u0026#39;: overall_score \u0026gt; 75 } Results 5,000 events tested 92% accuracy 70% reduction in fake event complaints Key Takeaway Simple fuzzy matching can solve complex fraud detection problems.\nThis system was developed to combat event fraud across multiple event listing platforms.\n","permalink":"https://shubhamapat7.github.io/post/event-verifier-crawler/","summary":"\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eEvent platforms like BookMyShow are full of fake listings. Users waste money on non-existent events. Platforms need automated verification.\u003c/p\u003e\n\u003cp\u003eThe solution: \u003cstrong\u003ecompare claimed event details with actual page content using fuzzy string matching\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e fuzzywuzzy \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e fuzz\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003everify_event_credibility\u003c/span\u003e(claimed_info, actual_content):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    scores \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Compare titles\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    title_score \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e fuzz\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epartial_ratio(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        claimed_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;title\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elower(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        actual_content[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;title\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elower()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Compare descriptions\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    desc_score \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e fuzz\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etoken_sort_ratio(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        claimed_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;description\u0026#39;\u003c/span\u003e],\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        actual_content[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;description\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Overall credibility score\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    overall_score \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (title_score \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.3\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e desc_score \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.4\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    date_score \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.2\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e location_score \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.1\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;overall_score\u0026#39;\u003c/span\u003e: overall_score,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;credible\u0026#39;\u003c/span\u003e: overall_score \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e75\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e5,000 events tested\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e92% accuracy\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e70% reduction\u003c/strong\u003e in fake event complaints\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"key-takeaway\"\u003eKey Takeaway\u003c/h2\u003e\n\u003cp\u003eSimple fuzzy matching can solve complex fraud detection problems.\u003c/p\u003e","title":"I Built a Bot to Catch Fake Event Listings with Fuzzy Matching"},{"content":"The Mission College students in India struggle to find affordable PGs and hostels. I scraped data from three major property sites to build a comprehensive database.\nThe Stack class PropertyScraper: def __init__(self): self.scrapers = { \u0026#39;magicbricks\u0026#39;: MagicBricksScraper(), \u0026#39;justdial\u0026#39;: JustDialScraper(), \u0026#39;99acres\u0026#39;: NinetyNineAcresScraper() } Each platform needed custom scraping logic.\nThe Numbers Platform Properties Success Rate Magic Bricks 28,500 94% JustDial 25,000 89% 99acres 16,500 91% Total 70,000 91% Challenges Anti-bot measures: CAPTCHA, IP blocking, rate limiting Solution: Rotated user agents, added delays, proxy rotation\nDynamic content: JavaScript-rendered listings Solution: Selenium WebDriver with scroll handling\nData inconsistency: Different formats across platforms Solution: Normalization pipeline\nImpact 70,000 properties across 50 cities Student-focused filtering (budget, amenities, location) Platform integration for easy search Key Takeaway Web scraping at scale requires platform-specific solutions and robust anti-detection strategies.\nThis property data collection project was built to help students find affordable and suitable accommodation near their colleges.\n","permalink":"https://shubhamapat7.github.io/post/property-scraper-analysis/","summary":"\u003ch2 id=\"the-mission\"\u003eThe Mission\u003c/h2\u003e\n\u003cp\u003eCollege students in India struggle to find affordable PGs and hostels. I scraped data from three major property sites to build a comprehensive database.\u003c/p\u003e\n\u003ch2 id=\"the-stack\"\u003eThe Stack\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ePropertyScraper\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e__init__\u003c/span\u003e(self):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003escrapers \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;magicbricks\u0026#39;\u003c/span\u003e: MagicBricksScraper(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;justdial\u0026#39;\u003c/span\u003e: JustDialScraper(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;99acres\u0026#39;\u003c/span\u003e: NinetyNineAcresScraper()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eEach platform needed custom scraping logic.\u003c/p\u003e\n\u003ch2 id=\"the-numbers\"\u003eThe Numbers\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003ePlatform\u003c/th\u003e\n          \u003cth\u003eProperties\u003c/th\u003e\n          \u003cth\u003eSuccess Rate\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eMagic Bricks\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e28,500\u003c/td\u003e\n          \u003ctd\u003e94%\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eJustDial\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e25,000\u003c/td\u003e\n          \u003ctd\u003e89%\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e99acres\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e16,500\u003c/td\u003e\n          \u003ctd\u003e91%\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eTotal\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003e70,000\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003e91%\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"challenges\"\u003eChallenges\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eAnti-bot measures\u003c/strong\u003e: CAPTCHA, IP blocking, rate limiting\n\u003cstrong\u003eSolution\u003c/strong\u003e: Rotated user agents, added delays, proxy rotation\u003c/p\u003e","title":"I Scraped 70K Property Listings to Help Students Find Housing"},{"content":"Overview Built a multi-stage pipeline for verifying student ID cards using advanced deep learning models to detect various types of forgery and tampering.\nPipeline Architecture Test Dataset 97 manually collected and cleaned ID card images Diverse ID card formats and designs Model Components 1. Face Detection (YOLOv8) High-accuracy face detection in ID cards Fast inference for real-time processing 2. Gender Validation DeepFace-based gender classification Accuracy: 80% (with potential for improvement via dedicated training) 3. Splicing Detection Model: Image Splicing Detector (image_splicer_quant.tflite) Architecture: DenseNet121 with ELA preprocessing Training Data: CASIA dataset Preprocessing: Error Level Analysis (ELA) Performance: Train accuracy: 97.35% Validation accuracy: 82.31% Output: Splicing score (threshold: 0.90) Average test score: 0.95 4. Copy-Move Detection Model: New UNet (new_unet.tflite) Training: COMOFOD dataset Training: 30 epochs Accuracy: 98.7% Output: Copy-move score (threshold: 0.22) Average test score: 0.19 OCR \u0026amp; Text Validation File: speed_uppp2.py Features: OCR text extraction Text consistency check Keyword matching (case/special character validation) Keyword match percentage calculation ELA analysis Pipeline Flow Face Detection (FaceDetection_YOLO.py) Gender Validation (gender_validation.py) Splicing Detection (splicing_detector.py) Copy-Move Detection (copymove_detector.py) OCR \u0026amp; Text Analysis (speed_uppp2.py) Validation Results Successfully processes diverse ID card formats Robust detection of multiple forgery types Multi-layer validation ensures high accuracy Real-time processing capability Technologies Used YOLOv8 TensorFlow Lite UNet DenseNet121 DeepFace OCR (Optical Character Recognition) Error Level Analysis (ELA) Python Computer Vision ","permalink":"https://shubhamapat7.github.io/projects/student-id-verification-pipeline/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eBuilt a multi-stage pipeline for verifying student ID cards using advanced deep learning models to detect various types of forgery and tampering.\u003c/p\u003e\n\u003ch2 id=\"pipeline-architecture\"\u003ePipeline Architecture\u003c/h2\u003e\n\u003ch3 id=\"test-dataset\"\u003eTest Dataset\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e97 manually collected and cleaned ID card images\u003c/li\u003e\n\u003cli\u003eDiverse ID card formats and designs\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"model-components\"\u003eModel Components\u003c/h3\u003e\n\u003ch4 id=\"1-face-detection-yolov8\"\u003e1. Face Detection (YOLOv8)\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eHigh-accuracy face detection in ID cards\u003c/li\u003e\n\u003cli\u003eFast inference for real-time processing\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"2-gender-validation\"\u003e2. Gender Validation\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eDeepFace-based gender classification\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAccuracy\u003c/strong\u003e: 80% (with potential for improvement via dedicated training)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"3-splicing-detection\"\u003e3. Splicing Detection\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: Image Splicing Detector (image_splicer_quant.tflite)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eArchitecture\u003c/strong\u003e: DenseNet121 with ELA preprocessing\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraining Data\u003c/strong\u003e: CASIA dataset\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePreprocessing\u003c/strong\u003e: Error Level Analysis (ELA)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePerformance\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eTrain accuracy: 97.35%\u003c/li\u003e\n\u003cli\u003eValidation accuracy: 82.31%\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOutput\u003c/strong\u003e: Splicing score (threshold: 0.90)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAverage test score\u003c/strong\u003e: 0.95\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"4-copy-move-detection\"\u003e4. Copy-Move Detection\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eModel\u003c/strong\u003e: New UNet (new_unet.tflite)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraining\u003c/strong\u003e: COMOFOD dataset\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraining\u003c/strong\u003e: 30 epochs\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAccuracy\u003c/strong\u003e: 98.7%\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOutput\u003c/strong\u003e: Copy-move score (threshold: 0.22)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAverage test score\u003c/strong\u003e: 0.19\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"ocr--text-validation\"\u003eOCR \u0026amp; Text Validation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFile\u003c/strong\u003e: speed_uppp2.py\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFeatures\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eOCR text extraction\u003c/li\u003e\n\u003cli\u003eText consistency check\u003c/li\u003e\n\u003cli\u003eKeyword matching (case/special character validation)\u003c/li\u003e\n\u003cli\u003eKeyword match percentage calculation\u003c/li\u003e\n\u003cli\u003eELA analysis\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"pipeline-flow\"\u003ePipeline Flow\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eFace Detection\u003c/strong\u003e (FaceDetection_YOLO.py)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGender Validation\u003c/strong\u003e (gender_validation.py)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSplicing Detection\u003c/strong\u003e (splicing_detector.py)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCopy-Move Detection\u003c/strong\u003e (copymove_detector.py)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOCR \u0026amp; Text Analysis\u003c/strong\u003e (speed_uppp2.py)\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"validation-results\"\u003eValidation Results\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSuccessfully processes diverse ID card formats\u003c/li\u003e\n\u003cli\u003eRobust detection of multiple forgery types\u003c/li\u003e\n\u003cli\u003eMulti-layer validation ensures high accuracy\u003c/li\u003e\n\u003cli\u003eReal-time processing capability\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"technologies-used\"\u003eTechnologies Used\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eYOLOv8\u003c/li\u003e\n\u003cli\u003eTensorFlow Lite\u003c/li\u003e\n\u003cli\u003eUNet\u003c/li\u003e\n\u003cli\u003eDenseNet121\u003c/li\u003e\n\u003cli\u003eDeepFace\u003c/li\u003e\n\u003cli\u003eOCR (Optical Character Recognition)\u003c/li\u003e\n\u003cli\u003eError Level Analysis (ELA)\u003c/li\u003e\n\u003cli\u003ePython\u003c/li\u003e\n\u003cli\u003eComputer Vision\u003c/li\u003e\n\u003c/ul\u003e","title":"Student ID Verification Pipeline for dip"},{"content":"The Problem Students need to find:\nPGs and hostels near colleges Restaurants and cafes Study spaces Other student services I scraped Google Maps for 3GB of business data around colleges in 7 major Indian cities.\nBuilding the dashboard was easy. Making it fast? That\u0026rsquo;s where it got interesting.\nFirst Version: Painfully Slow # This took 90+ seconds to load df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) # 3GB CSV filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion # Convert to Parquet (columnar storage) df.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) # Load with efficient filtering df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) # Load time: 60 seconds (better, but still terrible) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization # Optimize data types df[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) # Uses less memory df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): # Cache city-specific data return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) # Cached after first call return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time # Don\u0026#39;t load everything, just what you need df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] # Only load Mumbai data if Mumbai selected ) Result: 1 second load time!\nThe Final Architecture # Optimized pipeline @app.callback( Output(\u0026#39;map\u0026#39;, \u0026#39;figure\u0026#39;), Input(\u0026#39;city-dropdown\u0026#39;, \u0026#39;value\u0026#39;) ) def update_map(selected_city): # 1. Load only filtered data (1 second) df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) # 2. Create map (0.5 seconds) fig = px.scatter_mapbox( df, lat=\u0026#39;lat\u0026#39;, lon=\u0026#39;lng\u0026#39;, hover_name=\u0026#39;name\u0026#39;, hover_data=[\u0026#39;category\u0026#39;, \u0026#39;rating\u0026#39;], color=\u0026#39;category\u0026#39;, size=\u0026#39;rating\u0026#39; ) # 3. Update layout (0.5 seconds) fig.update_layout( mapbox_style=\u0026#39;open-street-map\u0026#39;, height=600 ) return fig The Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Dashboard Features Interactive Visualizations ScatterMapBox: Geographic distribution with city filtering Sunburst Chart: Category breakdown by pincode Histogram: Top pincodes by business count Data Table: Sortable, filterable business listings Tree Map: Category distribution User Flow # User selects Mumbai â†’ Map updates (1 second) # User clicks Pincode 400001 â†’ Sunburst chart updates # User selects \u0026#34;Restaurants\u0026#34; â†’ Table filters to restaurants # User clicks \u0026#34;Download\u0026#34; â†’ CSV exports (2 seconds) Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"https://shubhamapat7.github.io/projects/college-hotspots-dashboard/","summary":"\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eStudents need to find:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePGs and hostels near colleges\u003c/li\u003e\n\u003cli\u003eRestaurants and cafes\u003c/li\u003e\n\u003cli\u003eStudy spaces\u003c/li\u003e\n\u003cli\u003eOther student services\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI scraped Google Maps for \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 7 major Indian cities.\u003c/p\u003e\n\u003cp\u003eBuilding the dashboard was easy. Making it fast? That\u0026rsquo;s where it got interesting.\u003c/p\u003e\n\u003ch2 id=\"first-version-painfully-slow\"\u003eFirst Version: Painfully Slow\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This took 90+ seconds to load\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edf \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pd\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread_csv(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;businesses.csv\u0026#39;\u003c/span\u003e)  \u003cspan style=\"color:#75715e\"\u003e# 3GB CSV\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003efiltered_df \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e df[df[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;city\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e selected_city]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emap_plot \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e px\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003escatter_mapbox(filtered_df, \u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 90+ second load time. No one would use this.\u003c/p\u003e","title":"I Optimized a 3GB Dashboard from 90 Seconds to 1 Second (90x Speedup)"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"https://shubhamapat7.github.io/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"https://shubhamapat7.github.io/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms.\nChallenge Mangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality Solution Data Collection Strategy Geo point-based sampling: Over 1,100 manually labeled points across India\u0026rsquo;s coastal regions Used Global Mangrove Watch data for reference Separated into mangrove/non-mangrove classes Feature Engineering Spectral bands: B2, B3, B4, B8, B11 from Sentinel-2A Indices: NDVI (Normalized Difference Vegetation Index), NDWI (Normalized Difference Water Index) Multi-spectral analysis for robust classification Model Performance Algorithm: Random Forest Classifier Accuracy: 99.03% on test data Generalization: Strong performance on unseen regions Deployment Google Earth Engine tile exports for classified imagery Custom Streamlit web application for real-time visualization Interactive classification by user clicks on map Technologies Used Google Earth Engine (GEE) Sentinel-2A Multispectral Imagery Random Forest Streamlit Python Geospatial Analysis ","permalink":"https://shubhamapat7.github.io/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms.\u003c/p\u003e\n\u003ch2 id=\"challenge\"\u003eChallenge\u003c/h2\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"solution\"\u003eSolution\u003c/h2\u003e\n\u003ch3 id=\"data-collection-strategy\"\u003eData Collection Strategy\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGeo point-based sampling\u003c/strong\u003e: Over 1,100 manually labeled points across India\u0026rsquo;s coastal regions\u003c/li\u003e\n\u003cli\u003eUsed Global Mangrove Watch data for reference\u003c/li\u003e\n\u003cli\u003eSeparated into mangrove/non-mangrove classes\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"feature-engineering\"\u003eFeature Engineering\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSpectral bands\u003c/strong\u003e: B2, B3, B4, B8, B11 from Sentinel-2A\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIndices\u003c/strong\u003e: NDVI (Normalized Difference Vegetation Index), NDWI (Normalized Difference Water Index)\u003c/li\u003e\n\u003cli\u003eMulti-spectral analysis for robust classification\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"model-performance\"\u003eModel Performance\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAlgorithm\u003c/strong\u003e: Random Forest Classifier\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAccuracy\u003c/strong\u003e: 99.03% on test data\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGeneralization\u003c/strong\u003e: Strong performance on unseen regions\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"deployment\"\u003eDeployment\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGoogle Earth Engine\u003c/strong\u003e tile exports for classified imagery\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCustom Streamlit web application\u003c/strong\u003e for real-time visualization\u003c/li\u003e\n\u003cli\u003eInteractive classification by user clicks on map\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"technologies-used\"\u003eTechnologies Used\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eGoogle Earth Engine (GEE)\u003c/li\u003e\n\u003cli\u003eSentinel-2A Multispectral Imagery\u003c/li\u003e\n\u003cli\u003eRandom Forest\u003c/li\u003e\n\u003cli\u003eStreamlit\u003c/li\u003e\n\u003cli\u003ePython\u003c/li\u003e\n\u003cli\u003eGeospatial Analysis\u003c/li\u003e\n\u003c/ul\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Overview Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking Key Features Real-time lane detection and tracking Obstacle avoidance system Autonomous navigation Sensor fusion for decision making Technologies Used Raspberry Pi OpenCV Python Computer Vision IoT Sensors ","permalink":"https://shubhamapat7.github.io/projects/real-time-self-driving-car/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"key-features\"\u003eKey Features\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eReal-time lane detection and tracking\u003c/li\u003e\n\u003cli\u003eObstacle avoidance system\u003c/li\u003e\n\u003cli\u003eAutonomous navigation\u003c/li\u003e\n\u003cli\u003eSensor fusion for decision making\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"technologies-used\"\u003eTechnologies Used\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRaspberry Pi\u003c/li\u003e\n\u003cli\u003eOpenCV\u003c/li\u003e\n\u003cli\u003ePython\u003c/li\u003e\n\u003cli\u003eComputer Vision\u003c/li\u003e\n\u003cli\u003eIoT Sensors\u003c/li\u003e\n\u003c/ul\u003e","title":"Real-Time Self-Driving Car with Lane Detection"}]