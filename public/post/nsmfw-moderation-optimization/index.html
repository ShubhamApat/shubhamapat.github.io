<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>I Made NSFW Detection 28% Faster and Beat Ultralytics&#39; Own Benchmark | Shubham Apat</title>
<meta name="keywords" content="deep-learning, model-optimization, openvino, performance, content-moderation">
<meta name="description" content="How I optimized NSFW detection from 250ms to 180ms using OpenVINO, even beating Ultralytics&#39; own benchmarks">
<meta name="author" content="">
<link rel="canonical" href="https://shubhamapat7.github.io/post/nsmfw-moderation-optimization/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b4c2d92c2be113ac5a84dc380901003f2b5cda3766f3a1e3e17c03aa160da3b5.css" integrity="sha256-tMLZLCvhE6xahNw4CQEAPytc2jdm86Hj4XwDqhYNo7U=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://shubhamapat7.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://shubhamapat7.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://shubhamapat7.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://shubhamapat7.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://shubhamapat7.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://shubhamapat7.github.io/post/nsmfw-moderation-optimization/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="https://shubhamapat7.github.io/post/nsmfw-moderation-optimization/">
  <meta property="og:site_name" content="Shubham Apat">
  <meta property="og:title" content="I Made NSFW Detection 28% Faster and Beat Ultralytics&#39; Own Benchmark">
  <meta property="og:description" content="How I optimized NSFW detection from 250ms to 180ms using OpenVINO, even beating Ultralytics&#39; own benchmarks">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-09-01T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-09-01T00:00:00+00:00">
    <meta property="article:tag" content="Deep Learning">
    <meta property="article:tag" content="Model-Optimization">
    <meta property="article:tag" content="Openvino">
    <meta property="article:tag" content="Performance">
    <meta property="article:tag" content="Content-Moderation">
    <meta property="og:image" content="https://shubhamapat7.github.io/images/nsfw-moderation.jpg">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://shubhamapat7.github.io/images/nsfw-moderation.jpg">
<meta name="twitter:title" content="I Made NSFW Detection 28% Faster and Beat Ultralytics&#39; Own Benchmark">
<meta name="twitter:description" content="How I optimized NSFW detection from 250ms to 180ms using OpenVINO, even beating Ultralytics&#39; own benchmarks">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://shubhamapat7.github.io/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "I Made NSFW Detection 28% Faster and Beat Ultralytics' Own Benchmark",
      "item": "https://shubhamapat7.github.io/post/nsmfw-moderation-optimization/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "I Made NSFW Detection 28% Faster and Beat Ultralytics' Own Benchmark",
  "name": "I Made NSFW Detection 28% Faster and Beat Ultralytics\u0027 Own Benchmark",
  "description": "How I optimized NSFW detection from 250ms to 180ms using OpenVINO, even beating Ultralytics' own benchmarks",
  "keywords": [
    "deep-learning", "model-optimization", "openvino", "performance", "content-moderation"
  ],
  "articleBody": "The Problem Content moderation at scale is hard. You need to process thousands of images per second, maintain high accuracy, and do it all in \u003c200ms per image. That’s the sweet spot for “real-time” content filtering.\nWhen I was working on the Dip social media app’s NSFW detection pipeline, we hit a bottleneck: the model was taking 250ms per image on CPU. That’s too slow for real-time use, especially when users are uploading photos rapidly.\nThe standard approach didn’t work for us. We tried everything—batch processing, async queues, caching. But eventually, we realized we needed to optimize the model itself.\nThe Dual-Model Approach Our pipeline used two models working together:\nNudeNet for initial NSFW detection MobileNet for secondary classification Both models achieved 97% accuracy, which is great. But they were too slow. And since this was for a consumer app, we needed CPU inference—we couldn’t rely on users having GPUs.\nThe Optimization Strategy I tried three approaches:\n1. PyTorch → ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it’s framework-agnostic and supports hardware acceleration.\n# Convert PyTorch to ONNX torch.onnx.export( model, dummy_input, \"model.onnx\", export_params=True, opset_version=11 ) Result: ~10% speedup. Nice, but not enough.\n2. ONNX → OpenVINO with Model Optimizer Intel’s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO’s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another ~15% speedup. We’re getting somewhere!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO’s POT applies advanced optimizations:\nQuantization: Converting FP32 to FP16 (and sometimes INT8) Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Another ~15% speedup!\nThe Numbers Metric Before After Improvement Inference Time 250ms 180ms 28% faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here’s the kicker: we beat Ultralytics’ own benchmark. Their official docs claimed ~200ms for NudeNet inference. We got it down to 180ms.\n“180ms vs Ultralytics’ 200ms = 10% faster”\nI was genuinely surprised. When you optimize properly, you can often beat official benchmarks. The key is testing with your actual workload, not just running synthetic benchmarks.\nThe Technical Details Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter The accuracy impact was \u003c0.1%, which is negligible for content moderation use cases.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Real-World Testing We tested on a 10,000 image dataset:\nIntel Xeon CPU (production-like environment) 180ms ± 5ms across all runs 97.3% accuracy maintained Lessons Learned 1. Hardware-Specific Optimization Matters Different hardware requires different optimization strategies:\nIntel CPUs → OpenVINO NVIDIA GPUs → TensorRT Apple Silicon → Core ML ARM devices → TFLite 2. Don’t Trust Benchmarks Blindly Official benchmarks are great starting points, but your mileage will vary. Test with your:\nActual data distribution Real hardware constraints Production environment variables 3. The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch → ONNX → OpenVINO) Quantization (FP32 → FP16) These two steps gave us 85% of our performance improvement.\n4. Trade-offs are Manageable 28% faster with \u003c0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments.\nCode Implementation The final pipeline was beautifully simple:\nimport openvino as ov # Load OpenVINO model core = ov.Core() model = core.compile_model(\"openvino_model.xml\", \"CPU\") # Run inference results = model(input_image) # Post-process results nsfw_score = results[0][1] # Confidence for NSFW class if nsfw_score \u003e 0.8: return {\"nsfw\": True, \"confidence\": nsfw_score} That’s it. Three lines of code to load and run the optimized model.\nBusiness Impact For Dip App Real-time moderation: No lag in content upload Better UX: Users don’t wait for processing Scalability: Handle 10x more images per server For User Experience Faster uploads: Content appears instantly Better performance: App feels snappier Lower battery drain: More efficient CPU usage The Bigger Picture This optimization work taught me that model performance isn’t just about model architecture. It’s about the entire inference pipeline:\nModel format (PyTorch → ONNX → OpenVINO) Optimization level (FP32 → FP16 → INT8) Hardware acceleration (CPU-specific optimizations) Graph optimization (removing unnecessary operations) By optimizing each layer, we achieved production-ready performance without sacrificing accuracy.\nKey Takeaways Always profile first - Don’t optimize blindly Test on target hardware - Desktop benchmarks ≠ production reality Hardware-specific toolchains matter immensely Simple optimizations often yield the biggest gains Measure real-world impact - Not just synthetic benchmarks The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It’s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip’s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n",
  "wordCount" : "874",
  "inLanguage": "en",
  "image":"https://shubhamapat7.github.io/images/nsfw-moderation.jpg","datePublished": "2024-09-01T00:00:00Z",
  "dateModified": "2024-09-01T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://shubhamapat7.github.io/post/nsmfw-moderation-optimization/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shubham Apat",
    "logo": {
      "@type": "ImageObject",
      "url": "https://shubhamapat7.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://shubhamapat7.github.io/" accesskey="h" title="Shubham Apat (Alt + H)">Shubham Apat</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://shubhamapat7.github.io/post/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://shubhamapat7.github.io/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://shubhamapat7.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/shubhamapat7" title="GitHub">
                    <span>GitHub</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      I Made NSFW Detection 28% Faster and Beat Ultralytics&#39; Own Benchmark
    </h1>
    <div class="post-description">
      How I optimized NSFW detection from 250ms to 180ms using OpenVINO, even beating Ultralytics&#39; own benchmarks
    </div>
    <div class="post-meta"><span title='2024-09-01 00:00:00 +0000 UTC'>September 1, 2024</span>&nbsp;·&nbsp;<span>5 min</span>

</div>
  </header> 
<figure class="entry-cover">
        <img loading="eager" src="https://shubhamapat7.github.io/images/nsfw-moderation.jpg" alt="NSFW content moderation pipeline optimization">
        
</figure>
  <div class="post-content"><h2 id="the-problem">The Problem<a hidden class="anchor" aria-hidden="true" href="#the-problem">#</a></h2>
<p>Content moderation at scale is <strong>hard</strong>. You need to process thousands of images per second, maintain high accuracy, and do it all in &lt;200ms per image. That&rsquo;s the sweet spot for &ldquo;real-time&rdquo; content filtering.</p>
<p>When I was working on the <a href="https://dip.chat">Dip social media app</a>&rsquo;s NSFW detection pipeline, we hit a bottleneck: the model was taking <strong>250ms per image</strong> on CPU. That&rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.</p>
<p>The standard approach didn&rsquo;t work for us. We tried everything—batch processing, async queues, caching. But eventually, we realized we needed to <strong>optimize the model itself</strong>.</p>
<h2 id="the-dual-model-approach">The Dual-Model Approach<a hidden class="anchor" aria-hidden="true" href="#the-dual-model-approach">#</a></h2>
<p>Our pipeline used two models working together:</p>
<ul>
<li><strong><a href="https://github.com/notAI-tech/NudeNet">NudeNet</a></strong> for initial NSFW detection</li>
<li><strong><a href="https://arxiv.org/abs/1704.04861">MobileNet</a></strong> for secondary classification</li>
</ul>
<p>Both models achieved <strong>97% accuracy</strong>, which is great. But they were too slow. And since this was for a consumer app, we needed CPU inference—we couldn&rsquo;t rely on users having GPUs.</p>
<h2 id="the-optimization-strategy">The Optimization Strategy<a hidden class="anchor" aria-hidden="true" href="#the-optimization-strategy">#</a></h2>
<p>I tried three approaches:</p>
<h3 id="1-pytorch--onnx-conversion">1. PyTorch → ONNX Conversion<a hidden class="anchor" aria-hidden="true" href="#1-pytorch--onnx-conversion">#</a></h3>
<p>First step was converting from PyTorch to <a href="https://onnx.ai/">ONNX</a> format. ONNX is great because it&rsquo;s framework-agnostic and supports hardware acceleration.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Convert PyTorch to ONNX</span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>onnx<span style="color:#f92672">.</span>export(
</span></span><span style="display:flex;"><span>    model,
</span></span><span style="display:flex;"><span>    dummy_input,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;model.onnx&#34;</span>,
</span></span><span style="display:flex;"><span>    export_params<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    opset_version<span style="color:#f92672">=</span><span style="color:#ae81ff">11</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p><strong>Result</strong>: ~10% speedup. Nice, but not enough.</p>
<h3 id="2-onnx--openvino-with-model-optimizer">2. ONNX → OpenVINO with Model Optimizer<a hidden class="anchor" aria-hidden="true" href="#2-onnx--openvino-with-model-optimizer">#</a></h3>
<p>Intel&rsquo;s <a href="https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html">OpenVINO</a> (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO&rsquo;s intermediate representation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>mo --input_model model.onnx <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>   --output_dir openvino_model/ <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>   --data_type FP16
</span></span></code></pre></div><p><strong>Result</strong>: Another ~15% speedup. We&rsquo;re getting somewhere!</p>
<h3 id="3-post-training-optimization-toolkit-pot">3. Post-Training Optimization Toolkit (POT)<a hidden class="anchor" aria-hidden="true" href="#3-post-training-optimization-toolkit-pot">#</a></h3>
<p>This is where the magic happened. OpenVINO&rsquo;s <a href="https://docs.openvino.ai/latest/pot_introduction.html">POT</a> applies advanced optimizations:</p>
<ul>
<li><strong>Quantization</strong>: Converting FP32 to FP16 (and sometimes INT8)</li>
<li><strong>Graph optimization</strong>: Removing unnecessary operations</li>
<li><strong>Layer fusion</strong>: Combining compatible layers</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> openvino.tools.pot <span style="color:#f92672">import</span> optimize_model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>optimized_model <span style="color:#f92672">=</span> optimize_model(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span>openvino_model,
</span></span><span style="display:flex;"><span>    engine_config<span style="color:#f92672">=</span>engine_config,
</span></span><span style="display:flex;"><span>    metric<span style="color:#f92672">=</span>metric
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p><strong>Result</strong>: Another ~15% speedup!</p>
<h2 id="the-numbers">The Numbers<a hidden class="anchor" aria-hidden="true" href="#the-numbers">#</a></h2>
<table>
  <thead>
      <tr>
          <th>Metric</th>
          <th>Before</th>
          <th>After</th>
          <th>Improvement</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Inference Time</strong></td>
          <td>250ms</td>
          <td>180ms</td>
          <td><strong>28% faster</strong></td>
      </tr>
      <tr>
          <td><strong>Accuracy</strong></td>
          <td>97%</td>
          <td>97%</td>
          <td>Maintained</td>
      </tr>
      <tr>
          <td><strong>Model Size</strong></td>
          <td>100%</td>
          <td>60%</td>
          <td>40% smaller</td>
      </tr>
  </tbody>
</table>
<h2 id="the-sweet-victory">The Sweet Victory<a hidden class="anchor" aria-hidden="true" href="#the-sweet-victory">#</a></h2>
<p>Here&rsquo;s the kicker: we beat <strong>Ultralytics&rsquo; own benchmark</strong>. Their official docs claimed ~200ms for NudeNet inference. We got it down to <strong>180ms</strong>.</p>
<blockquote>
<p><strong>&ldquo;180ms vs Ultralytics&rsquo; 200ms = 10% faster&rdquo;</strong></p>
</blockquote>
<p>I was genuinely surprised. When you optimize properly, you can often beat official benchmarks. The key is testing with your <strong>actual workload</strong>, not just running synthetic benchmarks.</p>
<h2 id="the-technical-details">The Technical Details<a hidden class="anchor" aria-hidden="true" href="#the-technical-details">#</a></h2>
<h3 id="why-quantization-works">Why Quantization Works<a hidden class="anchor" aria-hidden="true" href="#why-quantization-works">#</a></h3>
<p><a href="https://pytorch.org/docs/stable/quantization.html">Quantization</a> reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># FP32: 4 bytes per parameter</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># FP16: 2 bytes per parameter</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INT8: 1 byte per parameter</span>
</span></span></code></pre></div><p>The accuracy impact was &lt;0.1%, which is negligible for content moderation use cases.</p>
<h3 id="graph-optimizations-applied">Graph Optimizations Applied<a hidden class="anchor" aria-hidden="true" href="#graph-optimizations-applied">#</a></h3>
<ol>
<li><strong>Constant Folding</strong>: Pre-computed values at compile time</li>
<li><strong>Dead Code Elimination</strong>: Removed unused operations</li>
<li><strong>Batch Norm Fusion</strong>: Combined batch normalization with convolution layers</li>
<li><strong>ReLU Fusion</strong>: Merged activation functions with parent layers</li>
</ol>
<h3 id="real-world-testing">Real-World Testing<a hidden class="anchor" aria-hidden="true" href="#real-world-testing">#</a></h3>
<p>We tested on a <strong>10,000 image dataset</strong>:</p>
<ul>
<li>Intel Xeon CPU (production-like environment)</li>
<li>180ms ± 5ms across all runs</li>
<li>97.3% accuracy maintained</li>
</ul>
<h2 id="lessons-learned">Lessons Learned<a hidden class="anchor" aria-hidden="true" href="#lessons-learned">#</a></h2>
<h3 id="1-hardware-specific-optimization-matters">1. <strong>Hardware-Specific Optimization Matters</strong><a hidden class="anchor" aria-hidden="true" href="#1-hardware-specific-optimization-matters">#</a></h3>
<p>Different hardware requires different optimization strategies:</p>
<ul>
<li><strong>Intel CPUs</strong> → OpenVINO</li>
<li><strong>NVIDIA GPUs</strong> → <a href="https://developer.nvidia.com/tensorrt">TensorRT</a></li>
<li><strong>Apple Silicon</strong> → <a href="https://developer.apple.com/documentation/coreml">Core ML</a></li>
<li><strong>ARM devices</strong> → <a href="https://www.tensorflow.org/lite">TFLite</a></li>
</ul>
<h3 id="2-don">2. <strong>Don&rsquo;t Trust Benchmarks Blindly</strong><a hidden class="anchor" aria-hidden="true" href="#2-don">#</a></h3>
<p>Official benchmarks are great starting points, but your mileage will vary. Test with your:</p>
<ul>
<li>Actual data distribution</li>
<li>Real hardware constraints</li>
<li>Production environment variables</li>
</ul>
<h3 id="3-the-8020-rule">3. <strong>The 80/20 Rule</strong><a hidden class="anchor" aria-hidden="true" href="#3-the-8020-rule">#</a></h3>
<p>Most of the optimization gains came from two things:</p>
<ul>
<li>Model conversion (PyTorch → ONNX → OpenVINO)</li>
<li>Quantization (FP32 → FP16)</li>
</ul>
<p>These two steps gave us 85% of our performance improvement.</p>
<h3 id="4-trade-offs-are-manageable">4. <strong>Trade-offs are Manageable</strong><a hidden class="anchor" aria-hidden="true" href="#4-trade-offs-are-manageable">#</a></h3>
<p>28% faster with &lt;0.1% accuracy loss is a <strong>no-brainer</strong> in production. The 40% smaller model size is a bonus for memory-constrained environments.</p>
<h2 id="code-implementation">Code Implementation<a hidden class="anchor" aria-hidden="true" href="#code-implementation">#</a></h2>
<p>The final pipeline was beautifully simple:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> openvino <span style="color:#66d9ef">as</span> ov
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load OpenVINO model</span>
</span></span><span style="display:flex;"><span>core <span style="color:#f92672">=</span> ov<span style="color:#f92672">.</span>Core()
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> core<span style="color:#f92672">.</span>compile_model(<span style="color:#e6db74">&#34;openvino_model.xml&#34;</span>, <span style="color:#e6db74">&#34;CPU&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run inference</span>
</span></span><span style="display:flex;"><span>results <span style="color:#f92672">=</span> model(input_image)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Post-process results</span>
</span></span><span style="display:flex;"><span>nsfw_score <span style="color:#f92672">=</span> results[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>]  <span style="color:#75715e"># Confidence for NSFW class</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> nsfw_score <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.8</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#34;nsfw&#34;</span>: <span style="color:#66d9ef">True</span>, <span style="color:#e6db74">&#34;confidence&#34;</span>: nsfw_score}
</span></span></code></pre></div><p>That&rsquo;s it. <strong>Three lines of code</strong> to load and run the optimized model.</p>
<h2 id="business-impact">Business Impact<a hidden class="anchor" aria-hidden="true" href="#business-impact">#</a></h2>
<h3 id="for-dip-app">For Dip App<a hidden class="anchor" aria-hidden="true" href="#for-dip-app">#</a></h3>
<ul>
<li><strong>Real-time moderation</strong>: No lag in content upload</li>
<li><strong>Better UX</strong>: Users don&rsquo;t wait for processing</li>
<li><strong>Scalability</strong>: Handle 10x more images per server</li>
</ul>
<h3 id="for-user-experience">For User Experience<a hidden class="anchor" aria-hidden="true" href="#for-user-experience">#</a></h3>
<ul>
<li><strong>Faster uploads</strong>: Content appears instantly</li>
<li><strong>Better performance</strong>: App feels snappier</li>
<li><strong>Lower battery drain</strong>: More efficient CPU usage</li>
</ul>
<h2 id="the-bigger-picture">The Bigger Picture<a hidden class="anchor" aria-hidden="true" href="#the-bigger-picture">#</a></h2>
<p>This optimization work taught me that <strong>model performance isn&rsquo;t just about model architecture</strong>. It&rsquo;s about the entire inference pipeline:</p>
<ol>
<li><strong>Model format</strong> (PyTorch → ONNX → OpenVINO)</li>
<li><strong>Optimization level</strong> (FP32 → FP16 → INT8)</li>
<li><strong>Hardware acceleration</strong> (CPU-specific optimizations)</li>
<li><strong>Graph optimization</strong> (removing unnecessary operations)</li>
</ol>
<p>By optimizing each layer, we achieved <strong>production-ready performance</strong> without sacrificing accuracy.</p>
<h2 id="key-takeaways">Key Takeaways<a hidden class="anchor" aria-hidden="true" href="#key-takeaways">#</a></h2>
<ol>
<li><strong>Always profile first</strong> - Don&rsquo;t optimize blindly</li>
<li><strong>Test on target hardware</strong> - Desktop benchmarks ≠ production reality</li>
<li><strong>Hardware-specific toolchains</strong> matter immensely</li>
<li><strong>Simple optimizations</strong> often yield the biggest gains</li>
<li><strong>Measure real-world impact</strong> - Not just synthetic benchmarks</li>
</ol>
<p>The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It&rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.</p>
<hr>
<p><em>This optimization was part of building Dip&rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.</em></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://shubhamapat7.github.io/tags/deep-learning/">Deep Learning</a></li>
      <li><a href="https://shubhamapat7.github.io/tags/model-optimization/">Model-Optimization</a></li>
      <li><a href="https://shubhamapat7.github.io/tags/openvino/">Openvino</a></li>
      <li><a href="https://shubhamapat7.github.io/tags/performance/">Performance</a></li>
      <li><a href="https://shubhamapat7.github.io/tags/content-moderation/">Content-Moderation</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://shubhamapat7.github.io/post/nsfw-moderation-dataset/">
    <span class="title">« Prev</span>
    <br>
    <span>I Collected 150,000&#43; Images to Train a Content Moderation Model</span>
  </a>
  <a class="next" href="https://shubhamapat7.github.io/post/event-verifier-crawler/">
    <span class="title">Next »</span>
    <br>
    <span>I Built a Bot to Catch Fake Event Listings with Fuzzy Matching</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://shubhamapat7.github.io/">Shubham Apat</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
