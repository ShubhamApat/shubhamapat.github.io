<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head>
	<meta name="generator" content="Hugo 0.152.2"><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Shubham Apat</title>

<meta name="description" content="">
<meta name="author" content="">
<link rel="canonical" href="https://shubhamapat7.github.io/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b4c2d92c2be113ac5a84dc380901003f2b5cda3766f3a1e3e17c03aa160da3b5.css" integrity="sha256-tMLZLCvhE6xahNw4CQEAPytc2jdm86Hj4XwDqhYNo7U=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://shubhamapat7.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://shubhamapat7.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://shubhamapat7.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://shubhamapat7.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://shubhamapat7.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="https://shubhamapat7.github.io/index.xml" title="rss">
<link rel="alternate" type="application/json" href="https://shubhamapat7.github.io/index.json" title="json">
<link rel="alternate" hreflang="en" href="https://shubhamapat7.github.io/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="https://shubhamapat7.github.io/">
  <meta property="og:site_name" content="Shubham Apat">
  <meta property="og:title" content="Home">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Home">
<meta name="twitter:description" content="">

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Organization",
  "name": "Shubham Apat",
  "url": "https://shubhamapat7.github.io/",
  "description": "",
  "logo": "https://shubhamapat7.github.io/favicon.ico",
  "sameAs": [
      "https://github.com/shubhamapat7", "https://linkedin.com/in/shubham-apat", "mailto:shubhamapat24k@gmail.com"
  ]
}
</script>
</head>
<body class="list" id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://shubhamapat7.github.io/" accesskey="h" title="Shubham Apat (Alt + H)">Shubham Apat</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://shubhamapat7.github.io/post/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://shubhamapat7.github.io/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://shubhamapat7.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/shubhamapat7" title="GitHub">
                    <span>GitHub</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<article class="first-entry home-info">
    <div class="home-info-left">
        <img loading="lazy" draggable="false" src="https://shubhamapat7.github.io/images/profile2.jpg" alt="profile image" title="" height="300" width="300">
    </div>
    <div class="home-info-right">
        <div class="entry-content">
            <ul>
                <li>I'm a Data Scientist in Ahmedabad, currently working at a startup <a href="https://dipapp.in/">ByteCitadel</a>, before I went into industry I was a research intern contributing to<a href="https://www.sciencedirect.com/science/article/abs/pii/S1574954122003247"> Identification of free-ranging mugger crocodiles by applying deep learning methods on UAV imagery</a></li>
                <li>In my spare time, I build <a href="https://shubhamapat7.github.io/projects/">data science projects</a>, solves kaggle challenges, and writes reviews about movies on letterboxd.</li>
                <li><em>All projects and blogs content are documented here for learning and reference. Feel free to reach out for collaborations!</em></li>
            </ul>
        </div>
        <footer class="entry-footer">
            <div class="social-icons">
                <a href="https://github.com/shubhamapat7" target="_blank" rel="noopener noreferrer me" title="Github"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
                </a>
                <a href="https://linkedin.com/in/shubham-apat" target="_blank" rel="noopener noreferrer me" title="Linkedin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
    <rect x="2" y="9" width="4" height="12"></rect>
    <circle cx="4" cy="4" r="2"></circle>
</svg>
                </a>
                <a href="mailto:shubhamapat24k@gmail.com" target="_blank" rel="noopener noreferrer me" title="Email"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 21" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path>
    <polyline points="22,6 12,13 2,6"></polyline>
</svg>
                </a>
            </div>
        </footer>
    </div>
</article>

<article class="post-entry"> 
<figure class="entry-cover">
        <img loading="lazy" src="https://shubhamapat7.github.io/images/university-degrees.jpg" alt="Collection of university degree certificates from around the world">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">I Collected 1,400 University Degrees to Build a Verification Dataset
    </h2>
  </header>
  <div class="entry-content">
    <p>The Goal Building systems for degree verification and academic fraud detection requires authentic examples. I collected degrees from LinkedIn graduation posts across 8 countries.
Coverage 1,400 unique degrees from:
USA: Ivy League, top universities, business schools India: IITs, IIMs, AIIMS UK: Oxford, Cambridge, Russell Group Australia: Group of Eight (Go8) Europe: Top institutions (ETH Zurich, TUM, etc.) Asia: NUS, NTU, Seoul National, University of Tokyo Collection Method def collect_graduation_posts(keywords, universities): posts = [] for university in universities: search_queries = [ f&#34;{university} graduation 2024&#34;, f&#34;{university} degree ceremony&#34;, f&#34;graduated from {university}&#34; ] for query in search_queries: results = linkedin_scraper.search_posts( query=query, limit=50, date_range=&#34;2023-2024&#34; ) posts.extend(results) return posts Quality Control Deduplication Used perceptual hashing to remove duplicates:
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-11-01 00:00:00 +0000 UTC'>November 1, 2024</span>&nbsp;·&nbsp;<span>2 min</span></footer>
  <a class="entry-link" aria-label="post link to I Collected 1,400 University Degrees to Build a Verification Dataset" href="https://shubhamapat7.github.io/post/university-degrees-dataset/"></a>
</article>

<article class="post-entry"> 
<figure class="entry-cover">
        <img loading="lazy" src="https://shubhamapat7.github.io/images/ReCaptcha-blog-article-screen-2.png" alt="Description of the image">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">I bypassed Google reCAPTCHA and LinkedIn&#39;s Anti-Scraping
    </h2>
  </header>
  <div class="entry-content">
    <p>LinkedIn doesn’t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., “LLMs”,“transformers”,…., anything people posts about on linkedin.
The catch: No login required, and it has to bypass anti-bot measures.
The Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google’s search with “site:linkedin.com/posts llms” and it gave me links to all the popular posts about llms on linkedin.
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-10-20 00:00:00 +0000 UTC'>October 20, 2024</span>&nbsp;·&nbsp;<span>5 min</span></footer>
  <a class="entry-link" aria-label="post link to I bypassed Google reCAPTCHA and LinkedIn&#39;s Anti-Scraping" href="https://shubhamapat7.github.io/projects/linkedin-scraper/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">I Used OpenAI CLIP for Image Moderation - No Training Required
    </h2>
  </header>
  <div class="entry-content">
    <p>The Approach Traditional image classification requires:
Thousands of labeled images Weeks of training GPU resources CLIP is different: it already “understands” images through natural language.
The Method 1. Define Prompts Instead of training, I wrote text prompts for each class:
# NSFW class prompts nsfw_prompts = [ &#34;a nude person&#34;, &#34;explicit sexual content&#34;, &#34;inappropriate intimate image&#34;, &#34;adult content not suitable for work&#34;, # ... 60 total prompts ] # NEUTRAL class prompts neutral_prompts = [ &#34;a normal everyday photo&#34;, &#34;safe for work content&#34;, &#34;appropriate business image&#34;, &#34;general photography&#34;, # ... 60 total prompts ] 2. Compute Similarity import clip import torch # Load CLIP model model, preprocess = clip.load(&#34;ViT-B/32&#34;) device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34; model.to(device) def classify_image(image_path): # Load and preprocess image image = preprocess(Image.open(image_path)).unsqueeze(0).to(device) # Get image features with torch.no_grad(): image_features = model.encode_image(image) # Compare with all class prompts best_class = None best_score = -1 for class_name, prompts in classes.items(): class_scores = [] for prompt in prompts: # Tokenize text text = clip.tokenize(prompt).to(device) # Get text features with torch.no_grad(): text_features = model.encode_text(text) # Compute similarity (cosine similarity) similarity = torch.cosine_similarity(image_features, text_features) class_scores.append(similarity.item()) # Average score for this class avg_score = np.mean(class_scores) if avg_score &gt; best_score: best_score = avg_score best_class = class_name return best_class, best_score 3. Grid Search for Best Thresholds from sklearn.model_selection import GridSearchCV # Optimize decision threshold thresholds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7] best_threshold = 0.5 best_f1 = 0 for threshold in thresholds: predictions = [1 if score &gt; threshold else 0 for score in similarity_scores] f1 = f1_score(true_labels, predictions) if f1 &gt; best_f1: best_f1 = f1 best_threshold = threshold The Results Metric Value Overall Accuracy 92% NSFW Detection 94% precision Violence Detection 89% precision Neutral Images 96% precision Why This Works CLIP was trained on 400 million image-text pairs. It learned to understand the relationship between images and words.
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-10-01 00:00:00 +0000 UTC'>October 1, 2024</span>&nbsp;·&nbsp;<span>3 min</span></footer>
  <a class="entry-link" aria-label="post link to I Used OpenAI CLIP for Image Moderation - No Training Required" href="https://shubhamapat7.github.io/projects/image-moderation-clip/"></a>
</article>

<article class="post-entry"> 
<figure class="entry-cover">
        <img loading="lazy" src="https://shubhamapat7.github.io/images/image-dataset.jpg" alt="Large-scale image dataset collection for content moderation">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">I Collected 150,000&#43; Images to Train a Content Moderation Model
    </h2>
  </header>
  <div class="entry-content">
    <p>The Challenge Building a content moderation system for a social media app requires training data. Lots of it.
Dip needed to automatically detect:
NSFW content Violence Drugs Weapons Hateful content Neutral content The question: where do you get 150,000&#43; labeled images?
This was my task at ByteCitadel: collect, curate, and label a massive dataset for Dip’s image moderation pipeline.
The Plan Six categories, ~30,000 images each:
Violence (30K) Drugs (30K) NSFW (30K) Neutral (30K) Hateful Memes (30K) Weapons (30K) Total: 180,000 images (we collected extra for quality control)
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-09-15 00:00:00 +0000 UTC'>September 15, 2024</span>&nbsp;·&nbsp;<span>5 min</span></footer>
  <a class="entry-link" aria-label="post link to I Collected 150,000&#43; Images to Train a Content Moderation Model" href="https://shubhamapat7.github.io/post/nsfw-moderation-dataset/"></a>
</article>

<article class="post-entry"> 
<figure class="entry-cover">
        <img loading="lazy" src="https://shubhamapat7.github.io/images/nsfw-moderation.jpg" alt="NSFW content moderation pipeline optimization">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">I Made NSFW Detection 28% Faster and Beat Ultralytics&#39; Own Benchmark
    </h2>
  </header>
  <div class="entry-content">
    <p>The Problem Content moderation at scale is hard. You need to process thousands of images per second, maintain high accuracy, and do it all in &lt;200ms per image. That’s the sweet spot for “real-time” content filtering.
When I was working on the Dip social media app’s NSFW detection pipeline, we hit a bottleneck: the model was taking 250ms per image on CPU. That’s too slow for real-time use, especially when users are uploading photos rapidly.
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-09-01 00:00:00 +0000 UTC'>September 1, 2024</span>&nbsp;·&nbsp;<span>5 min</span></footer>
  <a class="entry-link" aria-label="post link to I Made NSFW Detection 28% Faster and Beat Ultralytics&#39; Own Benchmark" href="https://shubhamapat7.github.io/post/nsmfw-moderation-optimization/"></a>
</article>

<article class="post-entry"> 
<figure class="entry-cover">
        <img loading="lazy" src="https://shubhamapat7.github.io/images/event-verifier.jpg" alt="Event verification system using fuzzy string matching">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">I Built a Bot to Catch Fake Event Listings with Fuzzy Matching
    </h2>
  </header>
  <div class="entry-content">
    <p>The Problem Event platforms like BookMyShow are full of fake listings. Users waste money on non-existent events. Platforms need automated verification.
The solution: compare claimed event details with actual page content using fuzzy string matching.
The Approach from fuzzywuzzy import fuzz def verify_event_credibility(claimed_info, actual_content): scores = {} # Compare titles title_score = fuzz.partial_ratio( claimed_info[&#39;title&#39;].lower(), actual_content[&#39;title&#39;].lower() ) # Compare descriptions desc_score = fuzz.token_sort_ratio( claimed_info[&#39;description&#39;], actual_content[&#39;description&#39;] ) # Overall credibility score overall_score = (title_score * 0.3 &#43; desc_score * 0.4 &#43; date_score * 0.2 &#43; location_score * 0.1) return { &#39;overall_score&#39;: overall_score, &#39;credible&#39;: overall_score &gt; 75 } Results 5,000 events tested 92% accuracy 70% reduction in fake event complaints Key Takeaway Simple fuzzy matching can solve complex fraud detection problems.
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-08-01 00:00:00 +0000 UTC'>August 1, 2024</span>&nbsp;·&nbsp;<span>1 min</span></footer>
  <a class="entry-link" aria-label="post link to I Built a Bot to Catch Fake Event Listings with Fuzzy Matching" href="https://shubhamapat7.github.io/post/event-verifier-crawler/"></a>
</article>

<article class="post-entry"> 
<figure class="entry-cover">
        <img loading="lazy" src="https://shubhamapat7.github.io/images/property-scraper.jpg" alt="Property listing data collection from multiple platforms">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">I Scraped 70K Property Listings to Help Students Find Housing
    </h2>
  </header>
  <div class="entry-content">
    <p>The Mission College students in India struggle to find affordable PGs and hostels. I scraped data from three major property sites to build a comprehensive database.
The Stack class PropertyScraper: def __init__(self): self.scrapers = { &#39;magicbricks&#39;: MagicBricksScraper(), &#39;justdial&#39;: JustDialScraper(), &#39;99acres&#39;: NinetyNineAcresScraper() } Each platform needed custom scraping logic.
The Numbers Platform Properties Success Rate Magic Bricks 28,500 94% JustDial 25,000 89% 99acres 16,500 91% Total 70,000 91% Challenges Anti-bot measures: CAPTCHA, IP blocking, rate limiting Solution: Rotated user agents, added delays, proxy rotation
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-07-15 00:00:00 +0000 UTC'>July 15, 2024</span>&nbsp;·&nbsp;<span>1 min</span></footer>
  <a class="entry-link" aria-label="post link to I Scraped 70K Property Listings to Help Students Find Housing" href="https://shubhamapat7.github.io/post/property-scraper-analysis/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Student ID Verification Pipeline for dip
    </h2>
  </header>
  <div class="entry-content">
    <p>Overview Built a multi-stage pipeline for verifying student ID cards using advanced deep learning models to detect various types of forgery and tampering.
Pipeline Architecture Test Dataset 97 manually collected and cleaned ID card images Diverse ID card formats and designs Model Components 1. Face Detection (YOLOv8) High-accuracy face detection in ID cards Fast inference for real-time processing 2. Gender Validation DeepFace-based gender classification Accuracy: 80% (with potential for improvement via dedicated training) 3. Splicing Detection Model: Image Splicing Detector (image_splicer_quant.tflite) Architecture: DenseNet121 with ELA preprocessing Training Data: CASIA dataset Preprocessing: Error Level Analysis (ELA) Performance: Train accuracy: 97.35% Validation accuracy: 82.31% Output: Splicing score (threshold: 0.90) Average test score: 0.95 4. Copy-Move Detection Model: New UNet (new_unet.tflite) Training: COMOFOD dataset Training: 30 epochs Accuracy: 98.7% Output: Copy-move score (threshold: 0.22) Average test score: 0.19 OCR &amp; Text Validation File: speed_uppp2.py Features: OCR text extraction Text consistency check Keyword matching (case/special character validation) Keyword match percentage calculation ELA analysis Pipeline Flow Face Detection (FaceDetection_YOLO.py) Gender Validation (gender_validation.py) Splicing Detection (splicing_detector.py) Copy-Move Detection (copymove_detector.py) OCR &amp; Text Analysis (speed_uppp2.py) Validation Results Successfully processes diverse ID card formats Robust detection of multiple forgery types Multi-layer validation ensures high accuracy Real-time processing capability Technologies Used YOLOv8 TensorFlow Lite UNet DenseNet121 DeepFace OCR (Optical Character Recognition) Error Level Analysis (ELA) Python Computer Vision </p>
  </div>
  <footer class="entry-footer"><span title='2024-06-10 00:00:00 +0000 UTC'>June 10, 2024</span>&nbsp;·&nbsp;<span>2 min</span></footer>
  <a class="entry-link" aria-label="post link to Student ID Verification Pipeline for dip" href="https://shubhamapat7.github.io/projects/student-id-verification-pipeline/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">I Optimized a 3GB Dashboard from 90 Seconds to 1 Second (90x Speedup)
    </h2>
  </header>
  <div class="entry-content">
    <p>The Problem Students need to find:
PGs and hostels near colleges Restaurants and cafes Study spaces Other student services I scraped Google Maps for 3GB of business data around colleges in 7 major Indian cities.
Building the dashboard was easy. Making it fast? That’s where it got interesting.
First Version: Painfully Slow # This took 90&#43; seconds to load df = pd.read_csv(&#39;businesses.csv&#39;) # 3GB CSV filtered_df = df[df[&#39;city&#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90&#43; second load time. No one would use this.
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-05-01 00:00:00 +0000 UTC'>May 1, 2024</span>&nbsp;·&nbsp;<span>3 min</span></footer>
  <a class="entry-link" aria-label="post link to I Optimized a 3GB Dashboard from 90 Seconds to 1 Second (90x Speedup)" href="https://shubhamapat7.github.io/projects/college-hotspots-dashboard/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Individual Identification of Mugger Crocodiles using YOLOv8
    </h2>
  </header>
  <div class="entry-content">
    <p>Built a YOLOv8-based model to identify 160&#43; individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.
Optimized training on Param Shavak supercomputer using custom CLI scripts for data processing.
</p>
  </div>
  <footer class="entry-footer"><span title='2024-04-20 00:00:00 +0000 UTC'>April 20, 2024</span>&nbsp;·&nbsp;<span>1 min</span></footer>
  <a class="entry-link" aria-label="post link to Individual Identification of Mugger Crocodiles using YOLOv8" href="https://shubhamapat7.github.io/projects/individual-identification-of-mugger-crocodile/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="next" href="https://shubhamapat7.github.io/page/2/">Next&nbsp;&nbsp;»
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://shubhamapat7.github.io/">Shubham Apat</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
