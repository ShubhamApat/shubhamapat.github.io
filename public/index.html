<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head>
	<meta name="generator" content="Hugo 0.152.2"><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Shubham Apat</title>

<meta name="description" content="">
<meta name="author" content="">
<link rel="canonical" href="https://shubhamapat7.github.io/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.6e04ee46d2c7afece6b54a05f2620f6336a323618852a4391ecff8c50f09cf28.css" integrity="sha256-bgTuRtLHr&#43;zmtUoF8mIPYzajI2GIUqQ5Hs/4xQ8Jzyg=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://shubhamapat7.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://shubhamapat7.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://shubhamapat7.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://shubhamapat7.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://shubhamapat7.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="https://shubhamapat7.github.io/index.xml" title="rss">
<link rel="alternate" type="application/json" href="https://shubhamapat7.github.io/index.json" title="json">
<link rel="alternate" hreflang="en" href="https://shubhamapat7.github.io/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="https://shubhamapat7.github.io/">
  <meta property="og:site_name" content="Shubham Apat">
  <meta property="og:title" content="Home">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Home">
<meta name="twitter:description" content="">

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Organization",
  "name": "Shubham Apat",
  "url": "https://shubhamapat7.github.io/",
  "description": "",
  "logo": "https://shubhamapat7.github.io/favicon.ico",
  "sameAs": [
      "https://github.com/shubhamapat7", "https://linkedin.com/in/shubham-apat", "mailto:shubhamapat24k@gmail.com"
  ]
}
</script>
</head>
<body class="list" id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://shubhamapat7.github.io/" accesskey="h" title="Shubham Apat (Alt + H)">Shubham Apat</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://shubhamapat7.github.io/post/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://shubhamapat7.github.io/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://shubhamapat7.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/shubhamapat7" title="GitHub">
                    <span>GitHub</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<article class="first-entry home-info">
    <div class="home-info-left">
        <img loading="lazy" draggable="false" src="https://shubhamapat7.github.io/images/profile2.jpg" alt="profile image" title="" height="300" width="300">
    </div>
    <div class="home-info-right">
        <div class="entry-content">
            <ul>
                <li><strong>Shubham Apat</strong> is a Data Scientist/Analyst specializing in machine learning and data analytics. Currently pursuing studies in data science and working on various ML projects.</li>
                <li>In his spare time, Shubham builds <a href="https://shubhamapat7.github.io/projects/">data science projects</a>, contributes to <a href="https://github.com/shubhamapat7">open source on GitHub</a>, and writes about <a href="https://shubhamapat7.github.io/post/">analytics and machine learning</a>.</li>
                <li><em>All projects and content are documented here for learning and reference. Feel free to reach out for collaborations!</em></li>
            </ul>
        </div>
        <footer class="entry-footer">
            <div class="social-icons">
                <a href="https://github.com/shubhamapat7" target="_blank" rel="noopener noreferrer me" title="Github"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
                </a>
                <a href="https://linkedin.com/in/shubham-apat" target="_blank" rel="noopener noreferrer me" title="Linkedin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
    <rect x="2" y="9" width="4" height="12"></rect>
    <circle cx="4" cy="4" r="2"></circle>
</svg>
                </a>
                <a href="mailto:shubhamapat24k@gmail.com" target="_blank" rel="noopener noreferrer me" title="Email"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 21" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path>
    <polyline points="22,6 12,13 2,6"></polyline>
</svg>
                </a>
            </div>
        </footer>
    </div>
</article>

<article class="post-entry"> 
<figure class="entry-cover">
        <img loading="lazy" src="https://shubhamapat7.github.io/images/university-degrees.jpg" alt="Collection of university degree certificates from around the world">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">Collecting 1,400 University Degrees: A Global Dataset
    </h2>
  </header>
  <div class="entry-content">
    <p>Introduction Building systems for degree verification and academic fraud detection requires diverse, authentic examples. I collected a comprehensive dataset of 1,400 unique university degrees from LinkedIn graduation posts across 8 countries, creating a valuable resource for academic institutions and verification services.
Project Vision Use Cases Academic Verification: Automated degree authenticity checks Fraud Detection: Identify fake or tampered certificates University Research: Analyze degree formats and styles Machine Learning: Train models for document verification Market Analysis: Track educational trends globally Scope 1,400 unique degrees 8 countries/regions Top universities worldwide Real, verified degree images Geographic Coverage Strategy 1. United States Target Institutions:
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-11-01 00:00:00 +0000 UTC'>November 1, 2024</span>&nbsp;·&nbsp;<span>6 min</span></footer>
  <a class="entry-link" aria-label="post link to Collecting 1,400 University Degrees: A Global Dataset" href="https://shubhamapat7.github.io/post/university-degrees-dataset/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">LinkedIn Scraper - Comprehensive Social Media Data Extraction
    </h2>
  </header>
  <div class="entry-content">
    <p>Overview Built a comprehensive LinkedIn scraping tool that can extract posts, profiles, and job listings without requiring authentication, using advanced anti-detection techniques.
Features Data Extraction Capabilities Topic-based Posts
Query any topic (e.g., “LLMs”) Configurable number of posts (e.g., 50 recent posts) Dynamic input for flexible scraping User Profiles
Complete profile information extraction Profile descriptions and details Professional history Company Profiles
Company information extraction Business details and descriptions Company posts and updates Job Listings
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-10-20 00:00:00 +0000 UTC'>October 20, 2024</span>&nbsp;·&nbsp;<span>2 min</span></footer>
  <a class="entry-link" aria-label="post link to LinkedIn Scraper - Comprehensive Social Media Data Extraction" href="https://shubhamapat7.github.io/projects/linkedin-scraper/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Image Moderation using OpenAI CLIP
    </h2>
  </header>
  <div class="entry-content">
    <p>Overview Leveraged OpenAI’s CLIP (Contrastive Language-Image Pre-training) model for building an advanced image moderation pipeline with high accuracy and flexibility.
Classification System Classes NSFW SEXY NEUTRAL VIOLENCE DRUGS Methodology CLIP uses both vision and text encoders to understand images through natural language descriptions.
Approach Prompt Engineering Prompts per Class: 60&#43; text prompts per category Strategy: Diverse descriptive prompts for each class Text Encoder: CLIP’s language model generates embeddings Classification: Based on similarity scores between image and text embeddings Scoring Mechanism Similarity Calculation: Vision encoder processes input image Text Matching: Compare against all class prompts Normalization: Sigmoid function on similarity scores Classification: Highest score determines class Model Performance Optimization Techniques Grid Search CV: Systematic hyperparameter exploration Hyperparameter Tuning: Fine-tuned model parameters False Positive Analysis: Analyzed misclassifications per class Results Overall Accuracy: 92% Robust Classification: Effective across all 5 classes Generalization: Strong performance on diverse image types Advantages of CLIP Approach Flexibility: Easy to add new classes with prompts Zero-shot Learning: Can classify without class-specific training Multi-modal Understanding: Leverages both visual and textual understanding Scalable: Simple to expand classification categories Technologies Used OpenAI CLIP Computer Vision Multi-modal AI Prompt Engineering Hyperparameter Tuning Python Image Classification Natural Language Processing </p>
  </div>
  <footer class="entry-footer"><span title='2024-10-01 00:00:00 +0000 UTC'>October 1, 2024</span>&nbsp;·&nbsp;<span>1 min</span></footer>
  <a class="entry-link" aria-label="post link to Image Moderation using OpenAI CLIP" href="https://shubhamapat7.github.io/projects/image-moderation-clip/"></a>
</article>

<article class="post-entry"> 
<figure class="entry-cover">
        <img loading="lazy" src="https://shubhamapat7.github.io/images/image-dataset.jpg" alt="Large-scale image dataset collection for content moderation">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">Building a 150K&#43; Image Dataset for Content Moderation
    </h2>
  </header>
  <div class="entry-content">
    <p>Introduction Building an effective content moderation system requires a diverse, high-quality dataset. For the Dip social media app, I collected and curated a massive dataset of 150,000&#43; images across 6 content categories to train robust moderation models.
This post details the comprehensive data collection and curation process.
Dataset Specifications Scale &amp; Classes Total Images: 150,000&#43; Per Class: ~30,000 images Classes: Violence Drugs NSFW Neutral Hateful Memes Weapons Data Sources Strategy Multi-Source Approach Collecting from diverse sources ensures dataset diversity and prevents bias:
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-09-15 00:00:00 +0000 UTC'>September 15, 2024</span>&nbsp;·&nbsp;<span>3 min</span></footer>
  <a class="entry-link" aria-label="post link to Building a 150K&#43; Image Dataset for Content Moderation" href="https://shubhamapat7.github.io/post/nsfw-moderation-dataset/"></a>
</article>

<article class="post-entry"> 
<figure class="entry-cover">
        <img loading="lazy" src="https://shubhamapat7.github.io/images/nsfw-moderation.jpg" alt="NSFW content moderation pipeline optimization">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">Optimizing NSFW Content Moderation: From 250ms to 180ms
    </h2>
  </header>
  <div class="entry-content">
    <p>Introduction Content moderation at scale requires real-time processing with high accuracy. When working on the Dip social media app’s NSFW detection pipeline, I faced a challenge: the inference time was too slow for real-time use.
This post shares how I optimized the pipeline from 250ms to 180ms per image - a 28% improvement that even beat Ultralytics’ own benchmarks.
The Problem Initial Performance Inference Time: 250ms per image (CPU) Accuracy: 97% Target: &lt;200ms for real-time moderation Benchmark: Ultralytics’ own performance standards The pipeline used a dual-model approach:
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-09-01 00:00:00 +0000 UTC'>September 1, 2024</span>&nbsp;·&nbsp;<span>3 min</span></footer>
  <a class="entry-link" aria-label="post link to Optimizing NSFW Content Moderation: From 250ms to 180ms" href="https://shubhamapat7.github.io/post/nsmfw-moderation-optimization/"></a>
</article>

<article class="post-entry"> 
<figure class="entry-cover">
        <img loading="lazy" src="https://shubhamapat7.github.io/images/event-verifier.jpg" alt="Event verification system using fuzzy string matching">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">Event Verifier: Detecting Fake Event Listings with Fuzzy Matching
    </h2>
  </header>
  <div class="entry-content">
    <p>Introduction Event platforms often struggle with fake or unverified listings. Users waste time and money on events that don’t exist or don’t match their descriptions. I built the Event Verifier - a generalized crawler that automatically checks event credibility using intelligent content analysis.
The Problem Fake Event Listings Event platforms face several issues:
Fraudulent organizers posting non-existent events Outdated information - events that already happened Misleading descriptions - event doesn’t match reality No verification system to catch these issues Manual Verification is Impossible Thousands of events posted daily Human review is slow and expensive Inconsistent quality across reviewers Solution: Event Verifier Core Concept Compare the claimed event details with the actual page content using fuzzy string matching. If they don’t align, the event is suspicious.
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-08-01 00:00:00 +0000 UTC'>August 1, 2024</span>&nbsp;·&nbsp;<span>5 min</span></footer>
  <a class="entry-link" aria-label="post link to Event Verifier: Detecting Fake Event Listings with Fuzzy Matching" href="https://shubhamapat7.github.io/post/event-verifier-crawler/"></a>
</article>

<article class="post-entry"> 
<figure class="entry-cover">
        <img loading="lazy" src="https://shubhamapat7.github.io/images/property-scraper.jpg" alt="Property listing data collection from multiple platforms">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">Scraping 70K Property Listings: PG &amp; Hostel Data Collection
    </h2>
  </header>
  <div class="entry-content">
    <p>Introduction Finding affordable PGs (Paying Guest) and hostels near colleges is a major challenge for students. I built scrapers to collect comprehensive data from India’s top property sites: Magic Bricks, JustDial, and 99acres - resulting in a database of 70,000&#43; property listings.
Project Goal Target Audience College students seeking affordable accommodation Parents looking for safe housing for their children Platforms needing comprehensive property data What We Collected PGs (Paying Guest): Shared/dorm-style accommodations Hostels: Budget-friendly student housing Location: Major cities across India Total: 70,000&#43; unique properties Platform Strategy 1. Magic Bricks Focus: Professional property listings Strengths: Detailed property information, good search filters Challenge: Anti-bot measures 2. JustDial Focus: Local businesses and services Strengths: Wide coverage, local listings Challenge: Inconsistent data structure 3. 99acres Focus: Residential and commercial properties Strengths: Comprehensive property details Challenge: Dynamic content loading Technical Approach Unified Scraping Architecture class PropertyScraper: def __init__(self): self.scrapers = { &#39;magicbricks&#39;: MagicBricksScraper(), &#39;justdial&#39;: JustDialScraper(), &#39;99acres&#39;: NinetyNineAcresScraper() } def scrape_all_platforms(self, cities, property_type): all_properties = [] for platform, scraper in self.scrapers.items(): print(f&#34;Scraping {platform}...&#34;) properties = scraper.scrape( cities=cities, property_type=property_type ) all_properties.extend(properties) return all_properties Platform-Specific Scrapers Magic Bricks Scraper class MagicBricksScraper: def scrape(self, cities, property_type): properties = [] for city in cities: # Search for PG/hostel url = f&#34;https://www.magicbricks.com/property-for-rent/residential/pg-hostel-in-{city}&#34; driver.get(url) # Wait for listings to load time.sleep(3) # Extract listings listings = driver.find_elements(By.CLASS_NAME, &#34;mb-center-sect&#34;) for listing in listings: property_data = { &#39;platform&#39;: &#39;magicbricks&#39;, &#39;city&#39;: city, &#39;title&#39;: listing.find_element(By.CLASS_NAME, &#34;mb-name&#34;).text, &#39;price&#39;: extract_price(listing), &#39;location&#39;: extract_location(listing), &#39;amenities&#39;: extract_amenities(listing), &#39;contact&#39;: extract_contact(listing) } properties.append(property_data) return properties JustDial Scraper class JustDialScraper: def scrape(self, cities, property_type): properties = [] for city in cities: # JustDial uses AJAX heavily query = f&#34;pg hostel {city}&#34; url = f&#34;https://www.justdial.com/{city}/{query}&#34; # Handle dynamic content driver.get(url) scroll_to_bottom(driver) # Extract listings listings = driver.find_elements(By.CSS_SELECTOR, &#34;.store-list&#34;) for listing in listings: property_data = { &#39;platform&#39;: &#39;justdial&#39;, &#39;city&#39;: city, &#39;name&#39;: listing.find_element(By.CSS_SELECTOR, &#34;.store-name&#34;).text, &#39;address&#39;: listing.find_element(By.CSS_SELECTOR, &#34;.address-text&#34;).text, &#39;contact&#39;: extract_contact_number(listing), &#39;rating&#39;: extract_rating(listing) } properties.append(property_data) return properties 99acres Scraper class NinetyNineAcresScraper: def scrape(self, cities, property_type): properties = [] for city in cities: # Search for shared accommodation url = f&#34;https://www.99acres.com/pg-hostel-in-{city}&#34; driver.get(url) wait_for_ajax(driver) # Handle infinite scroll while True: properties = driver.find_elements(By.CLASS_NAME, &#34;projecttuple__tuple&#34;) if load_more_properties(driver): time.sleep(2) else: break for prop in properties: property_data = { &#39;platform&#39;: &#39;99acres&#39;, &#39;city&#39;: city, &#39;title&#39;: prop.find_element(By.CLASS_NAME, &#34;tu__fnt&#34;).text, &#39;price&#39;: extract_price_99(prop), &#39;locality&#39;: extract_locality(prop), &#39;details&#39;: extract_details(prop) } properties.append(property_data) return properties Data Challenges &amp; Solutions 1. Anti-Bot Measures Challenge CAPTCHA challenges Rate limiting IP blocking User-agent detection Solutions # Rotate user agents USER_AGENTS = [ &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)...&#39;, &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)...&#39;, &#39;Mozilla/5.0 (X11; Linux x86_64)...&#39; ] # Random delays time.sleep(random.uniform(2, 5)) # Use proxies proxy = random.choice(PROXY_LIST) driver = webdriver.Chrome(options=options, proxy=proxy) 2. Dynamic Content Loading Challenge JavaScript-rendered content Infinite scroll Lazy loading Solutions # Scroll to load all content def scroll_to_bottom(driver): last_height = driver.execute_script(&#34;return document.body.scrollHeight&#34;) while True: driver.execute_script(&#34;window.scrollTo(0, document.body.scrollHeight);&#34;) time.sleep(2) new_height = driver.execute_script(&#34;return document.body.scrollHeight&#34;) if new_height == last_height: break last_height = new_height 3. Inconsistent Data Formats Challenge Different price formats (₹8,000 vs 8000) Varying address structures Inconsistent amenity lists Solutions # Normalize price def normalize_price(price_str): # Remove currency symbols and commas price = re.sub(r&#39;[₹,]&#39;, &#39;&#39;, price_str) # Extract numbers price_num = re.findall(r&#39;\d&#43;&#39;, price) return int(price_num[0]) if price_num else 0 # Normalize location def normalize_location(location): # Remove extra spaces location = re.sub(r&#39;\s&#43;&#39;, &#39; &#39;, location.strip()) # Standardize case return location.title() Data Processing Pipeline 1. Data Collection # Collect from all platforms raw_data = scraper.scrape_all_platforms( cities=MAJOR_CITIES, property_type=&#39;pg_hostel&#39; ) 2. Data Cleaning def clean_property_data(properties): cleaned = [] for prop in properties: # Standardize fields cleaned_prop = { &#39;platform&#39;: prop[&#39;platform&#39;], &#39;city&#39;: normalize_city(prop[&#39;city&#39;]), &#39;title&#39;: clean_text(prop[&#39;title&#39;]), &#39;price&#39;: normalize_price(prop[&#39;price&#39;]), &#39;location&#39;: normalize_location(prop[&#39;location&#39;]), &#39;amenities&#39;: clean_amenities(prop[&#39;amenities&#39;]) } # Validate data if is_valid_property(cleaned_prop): cleaned.append(cleaned_prop) return cleaned 3. Deduplication # Remove duplicates across platforms def remove_duplicates(properties): seen = set() unique = [] for prop in properties: key = f&#34;{prop[&#39;title&#39;]}_{prop[&#39;location&#39;]}&#34; if key not in seen: seen.add(key) unique.append(prop) return unique 4. Data Enrichment # Add derived fields def enrich_property_data(properties): for prop in properties: # Categorize price range prop[&#39;price_range&#39;] = categorize_price(prop[&#39;price&#39;]) # Extract locality prop[&#39;locality&#39;] = extract_locality(prop[&#39;location&#39;]) # Add search keywords prop[&#39;keywords&#39;] = generate_keywords(prop) return properties Data Structure Final Dataset Schema { &#34;property_id&#34;: &#34;prop_001&#34;, &#34;platform&#34;: &#34;magicbricks&#34;, &#34;city&#34;: &#34;Mumbai&#34;, &#34;title&#34;: &#34;Girls PG in Andheri&#34;, &#34;price&#34;: 8000, &#34;price_range&#34;: &#34;budget&#34;, &#34;location&#34;: &#34;Andheri West, Mumbai&#34;, &#34;locality&#34;: &#34;Andheri West&#34;, &#34;amenities&#34;: [&#34;WiFi&#34;, &#34;Food&#34;, &#34;AC&#34;, &#34;Laundry&#34;], &#34;contact&#34;: &#34;&#43;91-9876543210&#34;, &#34;verified&#34;: true, &#34;date_scraped&#34;: &#34;2024-07-15&#34;, &#34;keywords&#34;: [&#34;pg&#34;, &#34;girls&#34;, &#34;andheri&#34;, &#34;budget&#34;] } Results &amp; Statistics Coverage Platform Properties Cities Success Rate Magic Bricks 28,500 50 94% JustDial 25,000 50 89% 99acres 16,500 50 91% Total 70,000 50 91% Top Cities by Property Count Bangalore: 8,500 Mumbai: 7,200 Pune: 6,800 Delhi: 6,500 Hyderabad: 5,900 Price Distribution Budget (₹3K-8K): 35% Mid-range (₹8K-15K): 45% Premium (₹15K&#43;): 20% Key Features Extracted For Students Proximity to colleges: Within 5km radius Safety features: CCTV, security guard Amenities: WiFi, food, laundry, AC Mess options: Vegetarian/Non-vegetarian Rules: Entry time, guest policy For Search Price filters: Range-based Location filters: City, locality, landmark Amenity filters: Checkbox selection Rating filters: User ratings Impact &amp; Applications 1. Student Platform Search engine for student accommodation Compare options across platforms Filter by college proximity Price comparison 2. Market Research Understand supply/demand Price trend analysis Location popularity Amenity preferences 3. Property Owners Monitor competitor pricing Optimize property listings Identify market gaps Challenges Faced 1. Legal Considerations Robots.txt compliance: Respected all robots.txt files Rate limiting: Avoided overwhelming servers Terms of service: Stayed within allowed usage 2. Data Quality Outdated listings: Some properties no longer available Fake contacts: Phone numbers that don’t work Price accuracy: Prices change frequently 3. Maintenance Website changes: Regular updates needed Anti-bot updates: Adapting to new measures Data freshness: Continuous monitoring required Future Enhancements 1. Real-time Updates Monitor price changes Track new listings Remove sold/rented properties 2. Image Analysis Verify property images Detect duplicate photos Quality scoring 3. Machine Learning Price prediction models Quality scoring Recommendation engine Technologies Used Python: Core language Selenium WebDriver: Browser automation Beautiful Soup: HTML parsing Pandas: Data processing SQLite: Data storage Requests: HTTP requests OpenPyXL: Excel export Lessons Learned 1. Platform-Specific Approaches Each platform needs custom scraping logic. Generic scrapers don’t work well.
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-07-15 00:00:00 +0000 UTC'>July 15, 2024</span>&nbsp;·&nbsp;<span>6 min</span></footer>
  <a class="entry-link" aria-label="post link to Scraping 70K Property Listings: PG & Hostel Data Collection" href="https://shubhamapat7.github.io/post/property-scraper-analysis/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Student ID Verification Pipeline for dip
    </h2>
  </header>
  <div class="entry-content">
    <p>Overview Built a multi-stage pipeline for verifying student ID cards using advanced deep learning models to detect various types of forgery and tampering.
Pipeline Architecture Test Dataset 97 manually collected and cleaned ID card images Diverse ID card formats and designs Model Components 1. Face Detection (YOLOv8) High-accuracy face detection in ID cards Fast inference for real-time processing 2. Gender Validation DeepFace-based gender classification Accuracy: 80% (with potential for improvement via dedicated training) 3. Splicing Detection Model: Image Splicing Detector (image_splicer_quant.tflite) Architecture: DenseNet121 with ELA preprocessing Training Data: CASIA dataset Preprocessing: Error Level Analysis (ELA) Performance: Train accuracy: 97.35% Validation accuracy: 82.31% Output: Splicing score (threshold: 0.90) Average test score: 0.95 4. Copy-Move Detection Model: New UNet (new_unet.tflite) Training: COMOFOD dataset Training: 30 epochs Accuracy: 98.7% Output: Copy-move score (threshold: 0.22) Average test score: 0.19 OCR &amp; Text Validation File: speed_uppp2.py Features: OCR text extraction Text consistency check Keyword matching (case/special character validation) Keyword match percentage calculation ELA analysis Pipeline Flow Face Detection (FaceDetection_YOLO.py) Gender Validation (gender_validation.py) Splicing Detection (splicing_detector.py) Copy-Move Detection (copymove_detector.py) OCR &amp; Text Analysis (speed_uppp2.py) Validation Results Successfully processes diverse ID card formats Robust detection of multiple forgery types Multi-layer validation ensures high accuracy Real-time processing capability Technologies Used YOLOv8 TensorFlow Lite UNet DenseNet121 DeepFace OCR (Optical Character Recognition) Error Level Analysis (ELA) Python Computer Vision </p>
  </div>
  <footer class="entry-footer"><span title='2024-06-10 00:00:00 +0000 UTC'>June 10, 2024</span>&nbsp;·&nbsp;<span>2 min</span></footer>
  <a class="entry-link" aria-label="post link to Student ID Verification Pipeline for dip" href="https://shubhamapat7.github.io/projects/student-id-verification-pipeline/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">College Hotspots Dashboard - Interactive Data Visualization
    </h2>
  </header>
  <div class="entry-content">
    <p>Overview Developed an interactive data visualization dashboard from scratch to analyze student-focused businesses around colleges and universities across major Indian cities.
Dataset Source: Scraped from Google Maps Size: 3 GB of comprehensive business data Coverage: Ahmedabad, Gandhinagar, Mumbai, Pune, Kolkata, Hyderabad, Bangalore Content: All business information available on Google Maps Data Pipeline Scraping: Automated data collection from Google Maps Cleaning: Data preprocessing and validation Feature Extraction: Relevant business attributes Data Visualization: Interactive charts and maps Dashboard Features Interactive Visualizations ScatterMapBox
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-05-01 00:00:00 +0000 UTC'>May 1, 2024</span>&nbsp;·&nbsp;<span>2 min</span></footer>
  <a class="entry-link" aria-label="post link to College Hotspots Dashboard - Interactive Data Visualization" href="https://shubhamapat7.github.io/projects/college-hotspots-dashboard/"></a>
</article>

<article class="post-entry"> 
<figure class="entry-cover">
        <img loading="lazy" src="https://shubhamapat7.github.io/images/crocodile-research.jpg" alt="Mugger crocodile identification using UAV imagery">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">Individual Identification of Mugger Crocodiles using YOLOv8
    </h2>
  </header>
  <div class="entry-content">
    <p>Introduction Wildlife conservation requires precise monitoring of individual animals to understand behavior patterns, population dynamics, and migration routes. Manual identification is time-consuming and often inaccurate. This project tackles the challenge of individual identification of mugger crocodiles using YOLOv8 and UAV (drone) imagery.
The Challenge Mugger crocodiles have unique back scute patterns, similar to human fingerprints. However:
Manual identification is error-prone Large geographical areas need monitoring Traditional methods are time-intensive Data volume is massive (500GB of images) Dataset Details Size: 500GB of high-resolution imagery Subjects: 160 individual mugger crocodiles Images per subject: 1,000 images Source: UAV imagery from Gujarat’s western and southern regions Hardware: Param Shavak Supercomputer (Linux-based, no GUI) Technical Approach Model Architecture Base Model: YOLOv8-XL (extra large for high accuracy) Task: Detection and classification of individual crocodiles Optimization: Custom CLI scripts for data processing Hyperparameter Tuning Extensive grid search across multiple runs Multi-day training sessions on NVIDIA P5000 GPUs Threshold optimization at 0.82 for best True Positive/Negative rates Results Performance Metrics mAP (Mean Average Precision): 98.50% Threshold: 0.82 (optimized for wildlife monitoring) Processing Time: Real-time capable on HPC infrastructure Impact Non-invasive monitoring: No need to capture or tag crocodiles Behavioral studies: Enables long-term tracking Conservation efforts: Better population management Research contribution: Advancement in wildlife biometrics Technical Challenges &amp; Solutions Challenge 1: Massive Dataset (500GB) Solution: Developed custom CLI scripts for efficient processing on the Param Shavak supercomputer
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-04-20 00:00:00 +0000 UTC'>April 20, 2024</span>&nbsp;·&nbsp;<span>2 min</span></footer>
  <a class="entry-link" aria-label="post link to Individual Identification of Mugger Crocodiles using YOLOv8" href="https://shubhamapat7.github.io/post/crocodile-identification-yolov8/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="next" href="https://shubhamapat7.github.io/page/2/">Next&nbsp;&nbsp;»
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://shubhamapat7.github.io/">Shubham Apat</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
