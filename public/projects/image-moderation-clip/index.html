<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Image Moderation using OpenAI CLIP | Shubham Apat</title>
<meta name="keywords" content="OpenAI CLIP, Computer Vision, Multi-modal AI, Image Classification">
<meta name="description" content="Implemented image moderation pipeline using OpenAI&#39;s CLIP model achieving 92% accuracy through prompt engineering and hyperparameter tuning.">
<meta name="author" content="">
<link rel="canonical" href="https://shubhamapat7.github.io/projects/image-moderation-clip/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.6e04ee46d2c7afece6b54a05f2620f6336a323618852a4391ecff8c50f09cf28.css" integrity="sha256-bgTuRtLHr&#43;zmtUoF8mIPYzajI2GIUqQ5Hs/4xQ8Jzyg=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://shubhamapat7.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://shubhamapat7.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://shubhamapat7.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://shubhamapat7.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://shubhamapat7.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://shubhamapat7.github.io/projects/image-moderation-clip/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="https://shubhamapat7.github.io/projects/image-moderation-clip/">
  <meta property="og:site_name" content="Shubham Apat">
  <meta property="og:title" content="Image Moderation using OpenAI CLIP">
  <meta property="og:description" content="Implemented image moderation pipeline using OpenAI&#39;s CLIP model achieving 92% accuracy through prompt engineering and hyperparameter tuning.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="projects">
    <meta property="article:published_time" content="2024-10-01T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-10-01T00:00:00+00:00">
    <meta property="article:tag" content="OpenAI CLIP">
    <meta property="article:tag" content="Computer Vision">
    <meta property="article:tag" content="Multi-Modal AI">
    <meta property="article:tag" content="Image Classification">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Image Moderation using OpenAI CLIP">
<meta name="twitter:description" content="Implemented image moderation pipeline using OpenAI&#39;s CLIP model achieving 92% accuracy through prompt engineering and hyperparameter tuning.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Projects",
      "item": "https://shubhamapat7.github.io/projects/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Image Moderation using OpenAI CLIP",
      "item": "https://shubhamapat7.github.io/projects/image-moderation-clip/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Image Moderation using OpenAI CLIP",
  "name": "Image Moderation using OpenAI CLIP",
  "description": "Implemented image moderation pipeline using OpenAI's CLIP model achieving 92% accuracy through prompt engineering and hyperparameter tuning.",
  "keywords": [
    "OpenAI CLIP", "Computer Vision", "Multi-modal AI", "Image Classification"
  ],
  "articleBody": "Overview Leveraged OpenAI’s CLIP (Contrastive Language-Image Pre-training) model for building an advanced image moderation pipeline with high accuracy and flexibility.\nClassification System Classes NSFW SEXY NEUTRAL VIOLENCE DRUGS Methodology CLIP uses both vision and text encoders to understand images through natural language descriptions.\nApproach Prompt Engineering Prompts per Class: 60+ text prompts per category Strategy: Diverse descriptive prompts for each class Text Encoder: CLIP’s language model generates embeddings Classification: Based on similarity scores between image and text embeddings Scoring Mechanism Similarity Calculation: Vision encoder processes input image Text Matching: Compare against all class prompts Normalization: Sigmoid function on similarity scores Classification: Highest score determines class Model Performance Optimization Techniques Grid Search CV: Systematic hyperparameter exploration Hyperparameter Tuning: Fine-tuned model parameters False Positive Analysis: Analyzed misclassifications per class Results Overall Accuracy: 92% Robust Classification: Effective across all 5 classes Generalization: Strong performance on diverse image types Advantages of CLIP Approach Flexibility: Easy to add new classes with prompts Zero-shot Learning: Can classify without class-specific training Multi-modal Understanding: Leverages both visual and textual understanding Scalable: Simple to expand classification categories Technologies Used OpenAI CLIP Computer Vision Multi-modal AI Prompt Engineering Hyperparameter Tuning Python Image Classification Natural Language Processing ",
  "wordCount" : "196",
  "inLanguage": "en",
  "datePublished": "2024-10-01T00:00:00Z",
  "dateModified": "2024-10-01T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://shubhamapat7.github.io/projects/image-moderation-clip/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shubham Apat",
    "logo": {
      "@type": "ImageObject",
      "url": "https://shubhamapat7.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://shubhamapat7.github.io/" accesskey="h" title="Shubham Apat (Alt + H)">Shubham Apat</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://shubhamapat7.github.io/post/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://shubhamapat7.github.io/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://shubhamapat7.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/shubhamapat7" title="GitHub">
                    <span>GitHub</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Image Moderation using OpenAI CLIP
    </h1>
    <div class="post-description">
      Implemented image moderation pipeline using OpenAI&#39;s CLIP model achieving 92% accuracy through prompt engineering and hyperparameter tuning.
    </div>
    <div class="post-meta"><span title='2024-10-01 00:00:00 +0000 UTC'>October 1, 2024</span>&nbsp;·&nbsp;<span>1 min</span>

</div>
  </header> 
  <div class="post-content"><h2 id="overview">Overview<a hidden class="anchor" aria-hidden="true" href="#overview">#</a></h2>
<p>Leveraged OpenAI&rsquo;s CLIP (Contrastive Language-Image Pre-training) model for building an advanced image moderation pipeline with high accuracy and flexibility.</p>
<h2 id="classification-system">Classification System<a hidden class="anchor" aria-hidden="true" href="#classification-system">#</a></h2>
<h3 id="classes">Classes<a hidden class="anchor" aria-hidden="true" href="#classes">#</a></h3>
<ul>
<li><strong>NSFW</strong></li>
<li><strong>SEXY</strong></li>
<li><strong>NEUTRAL</strong></li>
<li><strong>VIOLENCE</strong></li>
<li><strong>DRUGS</strong></li>
</ul>
<h3 id="methodology">Methodology<a hidden class="anchor" aria-hidden="true" href="#methodology">#</a></h3>
<p>CLIP uses both vision and text encoders to understand images through natural language descriptions.</p>
<h2 id="approach">Approach<a hidden class="anchor" aria-hidden="true" href="#approach">#</a></h2>
<h3 id="prompt-engineering">Prompt Engineering<a hidden class="anchor" aria-hidden="true" href="#prompt-engineering">#</a></h3>
<ul>
<li><strong>Prompts per Class</strong>: 60+ text prompts per category</li>
<li><strong>Strategy</strong>: Diverse descriptive prompts for each class</li>
<li><strong>Text Encoder</strong>: CLIP&rsquo;s language model generates embeddings</li>
<li><strong>Classification</strong>: Based on similarity scores between image and text embeddings</li>
</ul>
<h3 id="scoring-mechanism">Scoring Mechanism<a hidden class="anchor" aria-hidden="true" href="#scoring-mechanism">#</a></h3>
<ol>
<li><strong>Similarity Calculation</strong>: Vision encoder processes input image</li>
<li><strong>Text Matching</strong>: Compare against all class prompts</li>
<li><strong>Normalization</strong>: Sigmoid function on similarity scores</li>
<li><strong>Classification</strong>: Highest score determines class</li>
</ol>
<h2 id="model-performance">Model Performance<a hidden class="anchor" aria-hidden="true" href="#model-performance">#</a></h2>
<h3 id="optimization-techniques">Optimization Techniques<a hidden class="anchor" aria-hidden="true" href="#optimization-techniques">#</a></h3>
<ul>
<li><strong>Grid Search CV</strong>: Systematic hyperparameter exploration</li>
<li><strong>Hyperparameter Tuning</strong>: Fine-tuned model parameters</li>
<li><strong>False Positive Analysis</strong>: Analyzed misclassifications per class</li>
</ul>
<h3 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h3>
<ul>
<li><strong>Overall Accuracy</strong>: 92%</li>
<li><strong>Robust Classification</strong>: Effective across all 5 classes</li>
<li><strong>Generalization</strong>: Strong performance on diverse image types</li>
</ul>
<h2 id="advantages-of-clip-approach">Advantages of CLIP Approach<a hidden class="anchor" aria-hidden="true" href="#advantages-of-clip-approach">#</a></h2>
<ul>
<li><strong>Flexibility</strong>: Easy to add new classes with prompts</li>
<li><strong>Zero-shot Learning</strong>: Can classify without class-specific training</li>
<li><strong>Multi-modal Understanding</strong>: Leverages both visual and textual understanding</li>
<li><strong>Scalable</strong>: Simple to expand classification categories</li>
</ul>
<h2 id="technologies-used">Technologies Used<a hidden class="anchor" aria-hidden="true" href="#technologies-used">#</a></h2>
<ul>
<li>OpenAI CLIP</li>
<li>Computer Vision</li>
<li>Multi-modal AI</li>
<li>Prompt Engineering</li>
<li>Hyperparameter Tuning</li>
<li>Python</li>
<li>Image Classification</li>
<li>Natural Language Processing</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://shubhamapat7.github.io/tags/openai-clip/">OpenAI CLIP</a></li>
      <li><a href="https://shubhamapat7.github.io/tags/computer-vision/">Computer Vision</a></li>
      <li><a href="https://shubhamapat7.github.io/tags/multi-modal-ai/">Multi-Modal AI</a></li>
      <li><a href="https://shubhamapat7.github.io/tags/image-classification/">Image Classification</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://shubhamapat7.github.io/projects/linkedin-scraper/">
    <span class="title">« Prev</span>
    <br>
    <span>LinkedIn Scraper - Comprehensive Social Media Data Extraction</span>
  </a>
  <a class="next" href="https://shubhamapat7.github.io/post/nsfw-moderation-dataset/">
    <span class="title">Next »</span>
    <br>
    <span>Building a 150K&#43; Image Dataset for Content Moderation</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://shubhamapat7.github.io/">Shubham Apat</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
