[{"content":"When you work with drone footage, nothing is easy. You‚Äôve got shaky cameras, tiny objects, crowded scenes, occlusion everywhere, and the constant joy of dealing with aerial perspectives where cars look the size of matchboxes. When I started exploring the VisDrone 2019 dataset, I didn‚Äôt realize I was stepping into one of the most challenging computer vision benchmarks out there.\nBut that challenge became fun pretty quickly.\nI wanted to build something end-to-end ‚Äî not just object detection, not just tracking, but a complete UAV vision system: detect objects in images, detect in videos, track one vehicle persistently, track everything simultaneously, and finally use all of that to analyze traffic like a real-world AI surveillance system.\nSo I split the project into four modules: object detection, single-object tracking, multi-object tracking, and traffic analysis. Even though they build on each other, each one required solving a different kind of problem, and each one taught me something new about computer vision.\nObject Detection on VisDrone Images and Videos\nThe starting point of everything was object detection. You can‚Äôt track anything if you can‚Äôt detect it first. The VisDrone2019 dataset is collected by the AISKYEYE team at Lab of Machine Learning and Data Mining , Tianjin University, China. The benchmark dataset consists of 288 video clips formed by 261,908 frames and 10,209 static images, captured by various drone-mounted cameras, covering a wide range of aspects including location (taken from 14 different cities separated by thousands of kilometers in China), environment (urban and country), objects (pedestrian, vehicles, bicycles, etc.), and density (sparse and crowded scenes). Note that, the dataset was collected using various drone platforms (i.e., drones with different models), in different scenarios, and under various weather and lighting conditions. These frames are manually annotated with more than 2.6 million bounding boxes of targets of frequent interests, such as pedestrians, cars, bicycles, and tricycles. Some important attributes including scene visibility, object class and occlusion, are also provided for better data utilization.\nI finetuned YOLOv8 because its State-of-the-Art and simply perfect for fast prototyping. After the fine tuning entire detection step barely requires 10 lines of code, which still feels illegal to me:\nfrom ultralytics import YOLO model = YOLO(\u0026#34;yolov8m.pt\u0026#34;) results = model(\u0026#34;image.jpg\u0026#34;) for result in results: boxes = result.boxes print(boxes) YOLOv8 handles everything behind the scenes preprocessing, non-maximum suppression, post-processing which is why the model is so great for UAV pipelines. The model was surprisingly robust even with VisDrone‚Äôs tiny pedestrians and distant vehicles. When running detection on full drone videos, YOLO‚Äôs streaming API made the process extremely smooth:\nfor frame in model.predict(source=\u0026#34;video.mp4\u0026#34;, stream=True): detections = frame.boxes.xyxy By this point, I had a working detection system. But detection alone doesn‚Äôt feel smart. It‚Äôs just finding objects, not understanding them. To make the system behave ‚Äúintelligently,‚Äù I needed tracking.\nSingle Object Tracking With Click-to-Select Re-Identification\nSingle-object tracking sounds trivial until you actually try it. The challenge is not following an object frame to frame! The challenge is keeping the same ID even when the detector switches IDs, the object gets occluded, or it shrinks to 6 pixels because the drone flew higher.\nI wrote a custom tracking script where you can click on any bounding box in the first frame, and the system will track that object across the entire video. Here\u0026rsquo;s the exact code for the click-to-select logic from my script:\ndef select_vehicle(event, x, y, flags, param): if event == cv2.EVENT_LBUTTONDOWN: for box in param[\u0026#39;boxes\u0026#39;]: x1, y1, x2, y2 = map(int, box[:4]) if x1 \u0026lt;= x \u0026lt;= x2 and y1 \u0026lt;= y \u0026lt;= y2: selected_vehicle_id = box[4] print(\u0026#34;Selected vehicle:\u0026#34;, selected_vehicle_id) break This part alone makes the pipeline feel interactive. I can visually inspect the video, click a car I‚Äôm interested in, and the rest of the system revolves around that one click.\nBut UAV tracking is dirty. IDs switch. Objects disappear. So I added a re-identification mechanism. It remembers the last known position of the target, and if YOLO loses the ID, it automatically reselects the closest detected box to the previous location:\ndef reselect_vehicle(current_boxes): global selected_vehicle_id, last_position if selected_vehicle_id not in [int(b.id) for b in current_boxes]: best, best_id = float(\u0026#39;inf\u0026#39;), None for box in current_boxes: center = (int(box.xywh[0][0]), int(box.xywh[0][1])) distance = np.linalg.norm(np.array(center) - np.array(last_position)) if distance \u0026lt; best: best, best_id = distance, box.id selected_vehicle_id = best_id It‚Äôs a simple heuristic but in drone footage, simple heuristics can outperform complex algorithms.\nBy the end of this part, I had a smooth, stable single-object tracking system that can lock onto a specific vehicle even when YOLO momentarily fails.\nMulti-Object Tracking With YOLO + ByteTrack/SORT-Style Logic While single-object tracking feels satisfying, multi-object tracking is where things start looking like an actual surveillance system. The idea is to detect every object in every frame and maintain consistent identities through time.\nVisDrone videos can contain 20‚Äì100 moving objects, and YOLOv8‚Äôs built-in tracker is already aligned with the concepts from algorithms like SORT and ByteTrack. YOLO gives you consistent box.id values through its built-in tracking:\nfor frame in model.track(source=\u0026#34;video.mp4\u0026#34;, stream=True): for box in frame.boxes: print(\u0026#34;ID:\u0026#34;, box.id, \u0026#34;Coordinates:\u0026#34;, box.xyxy) Your browser does not support the video tag. Behind the scenes, YOLO‚Äôs tracker does three main things: Associates detections across frames Keeps track of disappeared objects Handles ID reassignment\nIt uses the same ideas as SORT: IoU matching + Kalman filtering. ByteTrack improves this by also linking low-confidence detections, which matters a lot in drone footage because distant vehicles often get low scores.\nOnce MOT is working, you suddenly see dozens of tiny colored labels dancing around a UAV video ‚Äî every person, bike, and car with its own persistent ID. This part became crucial for my next step: traffic analysis.\nTraffic Analysis: Turning UAV Tracking Into Real-World Insights\nTracking alone is cool, but I wanted to turn raw detections into something meaningful ‚Äî something that looks like real analytics. With consistent IDs from the MOT module, traffic analysis becomes almost trivial. You can count vehicles, measure how long they stay in the frame, estimate congestion, and get per-class statistics.\nFor example, counting the number of unique vehicles in a video becomes a dictionary operation:\nvehicle_count = set() for result in model.track(source=\u0026#34;video.mp4\u0026#34;, stream=True): for box in result.boxes: vehicle_count.add(int(box.id)) print(\u0026#34;Vehicles detected:\u0026#34;, len(vehicle_count)) If you define lane regions or zones, you can compute how many cars pass through each zone. If you track bounding box displacement across frames, you can approximate velocity.\nA typical displacement-based speed rough estimate looks like this:\ns p e e d = n p . l i n a l g . n o r m ( n p . a r r a y ( c u r r _ c e n t e r ) - n p . a r r a y ( p r e v _ c e n t e r ) ) / t i m e _ e l a p s e d Of course, you need scale calibration for real-world units, but for relative motion analysis, even pixel-based speed works well.\nYour browser does not support the video tag. This part of the project made the system feel like an actual drone-powered traffic monitor not just a computer vision demo.\n","permalink":"https://shubhamapat.github.io/projects/visdrone-challenge-2019/","summary":"\u003cp\u003eWhen you work with drone footage, nothing is easy. You‚Äôve got shaky cameras, tiny objects, crowded scenes, occlusion everywhere, and the constant joy of dealing with aerial perspectives where cars look the size of matchboxes. When I started exploring the VisDrone 2019 dataset, I didn‚Äôt realize I was stepping into one of the most challenging computer vision benchmarks out there.\u003c/p\u003e\n\u003cp\u003eBut that challenge became fun pretty quickly.\u003c/p\u003e\n\u003cp\u003eI wanted to build something end-to-end ‚Äî not just object detection, not just tracking, but a complete UAV vision system: detect objects in images, detect in videos, track one vehicle persistently, track everything simultaneously, and finally use all of that to analyze traffic like a real-world AI surveillance system.\u003c/p\u003e","title":"VisDrone Challenge"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\n\u0026lt;img src=\u0026ldquo;images/scattermap.png\u0026rdquo; | absURL\u0026gt;\nSecond was an interactive histogram of top 50 pincodes with highest business counts. \u0026lt;img src=\u0026ldquo;images/histogram.png\u0026rdquo; | absURL\u0026gt; We can select a pincode from histogram or from scattermap \u0026lt;img src=\u0026ldquo;images/city_selection.png\u0026rdquo; | absURL\u0026gt;\nUpon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! \u0026lt;img src=\u0026ldquo;images/business_table.png\u0026rdquo; | absURL\u0026gt;\nI added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. \u0026lt;img src=\u0026ldquo;images/pincode_download.png\u0026rdquo; | absURL\u0026gt;\nFirst Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV ‚Üí Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s ‚Üí 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB ‚Üí 1.2GB # Load time: 60s ‚Üí 30s Improvement: 2x faster (60s ‚Üí 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s ‚Üí 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"https://shubhamapat.github.io/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Abstract\nIndividual identification contributes significantly towards investigating behavioral mechanisms of animals and understanding underlying ecological principles. Most studies employ invasive procedures for individually identifying organisms. In recent times, computer-vision techniques have served as an alternative to invasive methods. However, these studies primarily rely on user input data collected from captivity or from individuals under partially restrained conditions. Challenges in collecting data from free-ranging individuals are higher when compared to captive populations. However, the former is a far more important priority for real-world applications.\nIndividual Identification of mugger crocodile, is where my computer vision journey started. I was part of a deep learning research project under Prof. Mehul Raval, where we were provided a dataset of free-ranging muggger crocodiles collected using Unmanned Aerial Vehicle (UAV). The dataset contained total 160,000 images focusing on mugger\u0026rsquo;s dorsal body. The data was collected from 160 individuals across 19 different locations along the western part of India. This was an extension to an already done research.\nUsing a CNN model, we aim to individually identify free ranging mugger crocodiles, Crocodylus palustris based on their dorsal scute patterns. Scutes are hard calcium bony plates called osteoderms (Fig. 1) that form a crocodile\u0026rsquo;s dorsal body. It serves as a unique identifier based on its placement on the dorsal body. With this background, we hypothesize that the developed CNN-based IID for mugger crocodiles will provide a robust understanding of behavioral processes and physiological patterns at an individual level, which in turn will contribute towards species conservation.\nThe dataset had 160,000 images and their annotations (bounding boxes of scute patterns), we decided to use YOLOv8 as it is the state of the art object detection model, known for its speed and accuracy. The size of dataset was ~450GB, for data preprocessing and model training we were assigned access to ParamShavak supercoputer which use NVIDIA A6000 GPU!\nOnly problem paramshavak did not have GUI, so we coded whole summer in pUTTY cli, doing image processing tasks on a machine where images couldn\u0026rsquo;t open, very ironic right!\nData Preprocessing\nStandard data preprocessing steps before any object detection model training are image processing which includes, image resizing, normalization and data augmentation. Also annotation processing is very important step, Adjust bounding box coordinates to match the image transformations applied during preprocessing (e.g., resizing, cropping, flipping). Format Conversion, Convert annotations to the format expected by the chosen object detection framework (e.g., COCO, PASCAL VOC, YOLO format).\nAs we were using YOLOv8, we resized every image to 640x640, applied normalization and converted annotations into YOLO txt format. Another step taken was out of 1000 images per class, we only used 250 images per class in training by taking every 4th frame, divided that train data into 90:10 ratio for train and validation. Other 750 images per class were taken as unseen testing data.\nModel Training\nSince we already had selected the model, next step was model configuration. Configured hyperparameters using Adam optimizer, batch normalization and 100 training epochs.\nFor yolov8 training you need to prepare a YAML mapping file where you need to give path to train data, val data and mapping of classes.\nyolo train model=yolov8m.pt data=mapping.yaml epochs=100 imgsz=640 This was first training version which acheived 97.5% mAP50, after that did hyperparameter tuning with train-settings and the images-per-class which varied from 50,100,150,200,500 and even taking all 1000. But the final best accuracy acheived was with optimizer Adam, aumentations settings like perspective, flipud, fliplr, scale and 250 images per class whcih acheived 99.5% mAP50.\nFor testing, we used new data collected six months after the training dataset. Because of the time gap, the weather conditions had changed significantly, creating a noticeable difference between the training and test images.\nWe used two parameters, True Positive Rate (TPR) and True Negative Rate (TNR), to validate the efficiency of the trained models. Using YOLO-v5l, TPR (re-identification of trained muggers) and TNR (differentiating untrained muggers as ‚Äòunknown‚Äô) values at the 0.84 decision threshold were 88.8% and 89.6%, respectively. The trained model showed 100% TNR for the non-mugger species, the Gharial, Gavialis gangeticus, and the Saltwater crocodile, Crocodylus porosus.\n\u0026lt;img src=\u0026ldquo;images/yolov8_output1.png\u0026rdquo; width=600 | absURL\u0026gt; \u0026lt;img src=\u0026ldquo;images/yolov8_output2.png\u0026rdquo; width=600 | absURL\u0026gt;\nMetrics\n\u0026lt;img src=\u0026ldquo;images/confusion_matrix.png\u0026rdquo; width =600 | absURL\u0026gt;\nThis confusion matrix displays the predicted values on the y-axis and the true values on the x-axis. The diagonal line represents perfect predictions, where the predicted value matches the true value.\n\u0026lt;img src=\u0026ldquo;images/other_metrics.png\u0026rdquo; | absURL\u0026gt;\nprecision(B): This metric shows some fluctuations but generally increases over time, indicating improving precision performance. recall(B): Similar to precision, the recall metric also improves as training progresses. mAP50(B): This metric, likely mean Average Precision with an IoU threshold of 0.5, starts low but steadily increases, suggesting better object detection/localization performance. mAP50-95(B): This metric, probably mean Average Precision averaged over multiple IoU thresholds (0.5 to 0.95), also improves consistently during training.\nThe performance of the CNN model was reliable and accurate while using only 250 images per individual for training purposes. Showing that a bounding box approach (YOLO-v5l model) with background elimination is a promising method to individually identify free-ranging mugger crocodiles. Our manuscript demonstrates that UAV imagery appears to be a promising tool for non-invasive collection of data from free-ranging populations. It can be used to train open-source algorithms for individual identification. Further, the identification method is entirely based upon dorsal scute patterns, which can be applied to different crocodilian species, as well.\n","permalink":"https://shubhamapat.github.io/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract\u003c/strong\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIndividual identification contributes significantly towards investigating behavioral mechanisms of animals and understanding underlying ecological principles. Most studies employ invasive procedures for individually identifying organisms. In recent times, computer-vision techniques have served as an alternative to invasive methods. However, these studies primarily rely on user input data collected from captivity or from individuals under partially restrained conditions. Challenges in collecting data from free-ranging individuals are higher when compared to captive populations. However, the former is a far more important priority for real-world applications.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected ‚Üí switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;üé§ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies ‚Üí {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs ‚Üí {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"https://shubhamapat.github.io/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch ‚Üí ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX ‚Üí OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs ‚Üí OpenVINO NVIDIA GPUs ‚Üí TensorRT Apple Silicon ‚Üí Core ML ARM devices ‚Üí TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch ‚Üí ONNX ‚Üí OpenVINO) Quantization (FP32 ‚Üí FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"https://shubhamapat.github.io/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Overview \u0026lt;img src=\u0026ldquo;images/mangrove_plantation.jpg\u0026rdquo; | absURL\u0026gt;\nFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) ‚Äì to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) ‚Äì to separate water features -MNDWI = (B3 - B11) / (B3 + B11) ‚Äì to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Àú570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8‚àíB4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3‚àíB8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\nOverall Architecture\nOutcome\nThe strong outcome validates the effectiveness of the supervised learning approach, the geo-point sampling strategy, and the value of vegetation indices like NDVI and NDWI in distinguishing mangroves from surrounding land covers.\n(a) Global Mangrove Watch Reference (b) Manually Labeled GeoPoints (c) Final Classified Overlay Figure 4.1.2: Comparison of classification process: (a) Reference from Global Mangrove Watch, (b) manually annotated geo-points used for training the classifier, and (c) final classification result overlaid as tile using the trained model.","permalink":"https://shubhamapat.github.io/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003e\u0026lt;img src=\u0026ldquo;images/mangrove_plantation.jpg\u0026rdquo; | absURL\u0026gt;\u003c/p\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"I had never made a scraper and the first scraper i was assigned to build was BookMyShow. I was asked to scrape the every event listed on all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nBookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges So as a newbie I started research on how to build a scraper from scratch, went on reddit, stack.com, medium and documentations of beautifulsoup, playwright and selenium. So I built a simple two step scraper, first step which can scrape just events urls from their cards using the selenium chromedriver and find the div class of card using beautifulsoup.\n\u0026lt;img src=\u0026ldquo;images/bms.png\u0026rdquo; | absURL\u0026gt;\nThe second step takes this scraped urls of cards, opens them one by one and scrape information like date, time, venue, language, age-limit, price and even about the event into a csv file. All of this using BeautifulSoup4.\n\u0026lt;img src=\u0026ldquo;images/bms-url.png\u0026rdquo; | absURL\u0026gt;\nOnly problem with the first attempt was that the first step was not able to scroll much further because BMS uses lazy loading. Then again I started research about how to scrape sites that uses lazy loading for scrolling down the page to trigger the loading of dynamic content. Addition to lazy-loading, bookmyshow has an advance bot detection, if you keep scrolling to the bottom of the page for all the event cards at same pace it assumes you\u0026rsquo;re bot and stops loading events!!\nTo tackle this I came up with my own human-like scroll method for bot after a lot of trial and errors. This human scroll makes the bot scroll the page to bottom till footer and as pages load more event cards it again scroll up to get the urls from those cards and then again repeat the whole loop till no more new cards are loaded, and the pace of scrolling is random everytime so that avoids bot detection.\ndef human_scroll(driver): actions = ActionChains(driver) for _ in range(random.randint(5, 8)): actions.send_keys(Keys.ARROW_DOWN).perform() time.sleep(random.uniform(0.08, 0.2)) The scraper was ready was ready but there are still optimizations needed, cause I needed to 1898 cities in 4 different categories(sports, events, plays, and activities). To scrape a city\u0026rsquo;s events, sports, activities and plays page one by one was taking 120 seconds overall, now 120x1898 = 64 hours or 2.63 days. Ofcourse it needs optimization!\nIf you have read my other blogs, the zeroth step I take before optimise a pipeline is by measuring time taken by each function using time.time(). I got to know each categories page was taking around 35s to load and get scraped. To reduce, I decreased the sleep() time for loading and scrolling of scraper, which reduced 30s to around 25s but its still 100s overall. I couldn\u0026rsquo;t decrease the sleep() time for scrolling beyond that because then it won\u0026rsquo;t wait for lazy loading to load more events anymore and exit first.\nAnother optimization I applied was multithreading where I can scrape all 4 categories(events, sports, plays and activities) parallel that reduced the time 100s/4 t= 25s. With that I applied some methods to avoid collecting duplicate data where it does not save the same card url if thats scraped before which reduce unnecessary reads/writes, skip the city if there are no new cards and made scraper resumable. Only problem was we had to keep every window running parallel in foreground if the window is running in background BMS somehow stops loading events IKR!!\nso i used windows 4-screens layout for scraping in parallel\ndriver_events.set_window_position(0, 0) # top-left driver_sports.set_window_position(960, 0) # top-right driver_activities.set_window_position(0, 540) # bottom-left driver_plays.set_window_position(960, 540) # bottom-right The final time was 18s per city which means 18*1898= 9.5 hours so finaly we optimized data scraping from 64 hours to 9.5 hours!\nHere\u0026rsquo;s a snap of my bookmyshow scraper:\nYour browser does not support the video tag. Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster code‚Äîit\u0026rsquo;s a different approach altogether.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"https://shubhamapat.github.io/projects/bookmyshow-scraper/","summary":"\u003cp\u003eI had never made a scraper and the first scraper i was assigned to build was BookMyShow. I was asked to scrape the every event listed on  \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSo as a newbie I started research on how to build a scraper from scratch, went on reddit, stack.com, medium and documentations of beautifulsoup, playwright and selenium. So I built a simple two step scraper, first step which can scrape just events urls from their cards using the selenium chromedriver and find the div class of card using beautifulsoup.\u003c/p\u003e","title":"BookMyShow Scraper"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\n\u0026lt;img src=\u0026ldquo;images/autonomous_car.png\u0026rdquo; | absURL\u0026gt;\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings ‚Äî frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a bird‚Äôs-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram ‚Äî these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (bird‚Äôs-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15‚Äì20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"https://shubhamapat.github.io/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"}]